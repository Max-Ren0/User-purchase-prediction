# ğŸ¯ å¬å›å’Œè¯„ä¼°å‡½æ•° - çœŸå®æŒ‡æ ‡å®ç°
# ä»1_recall.ipynbå’Œ3_eval.ipynbæå–çš„æ ¸å¿ƒå‡½æ•°

import pandas as pd
import numpy as np
import time
import os
from typing import Dict, Any, Tuple
import gc

def run_recall(params: Dict[str, Any], data_dir: str = 'x', 
               sample_ratio: float = 1.0) -> Dict[str, Any]:
    """
    è¿è¡Œå¬å›ç®—æ³• - çœŸå®æŒ‡æ ‡ç‰ˆæœ¬
    
    Args:
        params: å‚æ•°å­—å…¸
        data_dir: æ•°æ®ç›®å½•
        sample_ratio: ç”¨æˆ·é‡‡æ ·æ¯”ä¾‹
    
    Returns:
        artifacts: å¬å›ç»“æœ
    """
    print(f"ğŸ”„ è¿è¡Œå¬å›ç®—æ³•...")
    print(f"ğŸ“Š å‚æ•°: {params}")
    
    # ä¼˜å…ˆä½¿ç”¨æŠ½æ ·æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å…¨é‡æ•°æ®
    train_sampled_path = f"{data_dir}/train_vis_sampled.parquet"
    train_path = f"{data_dir}/train_sorted.parquet"
    item_attr_path = f"{data_dir}/item_attr.parquet"
    
    if os.path.exists(train_sampled_path):
        print(f"ğŸ“Š ä½¿ç”¨æŠ½æ ·æ•°æ®: {train_sampled_path}")
        train = pd.read_parquet(train_sampled_path)
    else:
        print(f"ğŸ“Š ä½¿ç”¨å…¨é‡æ•°æ®: {train_path}")
        train = pd.read_parquet(train_path)
    
    # åŠ è½½å•†å“å±æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(item_attr_path):
        item_attr = pd.read_parquet(item_attr_path)
        print(f"ğŸ“Š åŠ è½½å•†å“å±æ€§: {len(item_attr):,} ä¸ªå•†å“")
    else:
        # åˆ›å»ºæ¨¡æ‹Ÿå•†å“å±æ€§
        unique_items = train['item_id'].unique()
        item_attr = pd.DataFrame({
            'item_id': unique_items,
            'cate_id': np.random.randint(1, 20, len(unique_items)),
            'store_id': np.random.randint(1, 50, len(unique_items))
        })
        print(f"ğŸ“Š åˆ›å»ºæ¨¡æ‹Ÿå•†å“å±æ€§: {len(item_attr):,} ä¸ªå•†å“")
    
    # æ•°æ®é‡‡æ · - æ·»åŠ éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
    if sample_ratio < 1.0:
        unique_users = train['buyer_admin_id'].unique()
        rng = np.random.default_rng(42)  # å›ºå®šéšæœºç§å­
        sample_users = rng.choice(
            unique_users, 
            size=int(len(unique_users) * sample_ratio),
            replace=False
        )
        train = train[train['buyer_admin_id'].isin(sample_users)]
        print(f"ğŸ“Š é‡‡æ ·åæ•°æ®: {len(train):,} è¡Œ, {len(sample_users):,} ç”¨æˆ·")
    
    # ä¼˜åŒ–æ•°æ®ç±»å‹
    train = train.astype({
        'buyer_admin_id': 'int32',
        'item_id': 'int32'
    })
    
    # å¦‚æœå­˜åœ¨cate_idå’Œstore_idåˆ—ï¼Œä¹Ÿè¿›è¡Œç±»å‹è½¬æ¢
    if 'cate_id' in train.columns:
        train['cate_id'] = train['cate_id'].astype('int32')
    if 'store_id' in train.columns:
        train['store_id'] = train['store_id'].astype('int32')
    
    # æ„å»ºç»Ÿè®¡è¡¨
    print(f"ğŸ“Š æ„å»ºç»Ÿè®¡è¡¨...")
    
    # 1. å¤è´­è¯„åˆ†
    rebuy = build_rebuy_scores_optimized(train, params['tau_days'])
    
    # 2. å…±ç°å…³ç³»
    covisit = build_covisit_optimized(train, params['covisit_window'], 
                                    params['covisit_top_per_a'])
    
    # 3. çƒ­é—¨ç»Ÿè®¡
    cate_pop, store_pop, global_pop = build_popularity_stats_optimized(
        train, item_attr, params['pop_pool'], params['per_cate_pool'], 
        params['per_store_pool']
    )
    
    # 4. ç”Ÿæˆå€™é€‰
    candidates = build_candidates_ultra_fast(
        train, rebuy, covisit, cate_pop, store_pop, global_pop, params
    )
    
    artifacts = {
        'candidates': candidates,
        'rebuy': rebuy,
        'covisit': covisit,
        'cate_pop': cate_pop,
        'store_pop': store_pop,
        'global_pop': global_pop,
        'params': params,
        'train': train
    }
    
    print(f"âœ… å¬å›å®Œæˆ: {len(candidates):,} ä¸ªå€™é€‰")
    return artifacts

def eval_offline(artifacts: Dict[str, Any], data_dir: str = 'x') -> Dict[str, float]:
    """
    ç¦»çº¿è¯„ä¼° - çœŸå®æŒ‡æ ‡ç‰ˆæœ¬
    
    Args:
        artifacts: å¬å›ç»“æœ
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        metrics: è¯„ä¼°æŒ‡æ ‡
    """
    print(f"ğŸ“Š è¿è¡Œç¦»çº¿è¯„ä¼°...")
    
    try:
        # åŠ è½½æ ‡ç­¾æ•°æ®
        label_path = f"{data_dir}/label_df.parquet"
        if not os.path.exists(label_path):
            print(f"âš ï¸ æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {label_path}")
            return {
                'hr50': 0.0,
                'mrr50': 0.0,
                'ndcg50': 0.0
            }
        
        label_df = pd.read_parquet(label_path)
        candidates = artifacts['candidates']
        
        # ä¼˜åŒ–ï¼šé¢„å¤„ç†æ ‡ç­¾ä¸ºdictä¸set
        label_map = (label_df.groupby('buyer_admin_id')['label_item']
                     .apply(set).to_dict())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        hr50 = calculate_hr50_optimized(candidates, label_map)
        mrr50 = calculate_mrr50_optimized(candidates, label_map)
        ndcg50 = calculate_ndcg50_optimized(candidates, label_map)
        
        metrics = {
            'hr50': hr50,
            'mrr50': mrr50,
            'ndcg50': ndcg50
        }
        
        print(f"âœ… è¯„ä¼°å®Œæˆ: HR@50={hr50:.3f}, MRR@50={mrr50:.3f}, NDCG@50={ndcg50:.3f}")
        return metrics
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        return {
            'hr50': 0.0,
            'mrr50': 0.0,
            'ndcg50': 0.0
        }

# ä»1_recall.ipynbæå–çš„æ ¸å¿ƒå‡½æ•°
def build_rebuy_scores_optimized(df, tau_days=14):
    """æ„å»ºå¤è´­è¯„åˆ† - ä¼˜åŒ–ç‰ˆ"""
    print(f"  ğŸ”„ æ„å»ºå¤è´­è¯„åˆ†...")
    
    # ä¿®å¤ï¼šå¯¹æ¯æ¬¡è´­ä¹°ç›¸å¯¹å‚è€ƒæ—¶åˆ»ï¼ˆå¦‚è¯¥ç”¨æˆ·çš„æœ€åä¸€æ¬¡ï¼‰åšæŒ‡æ•°è¡°å‡ï¼Œå†æ±‚å’Œ
    ref = df.groupby('buyer_admin_id')['create_order_time'].transform('max')
    days = (ref - df['create_order_time']).dt.days.clip(lower=0)
    w = np.exp(-days / float(tau_days))
    
    # æŒ‰ç”¨æˆ·å’Œå•†å“èšåˆ
    rebuy = (df.assign(score_rebuy=w)
              .groupby(['buyer_admin_id', 'item_id'])['score_rebuy']
              .sum().reset_index())
    
    return rebuy

def build_covisit_optimized(df, W=3, topk=200):
    """æ„å»ºå…±ç°å…³ç³» - ä¼˜åŒ–ç‰ˆ"""
    print(f"  ğŸ”— æ„å»ºå…±ç°å…³ç³»...")
    
    # æŒ‰ç”¨æˆ·æ’åº
    df_sorted = df.sort_values(['buyer_admin_id', 'create_order_time'])
    
    pairs = []
    for uid, group in df_sorted.groupby('buyer_admin_id'):
        items = group['item_id'].values
        for i in range(len(items)):
            for j in range(i+1, min(i+W+1, len(items))):
                pairs.append({
                    'item_a': items[i],
                    'item_b': items[j],
                    'w': 1.0 / (j - i)
                })
    
    if not pairs:
        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])
    
    co = pd.DataFrame(pairs)
    co = co.groupby(['item_a', 'item_b'])['w'].sum().reset_index()
    co['rn'] = co.groupby('item_a')['w'].rank(ascending=False, method='first')
    
    return co[co['rn'] <= topk].drop(columns='rn')

def build_popularity_stats_optimized(df, item_attr, pop_pool=2000, 
                                   per_cate_pool=80, per_store_pool=60):
    """æ„å»ºçƒ­é—¨ç»Ÿè®¡ - ä¼˜åŒ–ç‰ˆ"""
    print(f"  ğŸŒ æ„å»ºçƒ­é—¨ç»Ÿè®¡...")
    
    # å…¨å±€çƒ­é—¨
    global_pop = df.groupby('item_id').size().rename('pop').reset_index()
    global_pop = global_pop.sort_values('pop', ascending=False).head(pop_pool)
    
    # ç±»ç›®çƒ­é—¨ï¼ˆå¦‚æœitem_atträ¸­æœ‰cate_idï¼‰
    if 'cate_id' in item_attr.columns:
        df_with_cate = df.merge(item_attr[['item_id', 'cate_id']], on='item_id', how='left')
        cate_pop = (df_with_cate.groupby(['cate_id', 'item_id']).size().rename('pop').reset_index())
        cate_pop['rn'] = cate_pop.groupby('cate_id')['pop'].rank(ascending=False, method='first')
        cate_pop = cate_pop[cate_pop['rn'] <= per_cate_pool]
    else:
        # åˆ›å»ºç©ºçš„ç±»ç›®çƒ­é—¨è¡¨
        cate_pop = pd.DataFrame(columns=['cate_id', 'item_id', 'rn'])
    
    # åº—é“ºçƒ­é—¨ï¼ˆå¦‚æœitem_atträ¸­æœ‰store_idï¼‰
    if 'store_id' in item_attr.columns:
        df_with_store = df.merge(item_attr[['item_id', 'store_id']], on='item_id', how='left')
        store_pop = (df_with_store.groupby(['store_id', 'item_id']).size().rename('pop').reset_index())
        store_pop['rn'] = store_pop.groupby('store_id')['pop'].rank(ascending=False, method='first')
        store_pop = store_pop[store_pop['rn'] <= per_store_pool]
    else:
        # åˆ›å»ºç©ºçš„åº—é“ºçƒ­é—¨è¡¨
        store_pop = pd.DataFrame(columns=['store_id', 'item_id', 'rn'])
    
    return cate_pop, store_pop, global_pop

def build_candidates_ultra_fast(df, rebuy, covisit, cate_pop, store_pop, global_pop, params):
    """æ„å»ºå€™é€‰ - è¶…å¿«ç‰ˆ"""
    print(f"  ğŸ¯ æ„å»ºå€™é€‰...")
    
    # é¢„è®¡ç®—æ˜ å°„
    cov_neighbors = {}
    for a, g in covisit.groupby('item_a'):
        sub = g[['item_b', 'w']].head(params['cand_per_recent']).to_numpy()
        if len(sub):
            cov_neighbors[int(a)] = (sub[:, 0].astype('int64'), sub[:, 1].astype('float32'))
    
    recent_map = (df.sort_values('create_order_time')
                  .groupby('buyer_admin_id')['item_id']
                  .apply(lambda s: s.tail(params['recent_k']).to_numpy('int64'))
                  ).to_dict()
    
    # ç”¨æˆ·åå¥½ï¼ˆå¦‚æœæ•°æ®ä¸­æœ‰cate_idå’Œstore_idï¼‰
    user_topc = {}
    user_tops = {}
    
    if 'cate_id' in item_attr.columns and 'store_id' in item_attr.columns:
        df_with_attr = df.merge(item_attr[['item_id', 'cate_id', 'store_id']], on='item_id', how='left')
        user_topc = (df_with_attr.groupby('buyer_admin_id')['cate_id']
                     .apply(lambda s: s.value_counts().head(params.get('user_top_cates', 3))
                            .index.to_numpy('int64')).to_dict())
        user_tops = (df_with_attr.groupby('buyer_admin_id')['store_id']
                     .apply(lambda s: s.value_counts().head(params.get('user_top_stores', 3))
                            .index.to_numpy('int64')).to_dict())
    
    # çƒ­é—¨æ± æ˜ å°„
    cate_top = {}
    if not cate_pop.empty and 'cate_id' in cate_pop.columns:
        cate_top = {int(c): grp.loc[grp['rn'] <= params['per_cate_pool'], 'item_id'].to_numpy('int64')
                    for c, grp in cate_pop.groupby('cate_id')}
    
    store_top = {}
    if not store_pop.empty and 'store_id' in store_pop.columns:
        store_top = {int(s): grp.loc[grp['rn'] <= params['per_store_pool'], 'item_id'].to_numpy('int64')
                     for s, grp in store_pop.groupby('store_id')}
    global_items = global_pop['item_id'].to_numpy('int64')
    
    # å¤è´­æ˜ å°„
    rebuy_map = {}
    for uid, g in rebuy.groupby('buyer_admin_id'):
        rebuy_map[int(uid)] = (g['item_id'].to_numpy('int64'), g['score_rebuy'].to_numpy('float32'))
    
    # ç”Ÿæˆå€™é€‰
    candidates = {}
    unique_users = df['buyer_admin_id'].unique()
    
    for uid in unique_users:
        cand = {}
        
        # 1. å¤è´­å¬å›
        if uid in rebuy_map:
            items, ws = rebuy_map[uid]
            for it, w in zip(items, ws):
                cand.setdefault(int(it), []).append(('rebuy', float(w)))
        
        # 2. ååŒè¿‡æ»¤å¬å›
        for a in recent_map.get(uid, []):
            pair = cov_neighbors.get(int(a))
            if pair is not None:
                bs, ws = pair
                for b, w in zip(bs, ws):
                    cand.setdefault(int(b), []).append(('covisit', float(w)))
        
        # 3. ä¸ªæ€§åŒ–çƒ­é—¨å¬å›
        for c in user_topc.get(uid, []):
            for it in cate_top.get(int(c), ()):
                cand.setdefault(int(it), []).append(('cate_hot', 1.0))
        
        for s in user_tops.get(uid, []):
            for it in store_top.get(int(s), ()):
                cand.setdefault(int(it), []).append(('store_hot', 1.0))
        
        # 4. å…¨å±€çƒ­é—¨å¬å› - ä¼˜åŒ–ï¼šä»…å½“å€™é€‰ä¸è¶³æ—¶è¡¥å…¨å±€çƒ­é—¨çš„å‰Mä¸ª
        if len(cand) < params['recall_cap'] * 0.6:
            for it in global_items[:min(100, len(global_items))]:
                cand.setdefault(int(it), []).append(('global_pop', 1.0))
        
        # è®¡ç®—æœ€ç»ˆè¯„åˆ†
        user_items = []
        for item_id, sources in cand.items():
            pre_score = sum(score for _, score in sources)
            user_items.append((item_id, pre_score, sources, len(sources)))
        
        # æ’åºå¹¶æˆªæ–­
        user_items.sort(key=lambda x: x[1], reverse=True)
        top_items = user_items[:params['recall_cap']]
        
        candidates[uid] = top_items
    
    return candidates

# ä»3_eval.ipynbæå–çš„è¯„ä¼°å‡½æ•°
def calculate_hr50_optimized(candidates, label_map):
    """è®¡ç®—Hit Rate@50 - ä¼˜åŒ–ç‰ˆ"""
    hits = 0
    total = 0
    
    for uid, items in candidates.items():
        if uid in label_map:
            total += 1
            user_labels = label_map[uid]
            top_items = set(item[0] for item in items[:50])  # Top-50
            if user_labels & top_items:  # é›†åˆäº¤é›†
                hits += 1
    
    return hits / total if total > 0 else 0.0

def calculate_hr50(candidates, label_df):
    """è®¡ç®—Hit Rate@50 - åŸç‰ˆï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    hits = 0
    total = 0
    
    for uid, items in candidates.items():
        if uid in label_df['buyer_admin_id'].values:
            total += 1
            user_labels = label_df[label_df['buyer_admin_id'] == uid]['label_item'].values
            top_items = [item[0] for item in items[:50]]  # Top-50
            if any(label in top_items for label in user_labels):
                hits += 1
    
    return hits / total if total > 0 else 0.0

def calculate_mrr50_optimized(candidates, label_map):
    """è®¡ç®—MRR@50 - ä¼˜åŒ–ç‰ˆ"""
    mrr_sum = 0
    total = 0
    
    for uid, items in candidates.items():
        if uid in label_map:
            total += 1
            user_labels = label_map[uid]
            top_items = [item[0] for item in items[:50]]  # Top-50
            for i, item in enumerate(top_items):
                if item in user_labels:
                    mrr_sum += 1.0 / (i + 1)
                    break
    
    return mrr_sum / total if total > 0 else 0.0

def calculate_mrr50(candidates, label_df):
    """è®¡ç®—MRR@50 - åŸç‰ˆï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    mrr_sum = 0
    total = 0
    
    for uid, items in candidates.items():
        if uid in label_df['buyer_admin_id'].values:
            total += 1
            user_labels = label_df[label_df['buyer_admin_id'] == uid]['label_item'].values
            top_items = [item[0] for item in items[:50]]  # Top-50
            
            for rank, item in enumerate(top_items, 1):
                if item in user_labels:
                    mrr_sum += 1.0 / rank
                    break
    
    return mrr_sum / total if total > 0 else 0.0

def calculate_ndcg50_optimized(candidates, label_map):
    """è®¡ç®—NDCG@50 - ä¼˜åŒ–ç‰ˆ"""
    ndcg_sum = 0
    total = 0
    
    for uid, items in candidates.items():
        if uid in label_map:
            total += 1
            user_labels = label_map[uid]
            top_items = [item[0] for item in items[:50]]  # Top-50
            
            # è®¡ç®—DCG
            dcg = 0
            for rank, item in enumerate(top_items, 1):
                if item in user_labels:
                    dcg += 1.0 / np.log2(rank + 1)
            
            # è®¡ç®—IDCG (ç†æƒ³æƒ…å†µ)
            idcg = sum(1.0 / np.log2(i + 1) for i in range(1, min(len(user_labels), 50) + 1))
            
            ndcg_sum += dcg / idcg if idcg > 0 else 0
    
    return ndcg_sum / total if total > 0 else 0.0

def calculate_ndcg50(candidates, label_df):
    """è®¡ç®—NDCG@50 - åŸç‰ˆï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    ndcg_sum = 0
    total = 0
    
    for uid, items in candidates.items():
        if uid in label_df['buyer_admin_id'].values:
            total += 1
            user_labels = label_df[label_df['buyer_admin_id'] == uid]['label_item'].values
            top_items = [item[0] for item in items[:50]]  # Top-50
            
            # è®¡ç®—DCG
            dcg = 0
            for rank, item in enumerate(top_items, 1):
                if item in user_labels:
                    dcg += 1.0 / np.log2(rank + 1)
            
            # è®¡ç®—IDCG (ç†æƒ³æƒ…å†µ)
            idcg = sum(1.0 / np.log2(i + 1) for i in range(1, min(len(user_labels), 50) + 1))
            
            ndcg_sum += dcg / idcg if idcg > 0 else 0
    
    return ndcg_sum / total if total > 0 else 0.0
