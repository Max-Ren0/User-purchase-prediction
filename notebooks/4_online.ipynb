{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f4c559",
   "metadata": {},
   "source": [
    "# 🚀 线上推理与提交生成模块 (4_online.ipynb)\n",
    "\n",
    "## 📋 模块功能\n",
    "基于训练好的模型进行**线上推理**，为测试用户生成Top-30推荐结果并输出标准提交文件。\n",
    "\n",
    "## 🎯 核心功能\n",
    "1. **📊 全量统计重建**: 使用完整训练数据重新构建召回统计\n",
    "2. **🎯 测试用户推理**: 为测试集用户生成推荐候选\n",
    "3. **🤖 模型评分**: 加载训练好的排序模型进行精准打分\n",
    "4. **📄 提交文件生成**: 输出符合比赛要求的提交格式\n",
    "\n",
    "## 🔄 推理流程\n",
    "1. **数据准备**: 加载全量训练数据 (`train_sorted.parquet`)\n",
    "2. **统计重建**: 基于全量数据重新计算召回统计\n",
    "3. **候选生成**: 为测试用户 (`test_sorted.parquet`) 生成候选\n",
    "4. **模型打分**: 使用 `model_lgb.pkl` 进行排序（如可用）\n",
    "5. **结果输出**: 生成Top-30推荐列表\n",
    "\n",
    "## 🔧 输入文件\n",
    "- `train_sorted.parquet`: 完整训练数据（用于统计重建）\n",
    "- `test_sorted.parquet`: 测试用户数据\n",
    "- `model_lgb.pkl`: 训练好的排序模型（可选）\n",
    "\n",
    "## 📄 输出文件\n",
    "- `submit_long.csv`: 长格式提交文件 (user, item, score, rank)\n",
    "- `submit_wide.csv`: 宽格式提交文件 (每行一个用户，30列推荐)\n",
    "\n",
    "## ⚙️ 配置参数\n",
    "- **TOPK**: 推荐商品数量 (默认30)\n",
    "- **降级策略**: 模型不可用时使用 `pre_score` 排序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2047b3a",
   "metadata": {},
   "source": [
    "## 1️⃣ 环境配置与参数设置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c430b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LightGBM 可用\n",
      "🚀 线上推理模块启动\n",
      "📁 数据目录: ../x\n",
      "🎯 推荐数量: Top-30\n",
      "⏰ 推理时间: 2025-09-26 20:25:16\n",
      "\n",
      "🔍 检查数据文件...\n",
      "  ✅ train_sorted.parquet\n",
      "  ✅ test_sorted.parquet\n",
      "  ✅ item_attr.parquet\n",
      "  ✅ model_lgb.pkl (模型文件)\n",
      "✅ 环境配置完成\n",
      "train rows: 170699  test rows: 140380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 环境配置与依赖导入\n",
    "# =============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 机器学习相关\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LIGHTGBM = True\n",
    "    print(\"✅ LightGBM 可用\")\n",
    "except ImportError:\n",
    "    HAS_LIGHTGBM = False\n",
    "    print(\"⚠️  LightGBM 不可用\")\n",
    "\n",
    "# 配置参数\n",
    "OUTDIR = '../x'  # 数据目录\n",
    "TOPK = 30       # 推荐Top-K数量\n",
    "\n",
    "print(f'🚀 线上推理模块启动')\n",
    "print(f'📁 数据目录: {OUTDIR}')\n",
    "print(f'🎯 推荐数量: Top-{TOPK}')\n",
    "print(f'⏰ 推理时间: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "# 检查必要文件\n",
    "required_files = [\n",
    "    'train_sorted.parquet',\n",
    "    'test_sorted.parquet',\n",
    "    'item_attr.parquet'\n",
    "]\n",
    "\n",
    "optional_files = [\n",
    "    'model_lgb.pkl'\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 检查数据文件...\")\n",
    "for file in required_files:\n",
    "    if not os.path.exists(f'{OUTDIR}/{file}'):\n",
    "        raise FileNotFoundError(f\"❌ 缺少必要文件: {file}\")\n",
    "    print(f\"  ✅ {file}\")\n",
    "\n",
    "for file in optional_files:\n",
    "    if os.path.exists(f'{OUTDIR}/{file}'):\n",
    "        print(f\"  ✅ {file} (模型文件)\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  {file} (模型文件不存在，将使用pre_score)\")\n",
    "\n",
    "print(\"✅ 环境配置完成\")\n",
    "train = pd.read_parquet(f\"{OUTDIR}/train_sorted.parquet\")\n",
    "test  = pd.read_parquet(f'{OUTDIR}/test_sorted.parquet')\n",
    "item_attr = pd.read_parquet(f'{OUTDIR}/item_attr.parquet')\n",
    "print('train rows:', len(train), ' test rows:', len(test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50eb2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PARAMS = dict(\n",
    "    covisit_window=4, covisit_top_per_a=317,  # 贝叶斯优化: 4, 317\n",
    "    recent_k=4, cand_per_recent=69,          # 贝叶斯优化: 4, 69\n",
    "    tau_days=11,                             # 贝叶斯优化: 11\n",
    "    user_top_cates=3, user_top_stores=3,     # 保持原值\n",
    "    per_cate_pool=38, per_store_pool=96,     # 贝叶斯优化: 38, 96\n",
    "    pop_pool=4863, recall_cap=866,           # 贝叶斯优化: 4863, 866\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3763d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full-train stats built.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def time_decay(days, tau=14.0):\n",
    "    days = np.maximum(days, 0.0)\n",
    "    return np.exp(-days / float(tau))\n",
    "\n",
    "def build_rebuy_scores(df, tau_days=14):\n",
    "    g = df.copy()\n",
    "    ref = g.groupby('buyer_admin_id')['create_order_time'].transform('max')\n",
    "    g['days_ago'] = (ref - g['create_order_time']).dt.days.clip(lower=0)\n",
    "    g['score_rebuy'] = time_decay(g['days_ago'].to_numpy(), tau=tau_days)\n",
    "    return g.groupby(['buyer_admin_id','item_id'])['score_rebuy'].sum().reset_index()\n",
    "\n",
    "def build_covisit(df, W=3, topk=200):\n",
    "    base = df[['buyer_admin_id','item_id']].copy()\n",
    "    pairs = []\n",
    "    for lag in range(1, W+1):\n",
    "        t = base.copy()\n",
    "        t['item_b'] = t.groupby('buyer_admin_id')['item_id'].shift(-lag)\n",
    "        t = t.dropna().rename(columns={'item_id':'item_a'})\n",
    "        t['w'] = 1.0/lag\n",
    "        pairs.append(t[['item_a','item_b','w']])\n",
    "    if not pairs:\n",
    "        return pd.DataFrame(columns=['item_a','item_b','w'])\n",
    "    co = pd.concat(pairs, ignore_index=True)\n",
    "    co = co.groupby(['item_a','item_b'])['w'].sum().reset_index()\n",
    "    co['rn'] = co.groupby('item_a')['w'].rank(ascending=False, method='first')\n",
    "    return co[co['rn']<=topk].drop(columns='rn')\n",
    "\n",
    "def build_pop_pools(df, item_attr, pop_pool=2000):\n",
    "    pop = df.groupby('item_id').size().rename('pop').reset_index()\n",
    "    cate_pop = (df.merge(item_attr, on='item_id', how='left')\n",
    "                .groupby(['cate_id','item_id']).size().rename('pop').reset_index())\n",
    "    cate_pop['rn'] = cate_pop.groupby('cate_id')['pop'].rank(ascending=False, method='first')\n",
    "    store_pop = (df.merge(item_attr, on='item_id', how='left')\n",
    "                 .groupby(['store_id','item_id']).size().rename('pop').reset_index())\n",
    "    store_pop['rn'] = store_pop.groupby('store_id')['pop'].rank(ascending=False, method='first')\n",
    "    global_pop = pop.sort_values('pop', ascending=False).head(pop_pool)\n",
    "    return cate_pop, store_pop, global_pop\n",
    "\n",
    "rebuy = build_rebuy_scores(train, tau_days=PARAMS['tau_days'])\n",
    "covisit = build_covisit(train, W=PARAMS['covisit_window'], topk=PARAMS['covisit_top_per_a'])\n",
    "cate_pop, store_pop, global_pop = build_pop_pools(train, item_attr, pop_pool=PARAMS['pop_pool'])\n",
    "print('full-train stats built.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f232cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputed maps ready (online).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "P = PARAMS\n",
    "\n",
    "cov_neighbors = {}\n",
    "for a, g in covisit.groupby('item_a'):\n",
    "    sub = g[['item_b','w']].head(P['cand_per_recent']).to_numpy()\n",
    "    if len(sub):\n",
    "        cov_neighbors[int(a)] = (sub[:,0].astype('int64'), sub[:,1].astype('float32'))\n",
    "\n",
    "recent_map = (test.sort_values('create_order_time')\n",
    "              .groupby('buyer_admin_id')['item_id']\n",
    "              .apply(lambda s: s.tail(P['recent_k']).to_numpy('int64'))\n",
    "              ).to_dict()\n",
    "\n",
    "ua = test.merge(item_attr, on='item_id', how='left')\n",
    "user_topc = ua.groupby('buyer_admin_id')['cate_id']               .apply(lambda s: s.value_counts().head(P['user_top_cates']).index.to_numpy('int64')).to_dict()\n",
    "user_tops = ua.groupby('buyer_admin_id')['store_id']               .apply(lambda s: s.value_counts().head(P['user_top_stores']).index.to_numpy('int64')).to_dict()\n",
    "\n",
    "cate_top = {int(c): grp.loc[grp['rn']<=P['per_cate_pool'],'item_id'].to_numpy('int64')\n",
    "            for c, grp in cate_pop.groupby('cate_id')}\n",
    "store_top = {int(s): grp.loc[grp['rn']<=P['per_store_pool'],'item_id'].to_numpy('int64')\n",
    "             for s, grp in store_pop.groupby('store_id')}\n",
    "global_items = global_pop['item_id'].to_numpy('int64')\n",
    "\n",
    "rebuy_map = {}\n",
    "for uid, g in rebuy.groupby('buyer_admin_id'):\n",
    "    rebuy_map[int(uid)] = (g['item_id'].to_numpy('int64'), g['score_rebuy'].to_numpy('float32'))\n",
    "print('precomputed maps ready (online).')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d83251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_candidates_fast(uid,\n",
    "                          use_rebuy=True, use_covisit=True,\n",
    "                          use_cate_store=True, use_global=True):\n",
    "    cand = {}\n",
    "    if use_rebuy and uid in rebuy_map:\n",
    "        items, ws = rebuy_map[uid]\n",
    "        for it, w in zip(items, ws):\n",
    "            cand.setdefault(int(it), []).append(('rebuy', float(w)))\n",
    "    if use_covisit:\n",
    "        for a in recent_map.get(uid, []):\n",
    "            pair = cov_neighbors.get(int(a))\n",
    "            if pair is None: \n",
    "                continue\n",
    "            bs, ws = pair\n",
    "            for b, w in zip(bs, ws):\n",
    "                cand.setdefault(int(b), []).append(('covisit', float(w)))\n",
    "    if use_cate_store:\n",
    "        for c in user_topc.get(uid, []):\n",
    "            for it in cate_top.get(int(c), ()):\n",
    "                cand.setdefault(int(it), []).append(('cate_hot', 1.0))\n",
    "        for s in user_tops.get(uid, []):\n",
    "            for it in store_top.get(int(s), ()):\n",
    "                cand.setdefault(int(it), []).append(('store_hot', 1.0))\n",
    "    if use_global:\n",
    "        for it in global_items:\n",
    "            cand.setdefault(int(it), []).append(('global_pop', 1.0))\n",
    "\n",
    "    if not cand:\n",
    "        cols = ['buyer_admin_id','item_id','score_rebuy','score_covisit',\n",
    "                'is_cate_hot','is_store_hot','is_global_pop','src_count','pre_score']\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    rows = []\n",
    "    for it, srcs in cand.items():\n",
    "        srcset = set()\n",
    "        sr=sc=0.0; is_c=is_s=is_g=0\n",
    "        for tag, w in srcs:\n",
    "            srcset.add(tag)\n",
    "            if tag=='rebuy': sr=max(sr,w)\n",
    "            elif tag=='covisit': sc=max(sc,w)\n",
    "            elif tag=='cate_hot': is_c=1\n",
    "            elif tag=='store_hot': is_s=1\n",
    "            elif tag=='global_pop': is_g=1\n",
    "        rows.append((int(uid), int(it), sr, sc, is_c, is_s, is_g, len(srcset)))\n",
    "    df = pd.DataFrame(rows, columns=['buyer_admin_id','item_id','score_rebuy','score_covisit',\n",
    "                                     'is_cate_hot','is_store_hot','is_global_pop','src_count'])\n",
    "    df['pre_score'] = (df['score_rebuy'] + df['score_covisit']\n",
    "                       + 0.3*df['is_cate_hot'] + 0.3*df['is_store_hot'] + 0.1*df['is_global_pop'])\n",
    "    return df.sort_values('pre_score', ascending=False).head(PARAMS['recall_cap'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b70a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ../x/model_lgb.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 加同分布特征 + 载入模型（若不存在则退化 pre_score） ===\n",
    "try:\n",
    "    import joblib, os\n",
    "    model_path = os.path.join(OUTDIR, 'model_lgb.pkl')\n",
    "    model = joblib.load(model_path) if os.path.exists(model_path) else None\n",
    "    print('Loaded model:', model_path if model is not None else 'None (fallback to pre_score)')\n",
    "except Exception:\n",
    "    model = None\n",
    "    print('joblib not available; fallback to pre_score.')\n",
    "\n",
    "feat_cols = ['score_rebuy','score_covisit','is_cate_hot','is_store_hot','is_global_pop',\n",
    "             'src_count','user_hist_cnt','item_pop_cnt','pre_score']\n",
    "\n",
    "user_cnt_full = train.groupby('buyer_admin_id').size().rename('user_hist_cnt')\n",
    "item_cnt_full = train.groupby('item_id').size().rename('item_pop_cnt')\n",
    "\n",
    "def score_dataframe(cdf):\n",
    "    cdf = (cdf.merge(user_cnt_full, on='buyer_admin_id', how='left')\n",
    "              .merge(item_cnt_full, on='item_id', how='left')).fillna(0)\n",
    "    if (model is not None) and hasattr(model, 'predict_proba'):\n",
    "        cdf['score'] = model.predict_proba(cdf[feat_cols])[:,1]\n",
    "    else:\n",
    "        cdf['score'] = cdf['pre_score']\n",
    "    return cdf[['buyer_admin_id','item_id','score']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8204242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(19988) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- ../x/submit_long.csv\n",
      "- ../x/submit_wide.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 生成 Top30 提交 ===\n",
    "# 根据比赛要求：为每个用户的最后一条购买数据预测Top30商品\n",
    "\n",
    "print(\"🎯 开始为每个用户的最后一条购买记录生成推荐...\")\n",
    "\n",
    "# 获取每个用户的最后一条购买记录\n",
    "test['create_order_time'] = pd.to_datetime(test['create_order_time'])\n",
    "last_purchases = test.sort_values('create_order_time').groupby('buyer_admin_id').tail(1)\n",
    "print(f\"📊 需要预测的用户数: {len(last_purchases)}\")\n",
    "\n",
    "rows = []\n",
    "processed_users = 0\n",
    "\n",
    "for idx, row in last_purchases.iterrows():\n",
    "    uid = int(row['buyer_admin_id'])\n",
    "    \n",
    "    # 为这个用户生成候选商品\n",
    "    cdf = build_candidates_fast(uid, True, True, True, True)\n",
    "    \n",
    "    if len(cdf) == 0:\n",
    "        print(f\"⚠️ 用户 {uid} 没有候选商品\")\n",
    "        continue\n",
    "    \n",
    "    # 使用模型评分\n",
    "    sdf = score_dataframe(cdf).sort_values('score', ascending=False).head(TOPK)\n",
    "    \n",
    "    # 添加排名\n",
    "    rank = np.arange(1, len(sdf) + 1)\n",
    "    sdf = sdf.assign(rank=rank)\n",
    "    \n",
    "    rows.append(sdf)\n",
    "    processed_users += 1\n",
    "    \n",
    "    if processed_users % 1000 == 0:\n",
    "        print(f\"✅ 已处理 {processed_users}/{len(last_purchases)} 用户\")\n",
    "\n",
    "print(f\"✅ 完成推荐生成，共处理 {processed_users} 个用户\")\n",
    "\n",
    "# 合并所有结果\n",
    "submit_long = pd.concat(rows, ignore_index=True) if rows else \\\n",
    "    pd.DataFrame(columns=['buyer_admin_id','item_id','score','rank'])\n",
    "\n",
    "print(f\"📊 最终提交数据: {len(submit_long)} 条推荐记录\")\n",
    "\n",
    "# 转换为宽格式\n",
    "def to_wide(df, topk=TOPK):\n",
    "    df = df.sort_values(['buyer_admin_id','rank'])\n",
    "    items = df.groupby('buyer_admin_id')['item_id'].apply(list).reset_index()\n",
    "    items['item_list'] = items['item_id'].apply(lambda L: (L + [None]*topk)[:topk])\n",
    "    out = items[['buyer_admin_id','item_list']].copy()\n",
    "    for i in range(topk):\n",
    "        out[f'item_{i+1}'] = out['item_list'].apply(lambda L: L[i])\n",
    "    return out.drop(columns=['item_list'])\n",
    "\n",
    "submit_wide = to_wide(submit_long[['buyer_admin_id','item_id','rank']], TOPK) if len(submit_long) else \\\n",
    "    pd.DataFrame(columns=['buyer_admin_id'] + [f'item_{i}' for i in range(1, TOPK+1)])\n",
    "\n",
    "# 保存文件\n",
    "submit_long.to_csv(f'{OUTDIR}/submit_long.csv', index=False)\n",
    "submit_wide.to_csv(f'{OUTDIR}/submit_wide.csv', index=False)\n",
    "\n",
    "print('💾 文件已保存:')\n",
    "print(f'- {OUTDIR}/submit_long.csv ({len(submit_long)} 条记录)')\n",
    "print(f'- {OUTDIR}/submit_wide.csv ({len(submit_wide)} 用户)')\n",
    "\n",
    "# 验证结果\n",
    "print(f\"\\n📊 提交结果验证:\")\n",
    "print(f\"- 用户数: {submit_wide['buyer_admin_id'].nunique()}\")\n",
    "print(f\"- 平均每用户推荐数: {submit_long.groupby('buyer_admin_id').size().mean():.1f}\")\n",
    "print(f\"- 推荐商品总数: {submit_long['item_id'].nunique()}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
