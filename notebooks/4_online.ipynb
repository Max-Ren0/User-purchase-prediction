{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1f4c559",
   "metadata": {},
   "source": [
    "# ğŸš€ çº¿ä¸Šæ¨ç†ä¸æäº¤ç”Ÿæˆæ¨¡å— (4_online.ipynb)\n",
    "\n",
    "## ğŸ“‹ æ¨¡å—åŠŸèƒ½\n",
    "åŸºäºè®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œ**çº¿ä¸Šæ¨ç†**ï¼Œä¸ºæµ‹è¯•ç”¨æˆ·ç”ŸæˆTop-30æ¨èç»“æœå¹¶è¾“å‡ºæ ‡å‡†æäº¤æ–‡ä»¶ã€‚\n",
    "\n",
    "## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½\n",
    "1. **ğŸ“Š å…¨é‡ç»Ÿè®¡é‡å»º**: ä½¿ç”¨å®Œæ•´è®­ç»ƒæ•°æ®é‡æ–°æ„å»ºå¬å›ç»Ÿè®¡\n",
    "2. **ğŸ¯ æµ‹è¯•ç”¨æˆ·æ¨ç†**: ä¸ºæµ‹è¯•é›†ç”¨æˆ·ç”Ÿæˆæ¨èå€™é€‰\n",
    "3. **ğŸ¤– æ¨¡å‹è¯„åˆ†**: åŠ è½½è®­ç»ƒå¥½çš„æ’åºæ¨¡å‹è¿›è¡Œç²¾å‡†æ‰“åˆ†\n",
    "4. **ğŸ“„ æäº¤æ–‡ä»¶ç”Ÿæˆ**: è¾“å‡ºç¬¦åˆæ¯”èµ›è¦æ±‚çš„æäº¤æ ¼å¼\n",
    "\n",
    "## ğŸ”„ æ¨ç†æµç¨‹\n",
    "1. **æ•°æ®å‡†å¤‡**: åŠ è½½å…¨é‡è®­ç»ƒæ•°æ® (`train_sorted.parquet`)\n",
    "2. **ç»Ÿè®¡é‡å»º**: åŸºäºå…¨é‡æ•°æ®é‡æ–°è®¡ç®—å¬å›ç»Ÿè®¡\n",
    "3. **å€™é€‰ç”Ÿæˆ**: ä¸ºæµ‹è¯•ç”¨æˆ· (`test_sorted.parquet`) ç”Ÿæˆå€™é€‰\n",
    "4. **æ¨¡å‹æ‰“åˆ†**: ä½¿ç”¨ `model_lgb.pkl` è¿›è¡Œæ’åºï¼ˆå¦‚å¯ç”¨ï¼‰\n",
    "5. **ç»“æœè¾“å‡º**: ç”ŸæˆTop-30æ¨èåˆ—è¡¨\n",
    "\n",
    "## ğŸ”§ è¾“å…¥æ–‡ä»¶\n",
    "- `train_sorted.parquet`: å®Œæ•´è®­ç»ƒæ•°æ®ï¼ˆç”¨äºç»Ÿè®¡é‡å»ºï¼‰\n",
    "- `test_sorted.parquet`: æµ‹è¯•ç”¨æˆ·æ•°æ®\n",
    "- `model_lgb.pkl`: è®­ç»ƒå¥½çš„æ’åºæ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "## ğŸ“„ è¾“å‡ºæ–‡ä»¶\n",
    "- `submit_long.csv`: é•¿æ ¼å¼æäº¤æ–‡ä»¶ (user, item, score, rank)\n",
    "- `submit_wide.csv`: å®½æ ¼å¼æäº¤æ–‡ä»¶ (æ¯è¡Œä¸€ä¸ªç”¨æˆ·ï¼Œ30åˆ—æ¨è)\n",
    "\n",
    "## âš™ï¸ é…ç½®å‚æ•°\n",
    "- **TOPK**: æ¨èå•†å“æ•°é‡ (é»˜è®¤30)\n",
    "- **é™çº§ç­–ç•¥**: æ¨¡å‹ä¸å¯ç”¨æ—¶ä½¿ç”¨ `pre_score` æ’åº"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2047b3a",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒé…ç½®ä¸å‚æ•°è®¾ç½®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c430b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LightGBM å¯ç”¨\n",
      "ğŸš€ çº¿ä¸Šæ¨ç†æ¨¡å—å¯åŠ¨\n",
      "ğŸ“ æ•°æ®ç›®å½•: ../x\n",
      "ğŸ¯ æ¨èæ•°é‡: Top-30\n",
      "â° æ¨ç†æ—¶é—´: 2025-09-26 20:25:16\n",
      "\n",
      "ğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...\n",
      "  âœ… train_sorted.parquet\n",
      "  âœ… test_sorted.parquet\n",
      "  âœ… item_attr.parquet\n",
      "  âœ… model_lgb.pkl (æ¨¡å‹æ–‡ä»¶)\n",
      "âœ… ç¯å¢ƒé…ç½®å®Œæˆ\n",
      "train rows: 170699  test rows: 140380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ç¯å¢ƒé…ç½®ä¸ä¾èµ–å¯¼å…¥\n",
    "# =============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æœºå™¨å­¦ä¹ ç›¸å…³\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LIGHTGBM = True\n",
    "    print(\"âœ… LightGBM å¯ç”¨\")\n",
    "except ImportError:\n",
    "    HAS_LIGHTGBM = False\n",
    "    print(\"âš ï¸  LightGBM ä¸å¯ç”¨\")\n",
    "\n",
    "# é…ç½®å‚æ•°\n",
    "OUTDIR = '../x'  # æ•°æ®ç›®å½•\n",
    "TOPK = 30       # æ¨èTop-Kæ•°é‡\n",
    "\n",
    "print(f'ğŸš€ çº¿ä¸Šæ¨ç†æ¨¡å—å¯åŠ¨')\n",
    "print(f'ğŸ“ æ•°æ®ç›®å½•: {OUTDIR}')\n",
    "print(f'ğŸ¯ æ¨èæ•°é‡: Top-{TOPK}')\n",
    "print(f'â° æ¨ç†æ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦æ–‡ä»¶\n",
    "required_files = [\n",
    "    'train_sorted.parquet',\n",
    "    'test_sorted.parquet',\n",
    "    'item_attr.parquet'\n",
    "]\n",
    "\n",
    "optional_files = [\n",
    "    'model_lgb.pkl'\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶...\")\n",
    "for file in required_files:\n",
    "    if not os.path.exists(f'{OUTDIR}/{file}'):\n",
    "        raise FileNotFoundError(f\"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file}\")\n",
    "    print(f\"  âœ… {file}\")\n",
    "\n",
    "for file in optional_files:\n",
    "    if os.path.exists(f'{OUTDIR}/{file}'):\n",
    "        print(f\"  âœ… {file} (æ¨¡å‹æ–‡ä»¶)\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸  {file} (æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨pre_score)\")\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®å®Œæˆ\")\n",
    "train = pd.read_parquet(f\"{OUTDIR}/train_sorted.parquet\")\n",
    "test  = pd.read_parquet(f'{OUTDIR}/test_sorted.parquet')\n",
    "item_attr = pd.read_parquet(f'{OUTDIR}/item_attr.parquet')\n",
    "print('train rows:', len(train), ' test rows:', len(test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50eb2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PARAMS = dict(\n",
    "    covisit_window=4, covisit_top_per_a=317,  # è´å¶æ–¯ä¼˜åŒ–: 4, 317\n",
    "    recent_k=4, cand_per_recent=69,          # è´å¶æ–¯ä¼˜åŒ–: 4, 69\n",
    "    tau_days=11,                             # è´å¶æ–¯ä¼˜åŒ–: 11\n",
    "    user_top_cates=3, user_top_stores=3,     # ä¿æŒåŸå€¼\n",
    "    per_cate_pool=38, per_store_pool=96,     # è´å¶æ–¯ä¼˜åŒ–: 38, 96\n",
    "    pop_pool=4863, recall_cap=866,           # è´å¶æ–¯ä¼˜åŒ–: 4863, 866\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3763d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full-train stats built.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def time_decay(days, tau=14.0):\n",
    "    days = np.maximum(days, 0.0)\n",
    "    return np.exp(-days / float(tau))\n",
    "\n",
    "def build_rebuy_scores(df, tau_days=14):\n",
    "    g = df.copy()\n",
    "    ref = g.groupby('buyer_admin_id')['create_order_time'].transform('max')\n",
    "    g['days_ago'] = (ref - g['create_order_time']).dt.days.clip(lower=0)\n",
    "    g['score_rebuy'] = time_decay(g['days_ago'].to_numpy(), tau=tau_days)\n",
    "    return g.groupby(['buyer_admin_id','item_id'])['score_rebuy'].sum().reset_index()\n",
    "\n",
    "def build_covisit(df, W=3, topk=200):\n",
    "    base = df[['buyer_admin_id','item_id']].copy()\n",
    "    pairs = []\n",
    "    for lag in range(1, W+1):\n",
    "        t = base.copy()\n",
    "        t['item_b'] = t.groupby('buyer_admin_id')['item_id'].shift(-lag)\n",
    "        t = t.dropna().rename(columns={'item_id':'item_a'})\n",
    "        t['w'] = 1.0/lag\n",
    "        pairs.append(t[['item_a','item_b','w']])\n",
    "    if not pairs:\n",
    "        return pd.DataFrame(columns=['item_a','item_b','w'])\n",
    "    co = pd.concat(pairs, ignore_index=True)\n",
    "    co = co.groupby(['item_a','item_b'])['w'].sum().reset_index()\n",
    "    co['rn'] = co.groupby('item_a')['w'].rank(ascending=False, method='first')\n",
    "    return co[co['rn']<=topk].drop(columns='rn')\n",
    "\n",
    "def build_pop_pools(df, item_attr, pop_pool=2000):\n",
    "    pop = df.groupby('item_id').size().rename('pop').reset_index()\n",
    "    cate_pop = (df.merge(item_attr, on='item_id', how='left')\n",
    "                .groupby(['cate_id','item_id']).size().rename('pop').reset_index())\n",
    "    cate_pop['rn'] = cate_pop.groupby('cate_id')['pop'].rank(ascending=False, method='first')\n",
    "    store_pop = (df.merge(item_attr, on='item_id', how='left')\n",
    "                 .groupby(['store_id','item_id']).size().rename('pop').reset_index())\n",
    "    store_pop['rn'] = store_pop.groupby('store_id')['pop'].rank(ascending=False, method='first')\n",
    "    global_pop = pop.sort_values('pop', ascending=False).head(pop_pool)\n",
    "    return cate_pop, store_pop, global_pop\n",
    "\n",
    "rebuy = build_rebuy_scores(train, tau_days=PARAMS['tau_days'])\n",
    "covisit = build_covisit(train, W=PARAMS['covisit_window'], topk=PARAMS['covisit_top_per_a'])\n",
    "cate_pop, store_pop, global_pop = build_pop_pools(train, item_attr, pop_pool=PARAMS['pop_pool'])\n",
    "print('full-train stats built.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f232cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputed maps ready (online).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "P = PARAMS\n",
    "\n",
    "cov_neighbors = {}\n",
    "for a, g in covisit.groupby('item_a'):\n",
    "    sub = g[['item_b','w']].head(P['cand_per_recent']).to_numpy()\n",
    "    if len(sub):\n",
    "        cov_neighbors[int(a)] = (sub[:,0].astype('int64'), sub[:,1].astype('float32'))\n",
    "\n",
    "recent_map = (test.sort_values('create_order_time')\n",
    "              .groupby('buyer_admin_id')['item_id']\n",
    "              .apply(lambda s: s.tail(P['recent_k']).to_numpy('int64'))\n",
    "              ).to_dict()\n",
    "\n",
    "ua = test.merge(item_attr, on='item_id', how='left')\n",
    "user_topc = ua.groupby('buyer_admin_id')['cate_id']               .apply(lambda s: s.value_counts().head(P['user_top_cates']).index.to_numpy('int64')).to_dict()\n",
    "user_tops = ua.groupby('buyer_admin_id')['store_id']               .apply(lambda s: s.value_counts().head(P['user_top_stores']).index.to_numpy('int64')).to_dict()\n",
    "\n",
    "cate_top = {int(c): grp.loc[grp['rn']<=P['per_cate_pool'],'item_id'].to_numpy('int64')\n",
    "            for c, grp in cate_pop.groupby('cate_id')}\n",
    "store_top = {int(s): grp.loc[grp['rn']<=P['per_store_pool'],'item_id'].to_numpy('int64')\n",
    "             for s, grp in store_pop.groupby('store_id')}\n",
    "global_items = global_pop['item_id'].to_numpy('int64')\n",
    "\n",
    "rebuy_map = {}\n",
    "for uid, g in rebuy.groupby('buyer_admin_id'):\n",
    "    rebuy_map[int(uid)] = (g['item_id'].to_numpy('int64'), g['score_rebuy'].to_numpy('float32'))\n",
    "print('precomputed maps ready (online).')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d83251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_candidates_fast(uid,\n",
    "                          use_rebuy=True, use_covisit=True,\n",
    "                          use_cate_store=True, use_global=True):\n",
    "    cand = {}\n",
    "    if use_rebuy and uid in rebuy_map:\n",
    "        items, ws = rebuy_map[uid]\n",
    "        for it, w in zip(items, ws):\n",
    "            cand.setdefault(int(it), []).append(('rebuy', float(w)))\n",
    "    if use_covisit:\n",
    "        for a in recent_map.get(uid, []):\n",
    "            pair = cov_neighbors.get(int(a))\n",
    "            if pair is None: \n",
    "                continue\n",
    "            bs, ws = pair\n",
    "            for b, w in zip(bs, ws):\n",
    "                cand.setdefault(int(b), []).append(('covisit', float(w)))\n",
    "    if use_cate_store:\n",
    "        for c in user_topc.get(uid, []):\n",
    "            for it in cate_top.get(int(c), ()):\n",
    "                cand.setdefault(int(it), []).append(('cate_hot', 1.0))\n",
    "        for s in user_tops.get(uid, []):\n",
    "            for it in store_top.get(int(s), ()):\n",
    "                cand.setdefault(int(it), []).append(('store_hot', 1.0))\n",
    "    if use_global:\n",
    "        for it in global_items:\n",
    "            cand.setdefault(int(it), []).append(('global_pop', 1.0))\n",
    "\n",
    "    if not cand:\n",
    "        cols = ['buyer_admin_id','item_id','score_rebuy','score_covisit',\n",
    "                'is_cate_hot','is_store_hot','is_global_pop','src_count','pre_score']\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    rows = []\n",
    "    for it, srcs in cand.items():\n",
    "        srcset = set()\n",
    "        sr=sc=0.0; is_c=is_s=is_g=0\n",
    "        for tag, w in srcs:\n",
    "            srcset.add(tag)\n",
    "            if tag=='rebuy': sr=max(sr,w)\n",
    "            elif tag=='covisit': sc=max(sc,w)\n",
    "            elif tag=='cate_hot': is_c=1\n",
    "            elif tag=='store_hot': is_s=1\n",
    "            elif tag=='global_pop': is_g=1\n",
    "        rows.append((int(uid), int(it), sr, sc, is_c, is_s, is_g, len(srcset)))\n",
    "    df = pd.DataFrame(rows, columns=['buyer_admin_id','item_id','score_rebuy','score_covisit',\n",
    "                                     'is_cate_hot','is_store_hot','is_global_pop','src_count'])\n",
    "    df['pre_score'] = (df['score_rebuy'] + df['score_covisit']\n",
    "                       + 0.3*df['is_cate_hot'] + 0.3*df['is_store_hot'] + 0.1*df['is_global_pop'])\n",
    "    return df.sort_values('pre_score', ascending=False).head(PARAMS['recall_cap'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b70a3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ../x/model_lgb.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === åŠ åŒåˆ†å¸ƒç‰¹å¾ + è½½å…¥æ¨¡å‹ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™é€€åŒ– pre_scoreï¼‰ ===\n",
    "try:\n",
    "    import joblib, os\n",
    "    model_path = os.path.join(OUTDIR, 'model_lgb.pkl')\n",
    "    model = joblib.load(model_path) if os.path.exists(model_path) else None\n",
    "    print('Loaded model:', model_path if model is not None else 'None (fallback to pre_score)')\n",
    "except Exception:\n",
    "    model = None\n",
    "    print('joblib not available; fallback to pre_score.')\n",
    "\n",
    "feat_cols = ['score_rebuy','score_covisit','is_cate_hot','is_store_hot','is_global_pop',\n",
    "             'src_count','user_hist_cnt','item_pop_cnt','pre_score']\n",
    "\n",
    "user_cnt_full = train.groupby('buyer_admin_id').size().rename('user_hist_cnt')\n",
    "item_cnt_full = train.groupby('item_id').size().rename('item_pop_cnt')\n",
    "\n",
    "def score_dataframe(cdf):\n",
    "    cdf = (cdf.merge(user_cnt_full, on='buyer_admin_id', how='left')\n",
    "              .merge(item_cnt_full, on='item_id', how='left')).fillna(0)\n",
    "    if (model is not None) and hasattr(model, 'predict_proba'):\n",
    "        cdf['score'] = model.predict_proba(cdf[feat_cols])[:,1]\n",
    "    else:\n",
    "        cdf['score'] = cdf['pre_score']\n",
    "    return cdf[['buyer_admin_id','item_id','score']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8204242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(19988) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- ../x/submit_long.csv\n",
      "- ../x/submit_wide.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === ç”Ÿæˆ Top30 æäº¤ ===\n",
    "# æ ¹æ®æ¯”èµ›è¦æ±‚ï¼šä¸ºæ¯ä¸ªç”¨æˆ·çš„æœ€åä¸€æ¡è´­ä¹°æ•°æ®é¢„æµ‹Top30å•†å“\n",
    "\n",
    "print(\"ğŸ¯ å¼€å§‹ä¸ºæ¯ä¸ªç”¨æˆ·çš„æœ€åä¸€æ¡è´­ä¹°è®°å½•ç”Ÿæˆæ¨è...\")\n",
    "\n",
    "# è·å–æ¯ä¸ªç”¨æˆ·çš„æœ€åä¸€æ¡è´­ä¹°è®°å½•\n",
    "test['create_order_time'] = pd.to_datetime(test['create_order_time'])\n",
    "last_purchases = test.sort_values('create_order_time').groupby('buyer_admin_id').tail(1)\n",
    "print(f\"ğŸ“Š éœ€è¦é¢„æµ‹çš„ç”¨æˆ·æ•°: {len(last_purchases)}\")\n",
    "\n",
    "rows = []\n",
    "processed_users = 0\n",
    "\n",
    "for idx, row in last_purchases.iterrows():\n",
    "    uid = int(row['buyer_admin_id'])\n",
    "    \n",
    "    # ä¸ºè¿™ä¸ªç”¨æˆ·ç”Ÿæˆå€™é€‰å•†å“\n",
    "    cdf = build_candidates_fast(uid, True, True, True, True)\n",
    "    \n",
    "    if len(cdf) == 0:\n",
    "        print(f\"âš ï¸ ç”¨æˆ· {uid} æ²¡æœ‰å€™é€‰å•†å“\")\n",
    "        continue\n",
    "    \n",
    "    # ä½¿ç”¨æ¨¡å‹è¯„åˆ†\n",
    "    sdf = score_dataframe(cdf).sort_values('score', ascending=False).head(TOPK)\n",
    "    \n",
    "    # æ·»åŠ æ’å\n",
    "    rank = np.arange(1, len(sdf) + 1)\n",
    "    sdf = sdf.assign(rank=rank)\n",
    "    \n",
    "    rows.append(sdf)\n",
    "    processed_users += 1\n",
    "    \n",
    "    if processed_users % 1000 == 0:\n",
    "        print(f\"âœ… å·²å¤„ç† {processed_users}/{len(last_purchases)} ç”¨æˆ·\")\n",
    "\n",
    "print(f\"âœ… å®Œæˆæ¨èç”Ÿæˆï¼Œå…±å¤„ç† {processed_users} ä¸ªç”¨æˆ·\")\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰ç»“æœ\n",
    "submit_long = pd.concat(rows, ignore_index=True) if rows else \\\n",
    "    pd.DataFrame(columns=['buyer_admin_id','item_id','score','rank'])\n",
    "\n",
    "print(f\"ğŸ“Š æœ€ç»ˆæäº¤æ•°æ®: {len(submit_long)} æ¡æ¨èè®°å½•\")\n",
    "\n",
    "# è½¬æ¢ä¸ºå®½æ ¼å¼\n",
    "def to_wide(df, topk=TOPK):\n",
    "    df = df.sort_values(['buyer_admin_id','rank'])\n",
    "    items = df.groupby('buyer_admin_id')['item_id'].apply(list).reset_index()\n",
    "    items['item_list'] = items['item_id'].apply(lambda L: (L + [None]*topk)[:topk])\n",
    "    out = items[['buyer_admin_id','item_list']].copy()\n",
    "    for i in range(topk):\n",
    "        out[f'item_{i+1}'] = out['item_list'].apply(lambda L: L[i])\n",
    "    return out.drop(columns=['item_list'])\n",
    "\n",
    "submit_wide = to_wide(submit_long[['buyer_admin_id','item_id','rank']], TOPK) if len(submit_long) else \\\n",
    "    pd.DataFrame(columns=['buyer_admin_id'] + [f'item_{i}' for i in range(1, TOPK+1)])\n",
    "\n",
    "# ä¿å­˜æ–‡ä»¶\n",
    "submit_long.to_csv(f'{OUTDIR}/submit_long.csv', index=False)\n",
    "submit_wide.to_csv(f'{OUTDIR}/submit_wide.csv', index=False)\n",
    "\n",
    "print('ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜:')\n",
    "print(f'- {OUTDIR}/submit_long.csv ({len(submit_long)} æ¡è®°å½•)')\n",
    "print(f'- {OUTDIR}/submit_wide.csv ({len(submit_wide)} ç”¨æˆ·)')\n",
    "\n",
    "# éªŒè¯ç»“æœ\n",
    "print(f\"\\nğŸ“Š æäº¤ç»“æœéªŒè¯:\")\n",
    "print(f\"- ç”¨æˆ·æ•°: {submit_wide['buyer_admin_id'].nunique()}\")\n",
    "print(f\"- å¹³å‡æ¯ç”¨æˆ·æ¨èæ•°: {submit_long.groupby('buyer_admin_id').size().mean():.1f}\")\n",
    "print(f\"- æ¨èå•†å“æ€»æ•°: {submit_long['item_id'].nunique()}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
