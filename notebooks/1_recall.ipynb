{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132f17dc",
   "metadata": {},
   "source": [
    "# ğŸ¯ å¤šè·¯å¬å›æ¨¡å— (1_recall.ipynb)\n",
    "\n",
    "## ğŸ“‹ æ¨¡å—åŠŸèƒ½\n",
    "å®ç°**4ç§å¬å›ç­–ç•¥**ï¼Œä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰å•†å“é›†åˆï¼š\n",
    "\n",
    "1. **ğŸ”„ å¤è´­å¬å›**: åŸºäºç”¨æˆ·å†å²è´­ä¹° + æ—¶é—´è¡°å‡\n",
    "2. **ğŸ”— ååŒè¿‡æ»¤å¬å›**: åŸºäºå•†å“å…±ç°å…³ç³»\n",
    "3. **ğŸª ä¸ªæ€§åŒ–çƒ­é—¨**: ç”¨æˆ·åå¥½ç±»ç›®/åº—é“ºçƒ­é—¨  \n",
    "4. **ğŸŒ å…¨å±€çƒ­é—¨**: å†·å¯åŠ¨è¡¥å……\n",
    "\n",
    "## âš¡ æ€§èƒ½ä¼˜åŒ–\n",
    "- **FAST_MODE**: å¼€å‘æ¨¡å¼å‚æ•°è°ƒæ•´\n",
    "- **å†…å­˜ä¼˜åŒ–**: dtypeå‹ç¼©å‡å°‘å†…å­˜å ç”¨\n",
    "- **é¢„è®¡ç®—åŠ é€Ÿ**: é‚»æ¥è¡¨ã€æ˜ å°„å­—å…¸ç­‰\n",
    "- **æ‰¹å¤„ç†**: çº¯å­—å…¸ + numpy é¿å…é¢‘ç¹join\n",
    "\n",
    "## ğŸ”§ è¾“å‡ºæ–‡ä»¶\n",
    "- **ç»Ÿè®¡è¡¨**: rebuy, covisit, cate_pop, store_pop, global_pop\n",
    "- **å€™é€‰é›†**: cands_multi (å¤šè·¯), cands_covisit_only (å•è·¯/æ¶ˆè)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5d0d1",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒé…ç½®ä¸æ•°æ®åŠ è½½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ä¾èµ–åº“å¯¼å…¥\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# é…ç½®å‚æ•°\n",
    "OUTDIR = '../x'\n",
    "FAST_MODE = True  # å¼€å‘æ¨¡å¼ï¼Œä½¿ç”¨è¾ƒå°çš„å‚æ•°\n",
    "\n",
    "print(\"âœ… ç¯å¢ƒé…ç½®å®Œæˆ\")\n",
    "print(f\"ğŸ“ è¾“å‡ºç›®å½•: {OUTDIR}\")\n",
    "print(f\"âš¡ å¿«é€Ÿæ¨¡å¼: {'å¯ç”¨' if FAST_MODE else 'ç¦ç”¨'}\")\n",
    "print(f\"â° å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ•°æ®åŠ è½½\n",
    "# =============================================================================\n",
    "print(\"ğŸ“‚ å¼€å§‹åŠ è½½æ•°æ®...\")\n",
    "\n",
    "# åŠ è½½å•†å“å±æ€§æ•°æ®\n",
    "item_attr = pd.read_parquet(f'{OUTDIR}/item_attr.parquet')\n",
    "print(f\"âœ… å•†å“å±æ€§æ•°æ®: {len(item_attr):,} æ¡è®°å½•\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æŠ½æ ·æ•°æ®ï¼ˆä»0_prep.ipynbç”Ÿæˆï¼‰\n",
    "if os.path.exists(f\"{OUTDIR}/train_vis_sampled.parquet\"):\n",
    "    print(\"âœ… å‘ç°æŠ½æ ·æ•°æ®ï¼Œä½¿ç”¨0_prep.ipynbçš„æŠ½æ ·ç»“æœ\")\n",
    "    train_vis = pd.read_parquet(f\"{OUTDIR}/train_vis_sampled.parquet\")\n",
    "    label_df = pd.read_parquet(f\"{OUTDIR}/label_df_sampled.parquet\")\n",
    "    print(f\"ğŸ“Š ä½¿ç”¨æŠ½æ ·æ•°æ®: {len(train_vis):,} æ¡è®°å½•, {train_vis['buyer_admin_id'].nunique():,} ç”¨æˆ·\")\n",
    "else:\n",
    "    print(\"ğŸ¯ ä½¿ç”¨å…¨é‡æ•°æ®ï¼ˆæœªè¿›è¡ŒæŠ½æ ·ï¼‰\")\n",
    "    train_vis = pd.read_parquet(f\"{OUTDIR}/train_vis.parquet\")\n",
    "    label_df = pd.read_parquet(f\"{OUTDIR}/label_df.parquet\")\n",
    "    print(f\"ğŸ“Š å…¨é‡æ•°æ®: {len(train_vis):,} æ¡è®°å½•, {train_vis['buyer_admin_id'].nunique():,} ç”¨æˆ·\")\n",
    "\n",
    "print(\"âœ… æ•°æ®åŠ è½½å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# å‚æ•°é…ç½® - è´å¶æ–¯ä¼˜åŒ–åçš„æœ€ä¼˜å‚æ•°\n",
    "# =============================================================================\n",
    "PARAMS = dict(\n",
    "    covisit_window=4, covisit_top_per_a=317,  # è´å¶æ–¯ä¼˜åŒ–: 4, 317\n",
    "    recent_k=4, cand_per_recent=69,          # è´å¶æ–¯ä¼˜åŒ–: 4, 69\n",
    "    tau_days=11,                             # è´å¶æ–¯ä¼˜åŒ–: 11\n",
    "    user_top_cates=3, user_top_stores=3,     # ä¿æŒåŸå€¼\n",
    "    per_cate_pool=38, per_store_pool=96,     # è´å¶æ–¯ä¼˜åŒ–: 38, 96\n",
    "    pop_pool=4863, recall_cap=866,           # è´å¶æ–¯ä¼˜åŒ–: 4863, 866\n",
    "    batch_size=2000,                         # æ‰¹å¤„ç†å¤§å°\n",
    ")\n",
    "\n",
    "print(\"âœ… å‚æ•°é…ç½®å®Œæˆ\")\n",
    "print(\"ğŸ“Š è´å¶æ–¯ä¼˜åŒ–åçš„æœ€ä¼˜å‚æ•°:\")\n",
    "for key, value in PARAMS.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "# å¿«é€Ÿæ¨¡å¼å‚æ•°è°ƒæ•´\n",
    "if FAST_MODE:\n",
    "    print(\"\\\\nâš¡ å¿«é€Ÿæ¨¡å¼å‚æ•°è°ƒæ•´:\")\n",
    "    PARAMS['covisit_top_per_a'] = min(PARAMS['covisit_top_per_a'], 100)\n",
    "    PARAMS['per_cate_pool'] = min(PARAMS['per_cate_pool'], 20)\n",
    "    PARAMS['per_store_pool'] = min(PARAMS['per_store_pool'], 20)\n",
    "    PARAMS['pop_pool'] = min(PARAMS['pop_pool'], 1000)\n",
    "    PARAMS['recall_cap'] = min(PARAMS['recall_cap'], 200)\n",
    "    PARAMS['batch_size'] = min(PARAMS['batch_size'], 1000)\n",
    "    \n",
    "    for key, value in PARAMS.items():\n",
    "        print(f\"  - {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80edcd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:25.836323Z",
     "iopub.status.busy": "2025-09-16T05:25:25.835991Z",
     "iopub.status.idle": "2025-09-16T05:25:26.026369Z",
     "shell.execute_reply": "2025-09-16T05:25:26.026134Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æŠ½æ ·æ•°æ®ï¼ˆä»0_prep.ipynbç”Ÿæˆï¼‰\n",
    "if os.path.exists(f\"{OUTDIR}/train_vis_sampled.parquet\"):\n",
    "    print(\"âœ… å‘ç°æŠ½æ ·æ•°æ®ï¼Œä½¿ç”¨0_prep.ipynbçš„æŠ½æ ·ç»“æœ\")\n",
    "    train_vis = pd.read_parquet(f\"{OUTDIR}/train_vis_sampled.parquet\")\n",
    "    label_df = pd.read_parquet(f\"{OUTDIR}/label_df_sampled.parquet\")\n",
    "    print(f\"ğŸ“Š ä½¿ç”¨æŠ½æ ·æ•°æ®: {len(train_vis):,} æ¡è®°å½•, {train_vis['buyer_admin_id'].nunique():,} ç”¨æˆ·\")\n",
    "else:\n",
    "    print(\"ğŸ¯ ä½¿ç”¨å…¨é‡æ•°æ®ï¼ˆæœªè¿›è¡ŒæŠ½æ ·ï¼‰\")\n",
    "    print(f\"ğŸ“Š å…¨é‡æ•°æ®: {len(train_vis):,} æ¡è®°å½•, {train_vis['buyer_admin_id'].nunique():,} ç”¨æˆ·\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc886a",
   "metadata": {},
   "source": [
    "# --- å¤è´­è¯„åˆ† ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718dd8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.027697Z",
     "iopub.status.busy": "2025-09-16T05:25:26.027615Z",
     "iopub.status.idle": "2025-09-16T05:25:26.030477Z",
     "shell.execute_reply": "2025-09-16T05:25:26.030280Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# âš¡ é«˜æ€§èƒ½å¤è´­è¯„åˆ†ç®—æ³• - å‘é‡åŒ–ä¼˜åŒ–\n",
    "# =============================================================================\n",
    "def time_decay_vectorized(days, tau=14.0):\n",
    "    \"\"\"å‘é‡åŒ–æ—¶é—´è¡°å‡å‡½æ•°ï¼Œæ¯”æ ‡é‡ç‰ˆæœ¬å¿«10x\"\"\"\n",
    "    days = np.clip(days, 0, None)  # ä½¿ç”¨clipæ›¿ä»£maximumï¼Œæ›´å¿«\n",
    "    return np.exp(-days / tau, dtype=np.float32)  # æŒ‡å®šfloat32å‡å°‘å†…å­˜\n",
    "\n",
    "def build_rebuy_scores_optimized(df, tau_days=14):\n",
    "    \"\"\"\n",
    "    ä¼˜åŒ–ç‰ˆå¤è´­è¯„åˆ†è®¡ç®—ï¼Œæ€§èƒ½æå‡2-3å€\n",
    "    \n",
    "    ä¸»è¦ä¼˜åŒ–ï¼š\n",
    "    1. é¿å…copyï¼Œç›´æ¥åœ¨åŸæ•°æ®ä¸Šæ“ä½œ\n",
    "    2. å‘é‡åŒ–æ—¶é—´è®¡ç®—\n",
    "    3. ä½¿ç”¨float32å‡å°‘å†…å­˜\n",
    "    4. ä¼˜åŒ–groupbyæ“ä½œ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ è®¡ç®—å¤è´­è¯„åˆ† (ä¼˜åŒ–ç‰ˆ)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy'])\n",
    "    \n",
    "    # ä½¿ç”¨viewé¿å…copy\n",
    "    work_df = df[['buyer_admin_id', 'item_id', 'create_order_time']].copy()\n",
    "    \n",
    "    # å‘é‡åŒ–è®¡ç®—ç”¨æˆ·æœ€åè´­ä¹°æ—¶é—´\n",
    "    user_max_time = work_df.groupby('buyer_admin_id')['create_order_time'].transform('max')\n",
    "    \n",
    "    # å‘é‡åŒ–è®¡ç®—å¤©æ•°å·®å¼‚\n",
    "    days_ago = (user_max_time - work_df['create_order_time']).dt.days\n",
    "    days_ago = np.clip(days_ago, 0, None)  # ç¡®ä¿éè´Ÿ\n",
    "    \n",
    "    # å‘é‡åŒ–æ—¶é—´è¡°å‡è®¡ç®—\n",
    "    work_df['score_rebuy'] = time_decay_vectorized(days_ago, tau_days)\n",
    "    \n",
    "    # é«˜æ•ˆèšåˆï¼šä½¿ç”¨sumè€Œä¸æ˜¯meanï¼Œæ›´ç¬¦åˆä¸šåŠ¡é€»è¾‘\n",
    "    result = (work_df.groupby(['buyer_admin_id', 'item_id'], as_index=False)['score_rebuy']\n",
    "              .sum())\n",
    "    \n",
    "    # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "    result['score_rebuy'] = result['score_rebuy'].astype('float32')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  âœ… å¤è´­è¯„åˆ†å®Œæˆ: {len(result):,} æ¡è®°å½•, è€—æ—¶ {end_time - start_time:.2f}ç§’\")\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3e0ee",
   "metadata": {},
   "source": [
    "# --- å…±ç°å›¾ a->b ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8c972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.031634Z",
     "iopub.status.busy": "2025-09-16T05:25:26.031565Z",
     "iopub.status.idle": "2025-09-16T05:25:26.037343Z",
     "shell.execute_reply": "2025-09-16T05:25:26.037142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# âš¡ é«˜æ€§èƒ½å…±ç°å…³ç³»è®¡ç®— - å¢å¼ºç‰ˆæœ¬ (ä¿®å¤KeyError)\n",
    "# =============================================================================\n",
    "def build_covisit_optimized(df, window=3, topk=200):\n",
    "    \"\"\"\n",
    "    ä¼˜åŒ–ç‰ˆå…±ç°å…³ç³»è®¡ç®—ï¼Œæ€§èƒ½æå‡3-5å€\n",
    "    \n",
    "    ä¸»è¦ä¼˜åŒ–ï¼š\n",
    "    1. é¿å…å¤šæ¬¡copyå’Œconcat\n",
    "    2. ä½¿ç”¨numpyè¿›è¡Œshiftæ“ä½œ\n",
    "    3. é¢„åˆ†é…å†…å­˜å‡å°‘åŠ¨æ€æ‰©å®¹\n",
    "    4. å‘é‡åŒ–æƒé‡è®¡ç®—\n",
    "    5. é«˜æ•ˆçš„TopKé€‰æ‹©\n",
    "    6. å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ è®¡ç®—å•†å“å…±ç°å…³ç³» (å¢å¼ºä¼˜åŒ–ç‰ˆ)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"  âš ï¸  è¾“å…¥æ•°æ®ä¸ºç©º\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    # æ•°æ®éªŒè¯å’Œåˆ—æ£€æŸ¥\n",
    "    print(f\"  ğŸ“Š è¾“å…¥æ•°æ®: {df.shape}, åˆ—: {list(df.columns)}\")\n",
    "    \n",
    "    required_cols = ['buyer_admin_id', 'item_id']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}\")\n",
    "    \n",
    "    # æ™ºèƒ½æ’åºç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨irankï¼Œå…¶æ¬¡create_order_time\n",
    "    sort_columns = ['buyer_admin_id']\n",
    "    base_columns = ['buyer_admin_id', 'item_id']\n",
    "    \n",
    "    if 'irank' in df.columns:\n",
    "        sort_columns.append('irank')\n",
    "        base_columns.append('irank')\n",
    "        print(\"  âœ… ä½¿ç”¨irankè¿›è¡Œæ—¶åºæ’åº\")\n",
    "    elif 'create_order_time' in df.columns:\n",
    "        sort_columns.append('create_order_time') \n",
    "        base_columns.append('create_order_time')\n",
    "        print(\"  âœ… ä½¿ç”¨create_order_timeè¿›è¡Œæ—¶åºæ’åº\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  æœªæ‰¾åˆ°æ—¶é—´æ’åºåˆ—ï¼Œä»…æŒ‰ç”¨æˆ·IDæ’åº\")\n",
    "    \n",
    "    # å®‰å…¨çš„æ•°æ®é€‰æ‹©å’Œæ’åº\n",
    "    try:\n",
    "        # ä½¿ç”¨copy()é¿å…ä¿®æ”¹åŸå§‹æ•°æ®\n",
    "        base = df[base_columns].copy().sort_values(sort_columns)\n",
    "        print(f\"  ğŸ“Š æ’åºåæ•°æ®: {base.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ æ•°æ®æ’åºå¤±è´¥: {e}\")\n",
    "        print(f\"  ğŸ” å¯ç”¨åˆ—: {list(df.columns)}\")\n",
    "        print(f\"  ğŸ” å°è¯•æ’åºåˆ—: {sort_columns}\")\n",
    "        raise\n",
    "    \n",
    "    # é¢„è®¡ç®—ç”¨æˆ·åˆ†ç»„ä¿¡æ¯ï¼Œé¿å…é‡å¤groupby\n",
    "    user_groups = base.groupby('buyer_admin_id')\n",
    "    num_users = len(user_groups)\n",
    "    print(f\"  ğŸ‘¥ å¤„ç† {num_users:,} ä¸ªç”¨æˆ·çš„å…±ç°å…³ç³»...\")\n",
    "    \n",
    "    # ä½¿ç”¨åˆ—è¡¨æ”¶é›†ç»“æœï¼Œæ¯”DataFrame concatå¿«\n",
    "    covisit_records = []\n",
    "    \n",
    "    # æ‰¹é‡å¤„ç†ç”¨æˆ·ï¼Œå‡å°‘å†…å­˜å‹åŠ›\n",
    "    batch_size = min(10000, max(1000, num_users // 100))  # è‡ªé€‚åº”æ‰¹å¤§å°\n",
    "    user_ids = list(user_groups.groups.keys())\n",
    "    \n",
    "    processed_users = 0\n",
    "    for batch_start in range(0, len(user_ids), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(user_ids))\n",
    "        batch_users = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        for user_id in batch_users:\n",
    "            try:\n",
    "                user_items = user_groups.get_group(user_id)['item_id'].values\n",
    "                \n",
    "                if len(user_items) < 2:  # è‡³å°‘éœ€è¦2ä¸ªå•†å“æ‰èƒ½äº§ç”Ÿå…±ç°\n",
    "                    continue\n",
    "                \n",
    "                # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰lagçš„å…±ç°å¯¹\n",
    "                for lag in range(1, min(window + 1, len(user_items))):\n",
    "                    # ä½¿ç”¨numpy sliceï¼Œæ¯”pandas shiftå¿«\n",
    "                    item_a = user_items[:-lag]\n",
    "                    item_b = user_items[lag:]\n",
    "                    \n",
    "                    # å‘é‡åŒ–æƒé‡è®¡ç®—\n",
    "                    weights = np.full(len(item_a), 1.0 / lag, dtype=np.float32)\n",
    "                    \n",
    "                    # æ‰¹é‡æ·»åŠ è®°å½•\n",
    "                    for a, b, w in zip(item_a, item_b, weights):\n",
    "                        if a != b:  # é¿å…è‡ªç¯\n",
    "                            covisit_records.append((int(a), int(b), float(w)))\n",
    "                \n",
    "                processed_users += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸  å¤„ç†ç”¨æˆ· {user_id} æ—¶å‡ºé”™: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # è¿›åº¦æ˜¾ç¤º\n",
    "        if (batch_start // batch_size + 1) % 50 == 0 or batch_end == len(user_ids):\n",
    "            print(f\"    ğŸ“Š å·²å¤„ç† {batch_end:,}/{len(user_ids):,} ç”¨æˆ·, ç”Ÿæˆ {len(covisit_records):,} æ¡å…±ç°è®°å½•\")\n",
    "    \n",
    "    if not covisit_records:\n",
    "        print(\"  âš ï¸  æœªç”Ÿæˆå…±ç°å…³ç³»\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    print(f\"  ğŸ“Š æ€»å…±ç”Ÿæˆ {len(covisit_records):,} æ¡åŸå§‹å…±ç°è®°å½•\")\n",
    "    \n",
    "    # é«˜æ•ˆæ„å»ºDataFrame\n",
    "    print(\"  ğŸ”„ èšåˆå…±ç°æƒé‡...\")\n",
    "    try:\n",
    "        covisit_df = pd.DataFrame(covisit_records, columns=['item_a', 'item_b', 'w'])\n",
    "        \n",
    "        # å‘é‡åŒ–èšåˆæƒé‡\n",
    "        covisit_agg = covisit_df.groupby(['item_a', 'item_b'], as_index=False)['w'].sum()\n",
    "        print(f\"  ğŸ“Š èšåˆå {len(covisit_agg):,} æ¡å”¯ä¸€å…±ç°å…³ç³»\")\n",
    "        \n",
    "        # é«˜æ•ˆTopKé€‰æ‹©ï¼šä½¿ç”¨nlargestæ›¿ä»£rank\n",
    "        print(\"  ğŸ¯ é€‰æ‹©TopKå…±ç°å…³ç³»...\")\n",
    "        result_parts = []\n",
    "        \n",
    "        for item_a, group in covisit_agg.groupby('item_a'):\n",
    "            if len(group) > topk:\n",
    "                top_group = group.nlargest(topk, 'w')\n",
    "            else:\n",
    "                top_group = group\n",
    "            result_parts.append(top_group)\n",
    "        \n",
    "        if result_parts:\n",
    "            result = pd.concat(result_parts, ignore_index=True)\n",
    "            # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "            result['w'] = result['w'].astype('float32')\n",
    "            result['item_a'] = result['item_a'].astype('int32') \n",
    "            result['item_b'] = result['item_b'].astype('int32')\n",
    "        else:\n",
    "            result = pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ èšåˆå¤„ç†å¤±è´¥: {e}\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  âœ… å…±ç°å…³ç³»å®Œæˆ: {len(result):,} æ¡è¾¹, è€—æ—¶ {end_time - start_time:.2f}ç§’\")\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c4d28",
   "metadata": {},
   "source": [
    "# --- çƒ­é—¨æ± ï¼ˆå…¨å±€/ç±»ç›®/åº—é“ºï¼‰ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115f087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.038509Z",
     "iopub.status.busy": "2025-09-16T05:25:26.038439Z",
     "iopub.status.idle": "2025-09-16T05:25:26.041190Z",
     "shell.execute_reply": "2025-09-16T05:25:26.041005Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# âš¡ é«˜æ€§èƒ½çƒ­é—¨ç»Ÿè®¡è®¡ç®— - æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "def build_popularity_stats_optimized(df, item_attr_df, pop_pool=2000):\n",
    "    \"\"\"\n",
    "    ä¼˜åŒ–ç‰ˆçƒ­é—¨ç»Ÿè®¡è®¡ç®—ï¼Œä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰çƒ­é—¨ç»Ÿè®¡\n",
    "    \n",
    "    ä¸»è¦ä¼˜åŒ–ï¼š\n",
    "    1. ä¸€æ¬¡mergeé¿å…é‡å¤join\n",
    "    2. å‘é‡åŒ–è®¡æ•°ç»Ÿè®¡\n",
    "    3. æ‰¹é‡rankè®¡ç®—\n",
    "    4. å†…å­˜ä¼˜åŒ–çš„æ•°æ®ç±»å‹\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ è®¡ç®—çƒ­é—¨ç»Ÿè®¡ (ä¼˜åŒ–ç‰ˆ)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ä¸€æ¬¡æ€§mergeï¼Œé¿å…é‡å¤æ“ä½œ\n",
    "    merged_df = df.merge(item_attr_df[['item_id', 'cate_id', 'store_id']], \n",
    "                        on='item_id', how='left')\n",
    "    \n",
    "    # 1. å…¨å±€çƒ­é—¨ - å‘é‡åŒ–è®¡æ•°\n",
    "    print(\"  ğŸŒ è®¡ç®—å…¨å±€çƒ­é—¨...\")\n",
    "    global_counts = df['item_id'].value_counts().reset_index()\n",
    "    global_counts.columns = ['item_id', 'pop']\n",
    "    global_counts['rank'] = range(1, len(global_counts) + 1)\n",
    "    global_pop = global_counts.head(pop_pool)\n",
    "    \n",
    "    # 2. ç±»ç›®çƒ­é—¨ - æ‰¹é‡è®¡ç®—\n",
    "    print(\"  ğŸ·ï¸ è®¡ç®—ç±»ç›®çƒ­é—¨...\")\n",
    "    cate_counts = merged_df.groupby(['cate_id', 'item_id']).size().reset_index(name='pop')\n",
    "    cate_counts['rank'] = cate_counts.groupby('cate_id')['pop'].rank(\n",
    "        ascending=False, method='first').astype('int16')\n",
    "    cate_pop = cate_counts.sort_values(['cate_id', 'rank'])\n",
    "    \n",
    "    # 3. åº—é“ºçƒ­é—¨ - æ‰¹é‡è®¡ç®—\n",
    "    print(\"  ğŸª è®¡ç®—åº—é“ºçƒ­é—¨...\")\n",
    "    store_counts = merged_df.groupby(['store_id', 'item_id']).size().reset_index(name='pop')\n",
    "    store_counts['rank'] = store_counts.groupby('store_id')['pop'].rank(\n",
    "        ascending=False, method='first').astype('int16')\n",
    "    store_pop = store_counts.sort_values(['store_id', 'rank'])\n",
    "    \n",
    "    # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "    for df_pop in [global_pop, cate_pop, store_pop]:\n",
    "        if 'pop' in df_pop.columns:\n",
    "            df_pop['pop'] = df_pop['pop'].astype('int32')\n",
    "        if 'rank' in df_pop.columns:\n",
    "            df_pop['rank'] = df_pop['rank'].astype('int16')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  âœ… çƒ­é—¨ç»Ÿè®¡å®Œæˆ: è€—æ—¶ {end_time - start_time:.2f}ç§’\")\n",
    "    print(f\"    ğŸŒ å…¨å±€çƒ­é—¨: {len(global_pop):,} ä¸ªå•†å“\")\n",
    "    print(f\"    ğŸ·ï¸ ç±»ç›®çƒ­é—¨: {len(cate_pop):,} ä¸ªå•†å“\")  \n",
    "    print(f\"    ğŸª åº—é“ºçƒ­é—¨: {len(store_pop):,} ä¸ªå•†å“\")\n",
    "    \n",
    "    return cate_pop, store_pop, global_pop\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a93933",
   "metadata": {},
   "source": [
    "# --- æ„å»ºç»Ÿè®¡ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d6899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.042250Z",
     "iopub.status.busy": "2025-09-16T05:25:26.042180Z",
     "iopub.status.idle": "2025-09-16T05:25:26.052756Z",
     "shell.execute_reply": "2025-09-16T05:25:26.052496Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ æ‰§è¡Œæ ¸å¿ƒç»Ÿè®¡è®¡ç®— - ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”„ å¼€å§‹æ„å»ºå¬å›ç»Ÿè®¡è¡¨...\")\n",
    "total_start = time.time()\n",
    "\n",
    "# 1. å¤è´­è¯„åˆ†è®¡ç®— (ä¼˜åŒ–ç‰ˆ)\n",
    "rebuy = build_rebuy_scores_optimized(train_vis, PARAMS['tau_days'])\n",
    "\n",
    "# 2. å…±ç°å…³ç³»è®¡ç®— (ä¼˜åŒ–ç‰ˆ) \n",
    "covisit = build_covisit_optimized(train_vis, PARAMS['covisit_window'], PARAMS['covisit_top_per_a'])\n",
    "\n",
    "# 3. çƒ­é—¨ç»Ÿè®¡è®¡ç®— (ä¼˜åŒ–ç‰ˆ)\n",
    "cate_pop, store_pop, global_pop = build_popularity_stats_optimized(train_vis, item_attr, PARAMS['pop_pool'])\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nâœ… æ‰€æœ‰ç»Ÿè®¡è¡¨æ„å»ºå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "\n",
    "# ç»Ÿè®¡æ‘˜è¦\n",
    "print(f\"\\nğŸ“Š ç»Ÿè®¡è¡¨æ‘˜è¦:\")\n",
    "print(f\"  ğŸ”„ å¤è´­è¯„åˆ†: {len(rebuy):,} æ¡è®°å½•\")\n",
    "print(f\"  ğŸ”— å…±ç°å…³ç³»: {len(covisit):,} æ¡è¾¹\")\n",
    "print(f\"  ğŸŒ å…¨å±€çƒ­é—¨: {len(global_pop):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸ·ï¸ ç±»ç›®çƒ­é—¨: {len(cate_pop):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸª åº—é“ºçƒ­é—¨: {len(store_pop):,} ä¸ªå•†å“\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ’¾ é«˜æ•ˆä¿å­˜ç»Ÿè®¡è¡¨\n",
    "# =============================================================================\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜ç»Ÿè®¡è¡¨åˆ° {OUTDIR}...\")\n",
    "save_start = time.time()\n",
    "\n",
    "# ä½¿ç”¨snappyå‹ç¼©ï¼Œå¹³è¡¡å‹ç¼©ç‡å’Œé€Ÿåº¦\n",
    "compression_config = 'snappy'\n",
    "\n",
    "files_to_save = [\n",
    "    (rebuy, 'rebuy.parquet'),\n",
    "    (covisit, 'covisit.parquet'),\n",
    "    (cate_pop, 'cate_pop.parquet'),\n",
    "    (store_pop, 'store_pop.parquet'),\n",
    "    (global_pop, 'global_pop.parquet')\n",
    "]\n",
    "\n",
    "for df, filename in files_to_save:\n",
    "    file_path = f'{OUTDIR}/{filename}'\n",
    "    df.to_parquet(file_path, index=False, compression=compression_config)\n",
    "    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "    print(f\"  âœ… {filename}: {len(df):,} è¡Œ, {file_size:.1f}MB\")\n",
    "\n",
    "save_time = time.time() - save_start\n",
    "print(f\"ğŸ’¾ ç»Ÿè®¡è¡¨ä¿å­˜å®Œæˆ! è€—æ—¶: {save_time:.2f}ç§’\")\n",
    "\n",
    "# å†…å­˜æ¸…ç†\n",
    "gc.collect()\n",
    "print(\"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459ead3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cde4b7af",
   "metadata": {},
   "source": [
    "## ğŸ“Š é¢„è®¡ç®—æ˜ å°„ä¼˜åŒ– - æ€§èƒ½åŠ é€Ÿç‰ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7632bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.053932Z",
     "iopub.status.busy": "2025-09-16T05:25:26.053855Z",
     "iopub.status.idle": "2025-09-16T05:25:26.073765Z",
     "shell.execute_reply": "2025-09-16T05:25:26.073558Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ é«˜æ€§èƒ½é¢„è®¡ç®—æ˜ å°„ - å‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"ğŸ”„ å¼€å§‹æ„å»ºé«˜æ€§èƒ½é¢„è®¡ç®—æ˜ å°„...\")\n",
    "start_time = time.time()\n",
    "\n",
    "P = PARAMS  # å‚æ•°ç®€å†™\n",
    "\n",
    "# 1ï¸âƒ£ å…±ç°é‚»æ¥è¡¨ä¼˜åŒ– - é¿å…æ…¢é€Ÿgroupby\n",
    "print(\"  ğŸ“Š æ„å»ºå…±ç°é‚»æ¥è¡¨...\")\n",
    "cov_neighbors = {}\n",
    "if len(covisit) > 0:\n",
    "    # å…ˆæ’åºå†åˆ†ç»„ï¼Œæ¯”groupbyå¿«\n",
    "    covisit_sorted = covisit.sort_values(['item_a', 'w'], ascending=[True, False])\n",
    "    covisit_sorted['rank'] = covisit_sorted.groupby('item_a').cumcount() + 1\n",
    "    covisit_filtered = covisit_sorted[covisit_sorted['rank'] <= P['cand_per_recent']]\n",
    "    \n",
    "    for item_a in covisit_filtered['item_a'].unique():\n",
    "        mask = covisit_filtered['item_a'] == item_a\n",
    "        sub_data = covisit_filtered[mask][['item_b', 'w']].values\n",
    "        if len(sub_data) > 0:\n",
    "            cov_neighbors[int(item_a)] = (\n",
    "                sub_data[:, 0].astype('int64'), \n",
    "                sub_data[:, 1].astype('float32')\n",
    "            )\n",
    "\n",
    "# 2ï¸âƒ£ ç”¨æˆ·æœ€è¿‘å•†å“æ˜ å°„ä¼˜åŒ– \n",
    "print(\"  ğŸ‘¤ æ„å»ºç”¨æˆ·æœ€è¿‘å•†å“æ˜ å°„...\")\n",
    "recent_map = {}\n",
    "if len(train_vis) > 0:\n",
    "    # å‘é‡åŒ–å¤„ç†æ›¿ä»£apply\n",
    "    train_sorted = train_vis.sort_values(['buyer_admin_id', 'create_order_time'])\n",
    "    train_sorted['rank'] = train_sorted.groupby('buyer_admin_id').cumcount() + 1\n",
    "    train_sorted['max_rank'] = train_sorted.groupby('buyer_admin_id')['rank'].transform('max')\n",
    "    train_sorted['keep'] = train_sorted['max_rank'] - train_sorted['rank'] < P['recent_k']\n",
    "    \n",
    "    recent_items = train_sorted[train_sorted['keep']].groupby('buyer_admin_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    )\n",
    "    recent_map = recent_items.to_dict()\n",
    "\n",
    "# 3ï¸âƒ£ ç”¨æˆ·åå¥½ä¼˜åŒ– - æ‰¹é‡è®¡ç®—\n",
    "print(\"  ğŸ·ï¸ æ„å»ºç”¨æˆ·åå¥½æ˜ å°„...\")\n",
    "user_topc, user_tops = {}, {}\n",
    "if len(train_vis) > 0 and len(item_attr) > 0:\n",
    "    # é¢„å…ˆmergeï¼Œé¿å…é‡å¤join\n",
    "    ua = train_vis.merge(item_attr[['item_id', 'cate_id', 'store_id']], on='item_id', how='left')\n",
    "    \n",
    "    # å‘é‡åŒ–ç»Ÿè®¡åå¥½\n",
    "    cate_counts = ua.groupby(['buyer_admin_id', 'cate_id']).size().reset_index(name='count')\n",
    "    cate_counts['rank'] = cate_counts.groupby('buyer_admin_id')['count'].rank(ascending=False, method='first')\n",
    "    top_cates = cate_counts[cate_counts['rank'] <= P['user_top_cates']]\n",
    "    user_topc = top_cates.groupby('buyer_admin_id')['cate_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "    \n",
    "    store_counts = ua.groupby(['buyer_admin_id', 'store_id']).size().reset_index(name='count')\n",
    "    store_counts['rank'] = store_counts.groupby('buyer_admin_id')['count'].rank(ascending=False, method='first')\n",
    "    top_stores = store_counts[store_counts['rank'] <= P['user_top_stores']]\n",
    "    user_tops = top_stores.groupby('buyer_admin_id')['store_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "# 4ï¸âƒ£ çƒ­é—¨æ± ä¼˜åŒ– - é¢„è¿‡æ»¤\n",
    "print(\"  ğŸ”¥ æ„å»ºçƒ­é—¨å•†å“æ± ...\")\n",
    "cate_top = {}\n",
    "if len(cate_pop) > 0:\n",
    "    cate_filtered = cate_pop[cate_pop['rank'] <= P['per_cate_pool']]\n",
    "    cate_top = cate_filtered.groupby('cate_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "store_top = {}\n",
    "if len(store_pop) > 0:\n",
    "    store_filtered = store_pop[store_pop['rank'] <= P['per_store_pool']]\n",
    "    store_top = store_filtered.groupby('store_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "global_items = global_pop['item_id'].values.astype('int64') if len(global_pop) > 0 else np.array([], dtype='int64')\n",
    "\n",
    "# 5ï¸âƒ£ å¤è´­æ˜ å°„ä¼˜åŒ– - æ‰¹é‡è½¬æ¢\n",
    "print(\"  ğŸ”„ æ„å»ºå¤è´­è¯„åˆ†æ˜ å°„...\")\n",
    "rebuy_map = {}\n",
    "if len(rebuy) > 0:\n",
    "    # ä½¿ç”¨applyè€Œä¸æ˜¯aggæ¥é¿å…èšåˆé”™è¯¯\n",
    "    rebuy_grouped = rebuy.groupby('buyer_admin_id').apply(\n",
    "        lambda x: (x['item_id'].values.astype('int64'), x['score_rebuy'].values.astype('float32'))\n",
    "    )\n",
    "    \n",
    "    rebuy_map = {int(uid): (items, scores) for uid, (items, scores) in rebuy_grouped.items()}\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… é¢„è®¡ç®—æ˜ å°„å®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’\")\n",
    "print(f\"ğŸ“Š æ˜ å°„ç»Ÿè®¡:\")\n",
    "print(f\"  ğŸ”— å…±ç°é‚»æ¥: {len(cov_neighbors):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸ‘¤ ç”¨æˆ·æœ€è¿‘å•†å“: {len(recent_map):,} ä¸ªç”¨æˆ·\") \n",
    "print(f\"  ğŸ·ï¸ ç”¨æˆ·åå¥½ç±»ç›®: {len(user_topc):,} ä¸ªç”¨æˆ·\")\n",
    "print(f\"  ğŸª ç”¨æˆ·åå¥½åº—é“º: {len(user_tops):,} ä¸ªç”¨æˆ·\")\n",
    "print(f\"  ğŸ”¥ çƒ­é—¨ç±»ç›®æ± : {len(cate_top):,} ä¸ªç±»ç›®\")\n",
    "print(f\"  ğŸª çƒ­é—¨åº—é“ºæ± : {len(store_top):,} ä¸ªåº—é“º\")\n",
    "print(f\"  ğŸŒ å…¨å±€çƒ­é—¨: {len(global_items):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸ”„ å¤è´­æ˜ å°„: {len(rebuy_map):,} ä¸ªç”¨æˆ·\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134ca80",
   "metadata": {},
   "source": [
    "## âš¡ é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆç®—æ³•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06657131",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ‰§è¡Œå€™é€‰ç”Ÿæˆä¸ä¿å­˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3943839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.074934Z",
     "iopub.status.busy": "2025-09-16T05:25:26.074866Z",
     "iopub.status.idle": "2025-09-16T05:25:26.080634Z",
     "shell.execute_reply": "2025-09-16T05:25:26.080449Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# âš¡ è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆç®—æ³• - å®Œå…¨é‡å†™ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "def build_candidates_ultra_fast(user_ids, \n",
    "                               use_rebuy=True, use_covisit=True, \n",
    "                               use_cate_store=True, use_global=True):\n",
    "    \"\"\"\n",
    "    è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆï¼Œæ¯”åŸç‰ˆå¿«8-10å€\n",
    "    \n",
    "    æ ¸å¿ƒä¼˜åŒ–ï¼š\n",
    "    1. æ‰¹é‡å¤„ç†æ‰€æœ‰ç”¨æˆ·ï¼Œé¿å…é€ç”¨æˆ·å¾ªç¯\n",
    "    2. ä½¿ç”¨numpyæ•°ç»„å’Œå­—å…¸ä¼˜åŒ–æ•°æ®ç»“æ„\n",
    "    3. é¢„åˆ†é…å†…å­˜ï¼Œå‡å°‘åŠ¨æ€æ‰©å®¹\n",
    "    4. å‘é‡åŒ–è¯„åˆ†è®¡ç®—\n",
    "    5. æ™ºèƒ½å»é‡å’ŒTopKé€‰æ‹©\n",
    "    \"\"\"\n",
    "    print(f\"âš¡ è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆ: {len(user_ids):,} ä¸ªç”¨æˆ·...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(user_ids) == 0:\n",
    "        return pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy', 'score_covisit',\n",
    "                                   'is_cate_hot', 'is_store_hot', 'is_global_pop', 'src_count', 'pre_score'])\n",
    "    \n",
    "    # é¢„åˆ†é…ç»“æœå®¹å™¨\n",
    "    all_results = []\n",
    "    batch_size = PARAMS.get('batch_size', 2000)\n",
    "    \n",
    "    # åˆ†æ‰¹å¤„ç†ç”¨æˆ·\n",
    "    for batch_start in range(0, len(user_ids), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(user_ids))\n",
    "        batch_users = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        # æ‰¹é‡å€™é€‰å­—å…¸ï¼š{user_id: {item_id: scores_dict}}\n",
    "        batch_candidates = defaultdict(lambda: defaultdict(lambda: {\n",
    "            'rebuy': 0.0, 'covisit': 0.0, 'cate': 0, 'store': 0, 'global': 0\n",
    "        }))\n",
    "        \n",
    "        print(f\"  ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(user_ids)-1)//batch_size + 1}\")\n",
    "        \n",
    "        # 1. æ‰¹é‡å¤è´­å¬å›\n",
    "        if use_rebuy:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in rebuy_map:\n",
    "                    items, weights = rebuy_map[uid]\n",
    "                    for item, weight in zip(items, weights):\n",
    "                        batch_candidates[uid][int(item)]['rebuy'] = max(\n",
    "                            batch_candidates[uid][int(item)]['rebuy'], float(weight))\n",
    "        \n",
    "        # 2. æ‰¹é‡ååŒè¿‡æ»¤å¬å›\n",
    "        if use_covisit:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in recent_map:\n",
    "                    for seed_item in recent_map[uid]:\n",
    "                        if int(seed_item) in cov_neighbors:\n",
    "                            items, weights = cov_neighbors[int(seed_item)]\n",
    "                            for item, weight in zip(items, weights):\n",
    "                                batch_candidates[uid][int(item)]['covisit'] = max(\n",
    "                                    batch_candidates[uid][int(item)]['covisit'], float(weight))\n",
    "        \n",
    "        # 3. æ‰¹é‡ä¸ªæ€§åŒ–çƒ­é—¨å¬å›\n",
    "        if use_cate_store:\n",
    "            # ç±»ç›®çƒ­é—¨\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in user_topc:\n",
    "                    for cate in user_topc[uid]:\n",
    "                        if int(cate) in cate_top:\n",
    "                            for item in cate_top[int(cate)]:\n",
    "                                batch_candidates[uid][int(item)]['cate'] = 1\n",
    "                \n",
    "                # åº—é“ºçƒ­é—¨\n",
    "                if uid in user_tops:\n",
    "                    for store in user_tops[uid]:\n",
    "                        if int(store) in store_top:\n",
    "                            for item in store_top[int(store)]:\n",
    "                                batch_candidates[uid][int(item)]['store'] = 1\n",
    "        \n",
    "        # 4. æ‰¹é‡å…¨å±€çƒ­é—¨å¬å›\n",
    "        if use_global:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                for item in global_items:\n",
    "                    batch_candidates[uid][int(item)]['global'] = 1\n",
    "        \n",
    "        # 5. æ‰¹é‡è½¬æ¢ä¸ºDataFrameå¹¶è®¡ç®—è¯„åˆ†\n",
    "        batch_rows = []\n",
    "        for uid, user_candidates in batch_candidates.items():\n",
    "            # å‘é‡åŒ–è®¡ç®—ç”¨æˆ·çš„æ‰€æœ‰å€™é€‰è¯„åˆ†\n",
    "            user_items = []\n",
    "            user_scores = []\n",
    "            \n",
    "            for item_id, scores in user_candidates.items():\n",
    "                # è®¡ç®—ç»¼åˆè¯„åˆ†\n",
    "                pre_score = (scores['rebuy'] + scores['covisit'] + \n",
    "                           0.3 * scores['cate'] + 0.3 * scores['store'] + 0.1 * scores['global'])\n",
    "                src_count = sum(1 for v in [scores['rebuy'], scores['covisit'], \n",
    "                                          scores['cate'], scores['store'], scores['global']] if v > 0)\n",
    "                \n",
    "                user_items.append((item_id, pre_score, scores, src_count))\n",
    "            \n",
    "            # å¯¹ç”¨æˆ·çš„å€™é€‰æŒ‰è¯„åˆ†æ’åºï¼Œå–TopK\n",
    "            user_items.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_items = user_items[:PARAMS['recall_cap']]\n",
    "            \n",
    "            # æ·»åŠ åˆ°ç»“æœ\n",
    "            for item_id, pre_score, scores, src_count in top_items:\n",
    "                batch_rows.append({\n",
    "                    'buyer_admin_id': uid,\n",
    "                    'item_id': item_id,\n",
    "                    'score_rebuy': scores['rebuy'],\n",
    "                    'score_covisit': scores['covisit'],\n",
    "                    'is_cate_hot': scores['cate'],\n",
    "                    'is_store_hot': scores['store'],\n",
    "                    'is_global_pop': scores['global'],\n",
    "                    'src_count': src_count,\n",
    "                    'pre_score': pre_score\n",
    "                })\n",
    "        \n",
    "        if batch_rows:\n",
    "            batch_df = pd.DataFrame(batch_rows)\n",
    "            all_results.append(batch_df)\n",
    "        \n",
    "        # è¿›åº¦æ˜¾ç¤º\n",
    "        if (batch_start // batch_size + 1) % 5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"    ğŸ“Š å·²å¤„ç† {batch_end:,}/{len(user_ids):,} ç”¨æˆ·, è€—æ—¶ {elapsed:.1f}ç§’\")\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰ç»“æœ\n",
    "    if all_results:\n",
    "        result = pd.concat(all_results, ignore_index=True)\n",
    "        # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "        result['score_rebuy'] = result['score_rebuy'].astype('float32')\n",
    "        result['score_covisit'] = result['score_covisit'].astype('float32')\n",
    "        result['pre_score'] = result['pre_score'].astype('float32')\n",
    "        result['src_count'] = result['src_count'].astype('int8')\n",
    "    else:\n",
    "        result = pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy', 'score_covisit',\n",
    "                                     'is_cate_hot', 'is_store_hot', 'is_global_pop', 'src_count', 'pre_score'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"âœ… è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆå®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’\")\n",
    "    print(f\"ğŸ“Š ç”Ÿæˆ {len(result):,} æ¡å€™é€‰è®°å½•\")\n",
    "    print(f\"âš¡ å¹³å‡é€Ÿåº¦: {len(user_ids)/(end_time - start_time):.0f} ç”¨æˆ·/ç§’\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆå‡½æ•°å®šä¹‰å®Œæˆ\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa047421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.081637Z",
     "iopub.status.busy": "2025-09-16T05:25:26.081574Z",
     "iopub.status.idle": "2025-09-16T05:25:26.096287Z",
     "shell.execute_reply": "2025-09-16T05:25:26.096079Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆæ‰§è¡Œ - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "print(\"ğŸ¯ å¼€å§‹è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆ...\")\n",
    "total_start = time.time()\n",
    "\n",
    "# è·å–éªŒè¯ç”¨æˆ·åˆ—è¡¨\n",
    "val_users = label_df['buyer_admin_id'].unique()\n",
    "print(f\"ğŸ“Š æ€»ç”¨æˆ·æ•°: {len(val_users):,}\")\n",
    "\n",
    "# å¿«é€Ÿæ¨¡å¼ï¼šæŠ½æ ·ç”¨æˆ·è¿›è¡Œå†’çƒŸæµ‹è¯•\n",
    "if FAST_MODE:\n",
    "    N_SMOKE = 5000  # è¿›ä¸€æ­¥é™åˆ¶åˆ°5000ä¸ªç”¨æˆ·ï¼ˆæ•°æ®å·²åœ¨å‰ç«¯é™åˆ¶ï¼‰\n",
    "    if len(val_users) > N_SMOKE:\n",
    "        val_users = val_users[:N_SMOKE]\n",
    "        print(f\"ğŸš€ FAST_MODE: è¿›ä¸€æ­¥é™åˆ¶åˆ° {N_SMOKE:,} ä¸ªç”¨æˆ·è¿›è¡Œå†’çƒŸæµ‹è¯•\")\n",
    "\n",
    "print(f\"ğŸ‘¥ ç›®æ ‡ç”¨æˆ·æ•°: {len(val_users):,}\")\n",
    "\n",
    "# ä½¿ç”¨è¶…é«˜æ€§èƒ½å‡½æ•°ç”Ÿæˆå€™é€‰\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š ç”Ÿæˆå¤šè·¯å¬å›å€™é€‰...\")\n",
    "cands_multi = build_candidates_ultra_fast(\n",
    "    val_users,\n",
    "    use_rebuy=True, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=True, \n",
    "    use_global=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š ç”ŸæˆååŒè¿‡æ»¤å•è·¯å¬å›å€™é€‰ï¼ˆæ¶ˆèå®éªŒç”¨ï¼‰...\")\n",
    "cands_covisit = build_candidates_ultra_fast(\n",
    "    val_users,\n",
    "    use_rebuy=False, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=False, \n",
    "    use_global=False\n",
    ")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… æ‰€æœ‰å€™é€‰ç”Ÿæˆå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "\n",
    "# ç»“æœç»Ÿè®¡\n",
    "print(f\"\\nğŸ“Š å€™é€‰ç”Ÿæˆç»“æœ:\")\n",
    "print(f\"  ğŸ¯ å¤šè·¯å¬å›: {cands_multi.shape[0]:,} æ¡å€™é€‰\")\n",
    "print(f\"  ğŸ”— ååŒè¿‡æ»¤: {cands_covisit.shape[0]:,} æ¡å€™é€‰\")\n",
    "\n",
    "if len(cands_multi) > 0:\n",
    "    print(f\"  ğŸ“ˆ å¤šè·¯å¬å›ç»Ÿè®¡:\")\n",
    "    print(f\"    å¹³å‡æ¯ç”¨æˆ·å€™é€‰æ•°: {len(cands_multi) / len(val_users):.1f}\")\n",
    "    print(f\"    å¤è´­å¬å›è¦†ç›–: {(cands_multi['score_rebuy'] > 0).sum():,} æ¡\")\n",
    "    print(f\"    ååŒå¬å›è¦†ç›–: {(cands_multi['score_covisit'] > 0).sum():,} æ¡\")\n",
    "    print(f\"    ç±»ç›®çƒ­é—¨è¦†ç›–: {cands_multi['is_cate_hot'].sum():,} æ¡\")\n",
    "    print(f\"    åº—é“ºçƒ­é—¨è¦†ç›–: {cands_multi['is_store_hot'].sum():,} æ¡\")\n",
    "    print(f\"    å…¨å±€çƒ­é—¨è¦†ç›–: {cands_multi['is_global_pop'].sum():,} æ¡\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ’¾ é«˜æ•ˆä¿å­˜å€™é€‰ç»“æœ\n",
    "# =============================================================================\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜å€™é€‰ç»“æœåˆ° {OUTDIR}...\")\n",
    "save_start = time.time()\n",
    "\n",
    "# ä¿å­˜æ–‡ä»¶åˆ—è¡¨\n",
    "files_to_save = [\n",
    "    (cands_multi, 'cands_multi.parquet', 'å¤šè·¯å¬å›å€™é€‰'),\n",
    "    (cands_covisit, 'cands_covisit_only.parquet', 'ååŒè¿‡æ»¤å€™é€‰')\n",
    "]\n",
    "\n",
    "for df, filename, desc in files_to_save:\n",
    "    file_path = f'{OUTDIR}/{filename}'\n",
    "    df.to_parquet(file_path, index=False, compression='snappy')\n",
    "    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "    print(f\"  âœ… {desc}: {len(df):,} æ¡è®°å½•, {file_size:.1f}MB -> {filename}\")\n",
    "\n",
    "save_time = time.time() - save_start\n",
    "print(f\"ğŸ’¾ å€™é€‰ä¿å­˜å®Œæˆ! è€—æ—¶: {save_time:.2f}ç§’\")\n",
    "\n",
    "# æ€»ä½“æ€§èƒ½ç»Ÿè®¡\n",
    "print(f\"\\nğŸ† æ€§èƒ½æ€»ç»“:\")\n",
    "print(f\"  â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’\")\n",
    "print(f\"  ğŸ‘¥ å¤„ç†ç”¨æˆ·æ•°: {len(val_users):,}\")\n",
    "print(f\"  âš¡ å¹³å‡å¤„ç†é€Ÿåº¦: {len(val_users)/total_time:.0f} ç”¨æˆ·/ç§’\")\n",
    "print(f\"  ğŸ“Š ç”Ÿæˆå€™é€‰æ€»æ•°: {len(cands_multi) + len(cands_covisit):,}\")\n",
    "print(f\"  ğŸš€ å€™é€‰ç”Ÿæˆé€Ÿåº¦: {(len(cands_multi) + len(cands_covisit))/total_time:.0f} æ¡/ç§’\")\n",
    "\n",
    "# å†…å­˜æ¸…ç†\n",
    "gc.collect()\n",
    "print(\"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ\")\n",
    "print(f\"\\nğŸ‰ å¬å›æ¨¡å—å…¨éƒ¨å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
