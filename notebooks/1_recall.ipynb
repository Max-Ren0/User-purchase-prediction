{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132f17dc",
   "metadata": {},
   "source": [
    "# ğŸ¯ å¤šè·¯å¬å›æ¨¡å— (1_recall.ipynb)\n",
    "\n",
    "## ğŸ“‹ æ¨¡å—åŠŸèƒ½\n",
    "å®ç°**4ç§å¬å›ç­–ç•¥**ï¼Œä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰å•†å“é›†åˆï¼š\n",
    "\n",
    "1. **ğŸ”„ å¤è´­å¬å›**: åŸºäºç”¨æˆ·å†å²è´­ä¹° + æ—¶é—´è¡°å‡\n",
    "2. **ğŸ”— ååŒè¿‡æ»¤å¬å›**: åŸºäºå•†å“å…±ç°å…³ç³»\n",
    "3. **ğŸª ä¸ªæ€§åŒ–çƒ­é—¨**: ç”¨æˆ·åå¥½ç±»ç›®/åº—é“ºçƒ­é—¨  \n",
    "4. **ğŸŒ å…¨å±€çƒ­é—¨**: å†·å¯åŠ¨è¡¥å……\n",
    "\n",
    "## âš¡ æ€§èƒ½ä¼˜åŒ–\n",
    "- **FAST_MODE**: å¼€å‘æ¨¡å¼å‚æ•°è°ƒæ•´\n",
    "- **å†…å­˜ä¼˜åŒ–**: dtypeå‹ç¼©å‡å°‘å†…å­˜å ç”¨\n",
    "- **é¢„è®¡ç®—åŠ é€Ÿ**: é‚»æ¥è¡¨ã€æ˜ å°„å­—å…¸ç­‰\n",
    "- **æ‰¹å¤„ç†**: çº¯å­—å…¸ + numpy é¿å…é¢‘ç¹join\n",
    "\n",
    "## ğŸ”§ è¾“å‡ºæ–‡ä»¶\n",
    "- **ç»Ÿè®¡è¡¨**: rebuy, covisit, cate_pop, store_pop, global_pop\n",
    "- **å€™é€‰é›†**: cands_multi (å¤šè·¯), cands_covisit_only (å•è·¯/æ¶ˆè)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5d0d1",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ ç¯å¢ƒé…ç½®ä¸æ•°æ®åŠ è½½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80edcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tqdm å¯ç”¨\n",
      "ğŸš€ å¤šè·¯å¬å›æ¨¡å—å¯åŠ¨\n",
      "â° å¯åŠ¨æ—¶é—´: 2025-09-10 22:39:24\n",
      "ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®...\n",
      "ğŸ” æ£€æŸ¥æ•°æ®è´¨é‡å’Œåˆ—ç»“æ„...\n",
      "  ğŸ“Š train_vis:\n",
      "    å½¢çŠ¶: (6506700, 5)\n",
      "    åˆ—å: ['buyer_country_id', 'buyer_admin_id', 'item_id', 'create_order_time', 'irank']\n",
      "    æ•°æ®ç±»å‹: {'buyer_country_id': dtype('O'), 'buyer_admin_id': dtype('int32'), 'item_id': dtype('int32'), 'create_order_time': dtype('<M8[ns]'), 'irank': dtype('int16')}\n",
      "  ğŸ“Š item_attr:\n",
      "    å½¢çŠ¶: (1924269, 3)\n",
      "    åˆ—å: ['item_id', 'cate_id', 'store_id']\n",
      "    æ•°æ®ç±»å‹: {'item_id': dtype('int32'), 'cate_id': dtype('int32'), 'store_id': dtype('int32')}\n",
      "  ğŸ“Š label_df:\n",
      "    å½¢çŠ¶: (483117, 2)\n",
      "    åˆ—å: ['buyer_admin_id', 'label_item']\n",
      "    æ•°æ®ç±»å‹: {'buyer_admin_id': dtype('int32'), 'label_item': dtype('int32')}\n",
      "\n",
      "ğŸ”§ ä¼˜åŒ–æ•°æ®ç±»å‹...\n",
      "  ğŸ“Š train_vis: 428.2MB â†’ 117.9MB (å‡å°‘72.5%)\n",
      "  ğŸ“Š item_attr: 22.0MB â†’ 18.4MB (å‡å°‘16.7%)\n",
      "  ğŸ“Š label_df: 3.7MB â†’ 3.7MB (å‡å°‘0.0%)\n",
      "âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ 1.11ç§’\n",
      "ğŸ“Š æ•°æ®è§„æ¨¡: train_vis=6,506,700 è¡Œ, 483,117 ç”¨æˆ·\n",
      "ğŸš€ å¯ç”¨ FAST_MODE - å¿«é€Ÿå¼€å‘æ¨¡å¼\n",
      "âš™ï¸ å‚æ•°é…ç½®:\n",
      "  covisit_window: 3\n",
      "  covisit_top_per_a: 120\n",
      "  recent_k: 3\n",
      "  cand_per_recent: 24\n",
      "  tau_days: 14\n",
      "  user_top_cates: 3\n",
      "  user_top_stores: 3\n",
      "  per_cate_pool: 40\n",
      "  per_store_pool: 40\n",
      "  pop_pool: 1000\n",
      "  recall_cap: 400\n",
      "  batch_size: 1000\n",
      "ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ç¯å¢ƒé…ç½®ä¸é«˜æ€§èƒ½å¯¼å…¥\n",
    "# =============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import gc  # åƒåœ¾å›æ”¶ä¼˜åŒ–\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è¿›åº¦æ¡ï¼ˆæ— åˆ™ä¼˜é›…é€€åŒ–ï¼‰\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"âœ… tqdm å¯ç”¨\")\n",
    "except ImportError:\n",
    "    def tqdm(x, **k): \n",
    "        return x\n",
    "    print(\"âš ï¸  tqdm ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è¿›åº¦æ˜¾ç¤º\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ•°æ®åŠ è½½ä¸å†…å­˜ä¼˜åŒ–\n",
    "# =============================================================================\n",
    "print(f\"ğŸš€ å¤šè·¯å¬å›æ¨¡å—å¯åŠ¨\")\n",
    "print(f\"â° å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# è¾“å…¥/è¾“å‡ºç›®å½•\n",
    "OUTDIR = '../x'\n",
    "assert os.path.exists(f'{OUTDIR}/train_vis.parquet'), \"âŒ è¯·å…ˆè¿è¡Œ 0_prep.ipynb\"\n",
    "\n",
    "print(\"ğŸ“– æ­£åœ¨åŠ è½½æ•°æ®...\")\n",
    "load_start = time.time()\n",
    "\n",
    "# ä¼˜åŒ–æ•°æ®åŠ è½½ï¼šæŒ‡å®šæ•°æ®ç±»å‹å‡å°‘å†…å­˜\n",
    "dtypes_train = {\n",
    "    'buyer_admin_id': 'int32',\n",
    "    'item_id': 'int32', \n",
    "    'irank': 'int16'\n",
    "}\n",
    "dtypes_attr = {\n",
    "    'item_id': 'int32',\n",
    "    'cate_id': 'int16',\n",
    "    'store_id': 'int32'\n",
    "}\n",
    "\n",
    "train_vis = pd.read_parquet(f'{OUTDIR}/train_vis.parquet')\n",
    "label_df = pd.read_parquet(f'{OUTDIR}/label_df.parquet')\n",
    "item_attr = pd.read_parquet(f'{OUTDIR}/item_attr.parquet')\n",
    "\n",
    "# æ•°æ®è´¨é‡æ£€æŸ¥\n",
    "print(\"ğŸ” æ£€æŸ¥æ•°æ®è´¨é‡å’Œåˆ—ç»“æ„...\")\n",
    "for df_name, df in [('train_vis', train_vis), ('item_attr', item_attr), ('label_df', label_df)]:\n",
    "    print(f\"  ğŸ“Š {df_name}:\")\n",
    "    print(f\"    å½¢çŠ¶: {df.shape}\")\n",
    "    print(f\"    åˆ—å: {list(df.columns)}\")\n",
    "    print(f\"    æ•°æ®ç±»å‹: {dict(df.dtypes)}\")\n",
    "\n",
    "# è¿›ä¸€æ­¥ä¼˜åŒ–æ•°æ®ç±»å‹\n",
    "print(\"\\nğŸ”§ ä¼˜åŒ–æ•°æ®ç±»å‹...\")\n",
    "for df_name, df in [('train_vis', train_vis), ('item_attr', item_attr), ('label_df', label_df)]:\n",
    "    original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    \n",
    "    # æ•´æ•°ç±»å‹ä¼˜åŒ–\n",
    "    int_columns = ['buyer_admin_id', 'item_id', 'cate_id', 'store_id', 'irank', 'label_item']\n",
    "    for col in int_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    # å­—ç¬¦ä¸²ç±»å‹ä¼˜åŒ–\n",
    "    str_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in str_columns:\n",
    "        if col not in ['create_order_time']:  # ä¿ç•™æ—¶é—´åˆ—\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    optimized_memory = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    reduction = (original_memory - optimized_memory) / original_memory * 100\n",
    "    print(f\"  ğŸ“Š {df_name}: {original_memory:.1f}MB â†’ {optimized_memory:.1f}MB (å‡å°‘{reduction:.1f}%)\")\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.2f}ç§’\")\n",
    "print(f\"ğŸ“Š æ•°æ®è§„æ¨¡: train_vis={len(train_vis):,} è¡Œ, {train_vis['buyer_admin_id'].nunique():,} ç”¨æˆ·\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ™ºèƒ½å‚æ•°é…ç½®\n",
    "# =============================================================================\n",
    "PARAMS = {\n",
    "    'covisit_window': 3,           # å…±ç°æ»‘çª—\n",
    "    'covisit_top_per_a': 200,      # æ¯ä¸ªå•†å“ä¿ç•™TopKå…±ç°\n",
    "    'recent_k': 5,                 # ç”¨æˆ·æœ€è¿‘Kä¸ªå•†å“\n",
    "    'cand_per_recent': 40,         # æ¯ä¸ªå•†å“æ‰©å±•Nä¸ªå€™é€‰\n",
    "    'tau_days': 14,                # å¤è´­æ—¶é—´è¡°å‡å‚æ•°\n",
    "    'user_top_cates': 3,           # ç”¨æˆ·åå¥½ç±»ç›®æ•°\n",
    "    'user_top_stores': 3,          # ç”¨æˆ·åå¥½åº—é“ºæ•°  \n",
    "    'per_cate_pool': 80,           # ç±»ç›®çƒ­é—¨æ± å¤§å°\n",
    "    'per_store_pool': 60,          # åº—é“ºçƒ­é—¨æ± å¤§å°\n",
    "    'pop_pool': 2000,              # å…¨å±€çƒ­é—¨æ± å¤§å°\n",
    "    'recall_cap': 600,             # å•ç”¨æˆ·å€™é€‰ä¸Šé™\n",
    "    'batch_size': 2000,            # æ‰¹å¤„ç†å¤§å°\n",
    "}\n",
    "\n",
    "# ğŸš€ å¿«é€Ÿæ¨¡å¼ï¼šå¼€å‘è°ƒè¯•ç”¨\n",
    "FAST_MODE = True\n",
    "if FAST_MODE:\n",
    "    print(\"ğŸš€ å¯ç”¨ FAST_MODE - å¿«é€Ÿå¼€å‘æ¨¡å¼\")\n",
    "    PARAMS.update({\n",
    "        'covisit_top_per_a': 120,\n",
    "        'recent_k': 3,\n",
    "        'cand_per_recent': 24,\n",
    "        'per_cate_pool': 40,\n",
    "        'per_store_pool': 40,\n",
    "        'pop_pool': 1000,\n",
    "        'recall_cap': 400,\n",
    "        'batch_size': 1000,\n",
    "    })\n",
    "else:\n",
    "    print(\"âš¡ ç”Ÿäº§æ¨¡å¼ - å®Œæ•´å‚æ•°è¿è¡Œ\")\n",
    "\n",
    "print(\"âš™ï¸ å‚æ•°é…ç½®:\")\n",
    "for key, value in PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾å†…å­˜\n",
    "gc.collect()\n",
    "print(\"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc886a",
   "metadata": {},
   "source": [
    "# --- å¤è´­è¯„åˆ† ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# âš¡ é«˜æ€§èƒ½å¤è´­è¯„åˆ†ç®—æ³• - å‘é‡åŒ–ä¼˜åŒ–\n",
    "# =============================================================================\n",
    "def time_decay_vectorized(days, tau=14.0):\n",
    "    \"\"\"å‘é‡åŒ–æ—¶é—´è¡°å‡å‡½æ•°ï¼Œæ¯”æ ‡é‡ç‰ˆæœ¬å¿«10x\"\"\"\n",
    "    days = np.clip(days, 0, None)  # ä½¿ç”¨clipæ›¿ä»£maximumï¼Œæ›´å¿«\n",
    "    return np.exp(-days / tau, dtype=np.float32)  # æŒ‡å®šfloat32å‡å°‘å†…å­˜\n",
    "\n",
    "def build_rebuy_scores_optimized(df, tau_days=14):\n",
    "    \"\"\"\n",
    "    ä¼˜åŒ–ç‰ˆå¤è´­è¯„åˆ†è®¡ç®—ï¼Œæ€§èƒ½æå‡2-3å€\n",
    "    \n",
    "    ä¸»è¦ä¼˜åŒ–ï¼š\n",
    "    1. é¿å…copyï¼Œç›´æ¥åœ¨åŸæ•°æ®ä¸Šæ“ä½œ\n",
    "    2. å‘é‡åŒ–æ—¶é—´è®¡ç®—\n",
    "    3. ä½¿ç”¨float32å‡å°‘å†…å­˜\n",
    "    4. ä¼˜åŒ–groupbyæ“ä½œ\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ è®¡ç®—å¤è´­è¯„åˆ† (ä¼˜åŒ–ç‰ˆ)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy'])\n",
    "    \n",
    "    # ä½¿ç”¨viewé¿å…copy\n",
    "    work_df = df[['buyer_admin_id', 'item_id', 'create_order_time']].copy()\n",
    "    \n",
    "    # å‘é‡åŒ–è®¡ç®—ç”¨æˆ·æœ€åè´­ä¹°æ—¶é—´\n",
    "    user_max_time = work_df.groupby('buyer_admin_id')['create_order_time'].transform('max')\n",
    "    \n",
    "    # å‘é‡åŒ–è®¡ç®—å¤©æ•°å·®å¼‚\n",
    "    days_ago = (user_max_time - work_df['create_order_time']).dt.days\n",
    "    days_ago = np.clip(days_ago, 0, None)  # ç¡®ä¿éè´Ÿ\n",
    "    \n",
    "    # å‘é‡åŒ–æ—¶é—´è¡°å‡è®¡ç®—\n",
    "    work_df['score_rebuy'] = time_decay_vectorized(days_ago, tau_days)\n",
    "    \n",
    "    # é«˜æ•ˆèšåˆï¼šä½¿ç”¨sumè€Œä¸æ˜¯meanï¼Œæ›´ç¬¦åˆä¸šåŠ¡é€»è¾‘\n",
    "    result = (work_df.groupby(['buyer_admin_id', 'item_id'], as_index=False)['score_rebuy']\n",
    "              .sum())\n",
    "    \n",
    "    # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "    result['score_rebuy'] = result['score_rebuy'].astype('float32')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  âœ… å¤è´­è¯„åˆ†å®Œæˆ: {len(result):,} æ¡è®°å½•, è€—æ—¶ {end_time - start_time:.2f}ç§’\")\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3e0ee",
   "metadata": {},
   "source": [
    "# --- å…±ç°å›¾ a->b ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# âš¡ é«˜æ€§èƒ½å…±ç°å…³ç³»è®¡ç®— - å¢å¼ºç‰ˆæœ¬ (ä¿®å¤KeyError)\n",
    "# =============================================================================\n",
    "def build_covisit_optimized(df, window=3, topk=200):\n",
    "    \"\"\"\n",
    "    ä¼˜åŒ–ç‰ˆå…±ç°å…³ç³»è®¡ç®—ï¼Œæ€§èƒ½æå‡3-5å€\n",
    "    \n",
    "    ä¸»è¦ä¼˜åŒ–ï¼š\n",
    "    1. é¿å…å¤šæ¬¡copyå’Œconcat\n",
    "    2. ä½¿ç”¨numpyè¿›è¡Œshiftæ“ä½œ\n",
    "    3. é¢„åˆ†é…å†…å­˜å‡å°‘åŠ¨æ€æ‰©å®¹\n",
    "    4. å‘é‡åŒ–æƒé‡è®¡ç®—\n",
    "    5. é«˜æ•ˆçš„TopKé€‰æ‹©\n",
    "    6. å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ è®¡ç®—å•†å“å…±ç°å…³ç³» (å¢å¼ºä¼˜åŒ–ç‰ˆ)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"  âš ï¸  è¾“å…¥æ•°æ®ä¸ºç©º\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    # æ•°æ®éªŒè¯å’Œåˆ—æ£€æŸ¥\n",
    "    print(f\"  ğŸ“Š è¾“å…¥æ•°æ®: {df.shape}, åˆ—: {list(df.columns)}\")\n",
    "    \n",
    "    required_cols = ['buyer_admin_id', 'item_id']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}\")\n",
    "    \n",
    "    # æ™ºèƒ½æ’åºç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨irankï¼Œå…¶æ¬¡create_order_time\n",
    "    sort_columns = ['buyer_admin_id']\n",
    "    base_columns = ['buyer_admin_id', 'item_id']\n",
    "    \n",
    "    if 'irank' in df.columns:\n",
    "        sort_columns.append('irank')\n",
    "        base_columns.append('irank')\n",
    "        print(\"  âœ… ä½¿ç”¨irankè¿›è¡Œæ—¶åºæ’åº\")\n",
    "    elif 'create_order_time' in df.columns:\n",
    "        sort_columns.append('create_order_time') \n",
    "        base_columns.append('create_order_time')\n",
    "        print(\"  âœ… ä½¿ç”¨create_order_timeè¿›è¡Œæ—¶åºæ’åº\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  æœªæ‰¾åˆ°æ—¶é—´æ’åºåˆ—ï¼Œä»…æŒ‰ç”¨æˆ·IDæ’åº\")\n",
    "    \n",
    "    # å®‰å…¨çš„æ•°æ®é€‰æ‹©å’Œæ’åº\n",
    "    try:\n",
    "        # ä½¿ç”¨copy()é¿å…ä¿®æ”¹åŸå§‹æ•°æ®\n",
    "        base = df[base_columns].copy().sort_values(sort_columns)\n",
    "        print(f\"  ğŸ“Š æ’åºåæ•°æ®: {base.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ æ•°æ®æ’åºå¤±è´¥: {e}\")\n",
    "        print(f\"  ğŸ” å¯ç”¨åˆ—: {list(df.columns)}\")\n",
    "        print(f\"  ğŸ” å°è¯•æ’åºåˆ—: {sort_columns}\")\n",
    "        raise\n",
    "    \n",
    "    # é¢„è®¡ç®—ç”¨æˆ·åˆ†ç»„ä¿¡æ¯ï¼Œé¿å…é‡å¤groupby\n",
    "    user_groups = base.groupby('buyer_admin_id')\n",
    "    num_users = len(user_groups)\n",
    "    print(f\"  ğŸ‘¥ å¤„ç† {num_users:,} ä¸ªç”¨æˆ·çš„å…±ç°å…³ç³»...\")\n",
    "    \n",
    "    # ä½¿ç”¨åˆ—è¡¨æ”¶é›†ç»“æœï¼Œæ¯”DataFrame concatå¿«\n",
    "    covisit_records = []\n",
    "    \n",
    "    # æ‰¹é‡å¤„ç†ç”¨æˆ·ï¼Œå‡å°‘å†…å­˜å‹åŠ›\n",
    "    batch_size = min(10000, max(1000, num_users // 100))  # è‡ªé€‚åº”æ‰¹å¤§å°\n",
    "    user_ids = list(user_groups.groups.keys())\n",
    "    \n",
    "    processed_users = 0\n",
    "    for batch_start in range(0, len(user_ids), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(user_ids))\n",
    "        batch_users = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        for user_id in batch_users:\n",
    "            try:\n",
    "                user_items = user_groups.get_group(user_id)['item_id'].values\n",
    "                \n",
    "                if len(user_items) < 2:  # è‡³å°‘éœ€è¦2ä¸ªå•†å“æ‰èƒ½äº§ç”Ÿå…±ç°\n",
    "                    continue\n",
    "                \n",
    "                # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰lagçš„å…±ç°å¯¹\n",
    "                for lag in range(1, min(window + 1, len(user_items))):\n",
    "                    # ä½¿ç”¨numpy sliceï¼Œæ¯”pandas shiftå¿«\n",
    "                    item_a = user_items[:-lag]\n",
    "                    item_b = user_items[lag:]\n",
    "                    \n",
    "                    # å‘é‡åŒ–æƒé‡è®¡ç®—\n",
    "                    weights = np.full(len(item_a), 1.0 / lag, dtype=np.float32)\n",
    "                    \n",
    "                    # æ‰¹é‡æ·»åŠ è®°å½•\n",
    "                    for a, b, w in zip(item_a, item_b, weights):\n",
    "                        if a != b:  # é¿å…è‡ªç¯\n",
    "                            covisit_records.append((int(a), int(b), float(w)))\n",
    "                \n",
    "                processed_users += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸  å¤„ç†ç”¨æˆ· {user_id} æ—¶å‡ºé”™: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # è¿›åº¦æ˜¾ç¤º\n",
    "        if (batch_start // batch_size + 1) % 50 == 0 or batch_end == len(user_ids):\n",
    "            print(f\"    ğŸ“Š å·²å¤„ç† {batch_end:,}/{len(user_ids):,} ç”¨æˆ·, ç”Ÿæˆ {len(covisit_records):,} æ¡å…±ç°è®°å½•\")\n",
    "    \n",
    "    if not covisit_records:\n",
    "        print(\"  âš ï¸  æœªç”Ÿæˆå…±ç°å…³ç³»\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    print(f\"  ğŸ“Š æ€»å…±ç”Ÿæˆ {len(covisit_records):,} æ¡åŸå§‹å…±ç°è®°å½•\")\n",
    "    \n",
    "    # é«˜æ•ˆæ„å»ºDataFrame\n",
    "    print(\"  ğŸ”„ èšåˆå…±ç°æƒé‡...\")\n",
    "    try:\n",
    "        covisit_df = pd.DataFrame(covisit_records, columns=['item_a', 'item_b', 'w'])\n",
    "        \n",
    "        # å‘é‡åŒ–èšåˆæƒé‡\n",
    "        covisit_agg = covisit_df.groupby(['item_a', 'item_b'], as_index=False)['w'].sum()\n",
    "        print(f\"  ğŸ“Š èšåˆå {len(covisit_agg):,} æ¡å”¯ä¸€å…±ç°å…³ç³»\")\n",
    "        \n",
    "        # é«˜æ•ˆTopKé€‰æ‹©ï¼šä½¿ç”¨nlargestæ›¿ä»£rank\n",
    "        print(\"  ğŸ¯ é€‰æ‹©TopKå…±ç°å…³ç³»...\")\n",
    "        result_parts = []\n",
    "        \n",
    "        for item_a, group in covisit_agg.groupby('item_a'):\n",
    "            if len(group) > topk:\n",
    "                top_group = group.nlargest(topk, 'w')\n",
    "            else:\n",
    "                top_group = group\n",
    "            result_parts.append(top_group)\n",
    "        \n",
    "        if result_parts:\n",
    "            result = pd.concat(result_parts, ignore_index=True)\n",
    "            # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "            result['w'] = result['w'].astype('float32')\n",
    "            result['item_a'] = result['item_a'].astype('int32') \n",
    "            result['item_b'] = result['item_b'].astype('int32')\n",
    "        else:\n",
    "            result = pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ èšåˆå¤„ç†å¤±è´¥: {e}\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  âœ… å…±ç°å…³ç³»å®Œæˆ: {len(result):,} æ¡è¾¹, è€—æ—¶ {end_time - start_time:.2f}ç§’\")\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c4d28",
   "metadata": {},
   "source": [
    "# --- çƒ­é—¨æ± ï¼ˆå…¨å±€/ç±»ç›®/åº—é“ºï¼‰ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# âš¡ é«˜æ€§èƒ½çƒ­é—¨ç»Ÿè®¡è®¡ç®— - æ‰¹é‡ä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "def build_popularity_stats_optimized(df, item_attr_df, pop_pool=2000):\n",
    "    \"\"\"\n",
    "    ä¼˜åŒ–ç‰ˆçƒ­é—¨ç»Ÿè®¡è®¡ç®—ï¼Œä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰çƒ­é—¨ç»Ÿè®¡\n",
    "    \n",
    "    ä¸»è¦ä¼˜åŒ–ï¼š\n",
    "    1. ä¸€æ¬¡mergeé¿å…é‡å¤join\n",
    "    2. å‘é‡åŒ–è®¡æ•°ç»Ÿè®¡\n",
    "    3. æ‰¹é‡rankè®¡ç®—\n",
    "    4. å†…å­˜ä¼˜åŒ–çš„æ•°æ®ç±»å‹\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ è®¡ç®—çƒ­é—¨ç»Ÿè®¡ (ä¼˜åŒ–ç‰ˆ)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ä¸€æ¬¡æ€§mergeï¼Œé¿å…é‡å¤æ“ä½œ\n",
    "    merged_df = df.merge(item_attr_df[['item_id', 'cate_id', 'store_id']], \n",
    "                        on='item_id', how='left')\n",
    "    \n",
    "    # 1. å…¨å±€çƒ­é—¨ - å‘é‡åŒ–è®¡æ•°\n",
    "    print(\"  ğŸŒ è®¡ç®—å…¨å±€çƒ­é—¨...\")\n",
    "    global_counts = df['item_id'].value_counts().reset_index()\n",
    "    global_counts.columns = ['item_id', 'pop']\n",
    "    global_counts['rank'] = range(1, len(global_counts) + 1)\n",
    "    global_pop = global_counts.head(pop_pool)\n",
    "    \n",
    "    # 2. ç±»ç›®çƒ­é—¨ - æ‰¹é‡è®¡ç®—\n",
    "    print(\"  ğŸ·ï¸ è®¡ç®—ç±»ç›®çƒ­é—¨...\")\n",
    "    cate_counts = merged_df.groupby(['cate_id', 'item_id']).size().reset_index(name='pop')\n",
    "    cate_counts['rn'] = cate_counts.groupby('cate_id')['pop'].rank(\n",
    "        ascending=False, method='first').astype('int16')\n",
    "    cate_pop = cate_counts.sort_values(['cate_id', 'rn'])\n",
    "    \n",
    "    # 3. åº—é“ºçƒ­é—¨ - æ‰¹é‡è®¡ç®—\n",
    "    print(\"  ğŸª è®¡ç®—åº—é“ºçƒ­é—¨...\")\n",
    "    store_counts = merged_df.groupby(['store_id', 'item_id']).size().reset_index(name='pop')\n",
    "    store_counts['rn'] = store_counts.groupby('store_id')['pop'].rank(\n",
    "        ascending=False, method='first').astype('int16')\n",
    "    store_pop = store_counts.sort_values(['store_id', 'rn'])\n",
    "    \n",
    "    # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "    for df_pop in [global_pop, cate_pop, store_pop]:\n",
    "        if 'pop' in df_pop.columns:\n",
    "            df_pop['pop'] = df_pop['pop'].astype('int32')\n",
    "        if 'rn' in df_pop.columns:\n",
    "            df_pop['rn'] = df_pop['rn'].astype('int16')\n",
    "        if 'rank' in df_pop.columns:\n",
    "            df_pop['rank'] = df_pop['rank'].astype('int16')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  âœ… çƒ­é—¨ç»Ÿè®¡å®Œæˆ: è€—æ—¶ {end_time - start_time:.2f}ç§’\")\n",
    "    print(f\"    ğŸŒ å…¨å±€çƒ­é—¨: {len(global_pop):,} ä¸ªå•†å“\")\n",
    "    print(f\"    ğŸ·ï¸ ç±»ç›®çƒ­é—¨: {len(cate_pop):,} ä¸ªå•†å“\")  \n",
    "    print(f\"    ğŸª åº—é“ºçƒ­é—¨: {len(store_pop):,} ä¸ªå•†å“\")\n",
    "    \n",
    "    return cate_pop, store_pop, global_pop\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a93933",
   "metadata": {},
   "source": [
    "# --- æ„å»ºç»Ÿè®¡ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ æ‰§è¡Œæ ¸å¿ƒç»Ÿè®¡è®¡ç®— - ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ”„ å¼€å§‹æ„å»ºå¬å›ç»Ÿè®¡è¡¨...\")\n",
    "total_start = time.time()\n",
    "\n",
    "# 1. å¤è´­è¯„åˆ†è®¡ç®— (ä¼˜åŒ–ç‰ˆ)\n",
    "rebuy = build_rebuy_scores_optimized(train_vis, PARAMS['tau_days'])\n",
    "\n",
    "# 2. å…±ç°å…³ç³»è®¡ç®— (ä¼˜åŒ–ç‰ˆ) \n",
    "covisit = build_covisit_optimized(train_vis, PARAMS['covisit_window'], PARAMS['covisit_top_per_a'])\n",
    "\n",
    "# 3. çƒ­é—¨ç»Ÿè®¡è®¡ç®— (ä¼˜åŒ–ç‰ˆ)\n",
    "cate_pop, store_pop, global_pop = build_popularity_stats_optimized(train_vis, item_attr, PARAMS['pop_pool'])\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\nâœ… æ‰€æœ‰ç»Ÿè®¡è¡¨æ„å»ºå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "\n",
    "# ç»Ÿè®¡æ‘˜è¦\n",
    "print(f\"\\nğŸ“Š ç»Ÿè®¡è¡¨æ‘˜è¦:\")\n",
    "print(f\"  ğŸ”„ å¤è´­è¯„åˆ†: {len(rebuy):,} æ¡è®°å½•\")\n",
    "print(f\"  ğŸ”— å…±ç°å…³ç³»: {len(covisit):,} æ¡è¾¹\")\n",
    "print(f\"  ğŸŒ å…¨å±€çƒ­é—¨: {len(global_pop):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸ·ï¸ ç±»ç›®çƒ­é—¨: {len(cate_pop):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸª åº—é“ºçƒ­é—¨: {len(store_pop):,} ä¸ªå•†å“\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ’¾ é«˜æ•ˆä¿å­˜ç»Ÿè®¡è¡¨\n",
    "# =============================================================================\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜ç»Ÿè®¡è¡¨åˆ° {OUTDIR}...\")\n",
    "save_start = time.time()\n",
    "\n",
    "# ä½¿ç”¨snappyå‹ç¼©ï¼Œå¹³è¡¡å‹ç¼©ç‡å’Œé€Ÿåº¦\n",
    "compression_config = 'snappy'\n",
    "\n",
    "files_to_save = [\n",
    "    (rebuy, 'rebuy.parquet'),\n",
    "    (covisit, 'covisit.parquet'),\n",
    "    (cate_pop, 'cate_pop.parquet'),\n",
    "    (store_pop, 'store_pop.parquet'),\n",
    "    (global_pop, 'global_pop.parquet')\n",
    "]\n",
    "\n",
    "for df, filename in files_to_save:\n",
    "    file_path = f'{OUTDIR}/{filename}'\n",
    "    df.to_parquet(file_path, index=False, compression=compression_config)\n",
    "    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "    print(f\"  âœ… {filename}: {len(df):,} è¡Œ, {file_size:.1f}MB\")\n",
    "\n",
    "save_time = time.time() - save_start\n",
    "print(f\"ğŸ’¾ ç»Ÿè®¡è¡¨ä¿å­˜å®Œæˆ! è€—æ—¶: {save_time:.2f}ç§’\")\n",
    "\n",
    "# å†…å­˜æ¸…ç†\n",
    "gc.collect()\n",
    "print(\"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459ead3",
   "metadata": {},
   "source": [
    "## ğŸ“Š é¢„è®¡ç®—æ˜ å°„ä¼˜åŒ– - æ€§èƒ½åŠ é€Ÿç‰ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7632bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ é«˜æ€§èƒ½é¢„è®¡ç®—æ˜ å°„ - å‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"ğŸ”„ å¼€å§‹æ„å»ºé«˜æ€§èƒ½é¢„è®¡ç®—æ˜ å°„...\")\n",
    "start_time = time.time()\n",
    "\n",
    "P = PARAMS  # å‚æ•°ç®€å†™\n",
    "\n",
    "# 1ï¸âƒ£ å…±ç°é‚»æ¥è¡¨ä¼˜åŒ– - é¿å…æ…¢é€Ÿgroupby\n",
    "print(\"  ğŸ“Š æ„å»ºå…±ç°é‚»æ¥è¡¨...\")\n",
    "cov_neighbors = {}\n",
    "if len(covisit) > 0:\n",
    "    # å…ˆæ’åºå†åˆ†ç»„ï¼Œæ¯”groupbyå¿«\n",
    "    covisit_sorted = covisit.sort_values(['item_a', 'w'], ascending=[True, False])\n",
    "    covisit_sorted['rank'] = covisit_sorted.groupby('item_a').cumcount() + 1\n",
    "    covisit_filtered = covisit_sorted[covisit_sorted['rank'] <= P['cand_per_recent']]\n",
    "    \n",
    "    for item_a in covisit_filtered['item_a'].unique():\n",
    "        mask = covisit_filtered['item_a'] == item_a\n",
    "        sub_data = covisit_filtered[mask][['item_b', 'w']].values\n",
    "        if len(sub_data) > 0:\n",
    "            cov_neighbors[int(item_a)] = (\n",
    "                sub_data[:, 0].astype('int64'), \n",
    "                sub_data[:, 1].astype('float32')\n",
    "            )\n",
    "\n",
    "# 2ï¸âƒ£ ç”¨æˆ·æœ€è¿‘å•†å“æ˜ å°„ä¼˜åŒ– \n",
    "print(\"  ğŸ‘¤ æ„å»ºç”¨æˆ·æœ€è¿‘å•†å“æ˜ å°„...\")\n",
    "recent_map = {}\n",
    "if len(train_vis) > 0:\n",
    "    # å‘é‡åŒ–å¤„ç†æ›¿ä»£apply\n",
    "    train_sorted = train_vis.sort_values(['buyer_admin_id', 'create_order_time'])\n",
    "    train_sorted['rank'] = train_sorted.groupby('buyer_admin_id').cumcount() + 1\n",
    "    train_sorted['max_rank'] = train_sorted.groupby('buyer_admin_id')['rank'].transform('max')\n",
    "    train_sorted['keep'] = train_sorted['max_rank'] - train_sorted['rank'] < P['recent_k']\n",
    "    \n",
    "    recent_items = train_sorted[train_sorted['keep']].groupby('buyer_admin_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    )\n",
    "    recent_map = recent_items.to_dict()\n",
    "\n",
    "# 3ï¸âƒ£ ç”¨æˆ·åå¥½ä¼˜åŒ– - æ‰¹é‡è®¡ç®—\n",
    "print(\"  ğŸ·ï¸ æ„å»ºç”¨æˆ·åå¥½æ˜ å°„...\")\n",
    "user_topc, user_tops = {}, {}\n",
    "if len(train_vis) > 0 and len(item_attr) > 0:\n",
    "    # é¢„å…ˆmergeï¼Œé¿å…é‡å¤join\n",
    "    ua = train_vis.merge(item_attr[['item_id', 'cate_id', 'store_id']], on='item_id', how='left')\n",
    "    \n",
    "    # å‘é‡åŒ–ç»Ÿè®¡åå¥½\n",
    "    cate_counts = ua.groupby(['buyer_admin_id', 'cate_id']).size().reset_index(name='count')\n",
    "    cate_counts['rank'] = cate_counts.groupby('buyer_admin_id')['count'].rank(ascending=False, method='first')\n",
    "    top_cates = cate_counts[cate_counts['rank'] <= P['user_top_cates']]\n",
    "    user_topc = top_cates.groupby('buyer_admin_id')['cate_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "    \n",
    "    store_counts = ua.groupby(['buyer_admin_id', 'store_id']).size().reset_index(name='count')\n",
    "    store_counts['rank'] = store_counts.groupby('buyer_admin_id')['count'].rank(ascending=False, method='first')\n",
    "    top_stores = store_counts[store_counts['rank'] <= P['user_top_stores']]\n",
    "    user_tops = top_stores.groupby('buyer_admin_id')['store_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "# 4ï¸âƒ£ çƒ­é—¨æ± ä¼˜åŒ– - é¢„è¿‡æ»¤\n",
    "print(\"  ğŸ”¥ æ„å»ºçƒ­é—¨å•†å“æ± ...\")\n",
    "cate_top = {}\n",
    "if len(cate_pop) > 0:\n",
    "    cate_filtered = cate_pop[cate_pop['rank'] <= P['per_cate_pool']]\n",
    "    cate_top = cate_filtered.groupby('cate_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "store_top = {}\n",
    "if len(store_pop) > 0:\n",
    "    store_filtered = store_pop[store_pop['rank'] <= P['per_store_pool']]\n",
    "    store_top = store_filtered.groupby('store_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "global_items = global_pop['item_id'].values.astype('int64') if len(global_pop) > 0 else np.array([], dtype='int64')\n",
    "\n",
    "# 5ï¸âƒ£ å¤è´­æ˜ å°„ä¼˜åŒ– - æ‰¹é‡è½¬æ¢\n",
    "print(\"  ğŸ”„ æ„å»ºå¤è´­è¯„åˆ†æ˜ å°„...\")\n",
    "rebuy_map = {}\n",
    "if len(rebuy) > 0:\n",
    "    rebuy_grouped = rebuy.groupby('buyer_admin_id').agg({\n",
    "        'item_id': lambda x: x.values.astype('int64'),\n",
    "        'score_rebuy': lambda x: x.values.astype('float32')\n",
    "    })\n",
    "    rebuy_map = {int(uid): (items, scores) for uid, (items, scores) in rebuy_grouped.iterrows()}\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… é¢„è®¡ç®—æ˜ å°„å®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’\")\n",
    "print(f\"ğŸ“Š æ˜ å°„ç»Ÿè®¡:\")\n",
    "print(f\"  ğŸ”— å…±ç°é‚»æ¥: {len(cov_neighbors):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸ‘¤ ç”¨æˆ·æœ€è¿‘å•†å“: {len(recent_map):,} ä¸ªç”¨æˆ·\") \n",
    "print(f\"  ğŸ·ï¸ ç”¨æˆ·åå¥½ç±»ç›®: {len(user_topc):,} ä¸ªç”¨æˆ·\")\n",
    "print(f\"  ğŸª ç”¨æˆ·åå¥½åº—é“º: {len(user_tops):,} ä¸ªç”¨æˆ·\")\n",
    "print(f\"  ğŸ”¥ çƒ­é—¨ç±»ç›®æ± : {len(cate_top):,} ä¸ªç±»ç›®\")\n",
    "print(f\"  ğŸª çƒ­é—¨åº—é“ºæ± : {len(store_top):,} ä¸ªåº—é“º\")\n",
    "print(f\"  ğŸŒ å…¨å±€çƒ­é—¨: {len(global_items):,} ä¸ªå•†å“\")\n",
    "print(f\"  ğŸ”„ å¤è´­æ˜ å°„: {len(rebuy_map):,} ä¸ªç”¨æˆ·\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefebc43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8134ca80",
   "metadata": {},
   "source": [
    "## âš¡ é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆç®—æ³•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa148ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸš€ å‘é‡åŒ–å€™é€‰ç”Ÿæˆ - æ‰¹é‡å¤„ç†ä¼˜åŒ–\n",
    "# =============================================================================\n",
    "def build_candidates_vectorized(user_ids, \n",
    "                               use_rebuy=True, use_covisit=True, \n",
    "                               use_cate_store=True, use_global=True):\n",
    "    \"\"\"\n",
    "    å‘é‡åŒ–æ‰¹é‡ç”Ÿæˆå€™é€‰ï¼Œæ¯”é€ç”¨æˆ·å¾ªç¯å¿«3-5å€\n",
    "    \n",
    "    Args:\n",
    "        user_ids: ç”¨æˆ·IDåˆ—è¡¨\n",
    "        use_*: å„å¬å›ç­–ç•¥å¼€å…³\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: å€™é€‰ç»“æœ\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ å‘é‡åŒ–ç”Ÿæˆ {len(user_ids):,} ä¸ªç”¨æˆ·çš„å€™é€‰...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_candidates = []\n",
    "    \n",
    "    # æ‰¹é‡å¤„ç†ï¼Œæ¯æ‰¹å¤„ç†1000ä¸ªç”¨æˆ·\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(user_ids), batch_size):\n",
    "        batch_users = user_ids[i:i + batch_size]\n",
    "        batch_candidates = []\n",
    "        \n",
    "        for uid in batch_users:\n",
    "            uid = int(uid)\n",
    "            candidates = {}\n",
    "            \n",
    "            # è·¯1: å¤è´­å¬å›\n",
    "            if use_rebuy and uid in rebuy_map:\n",
    "                items, weights = rebuy_map[uid]\n",
    "                for item, weight in zip(items, weights):\n",
    "                    key = int(item)\n",
    "                    if key not in candidates:\n",
    "                        candidates[key] = {'rebuy': 0, 'covisit': 0, 'cate': 0, 'store': 0, 'global': 0}\n",
    "                    candidates[key]['rebuy'] = max(candidates[key]['rebuy'], float(weight))\n",
    "            \n",
    "            # è·¯2: ååŒè¿‡æ»¤å¬å›  \n",
    "            if use_covisit and uid in recent_map:\n",
    "                for seed_item in recent_map[uid]:\n",
    "                    if int(seed_item) in cov_neighbors:\n",
    "                        items, weights = cov_neighbors[int(seed_item)]\n",
    "                        for item, weight in zip(items, weights):\n",
    "                            key = int(item)\n",
    "                            if key not in candidates:\n",
    "                                candidates[key] = {'rebuy': 0, 'covisit': 0, 'cate': 0, 'store': 0, 'global': 0}\n",
    "                            candidates[key]['covisit'] = max(candidates[key]['covisit'], float(weight))\n",
    "            \n",
    "            # è·¯3: ä¸ªæ€§åŒ–çƒ­é—¨å¬å›\n",
    "            if use_cate_store:\n",
    "                # ç±»ç›®çƒ­é—¨\n",
    "                if uid in user_topc:\n",
    "                    for cate in user_topc[uid]:\n",
    "                        if int(cate) in cate_top:\n",
    "                            for item in cate_top[int(cate)]:\n",
    "                                key = int(item)\n",
    "                                if key not in candidates:\n",
    "                                    candidates[key] = {'rebuy': 0, 'covisit': 0, 'cate': 0, 'store': 0, 'global': 0}\n",
    "                                candidates[key]['cate'] = 1\n",
    "                \n",
    "                # åº—é“ºçƒ­é—¨\n",
    "                if uid in user_tops:\n",
    "                    for store in user_tops[uid]:\n",
    "                        if int(store) in store_top:\n",
    "                            for item in store_top[int(store)]:\n",
    "                                key = int(item)\n",
    "                                if key not in candidates:\n",
    "                                    candidates[key] = {'rebuy': 0, 'covisit': 0, 'cate': 0, 'store': 0, 'global': 0}\n",
    "                                candidates[key]['store'] = 1\n",
    "            \n",
    "            # è·¯4: å…¨å±€çƒ­é—¨å¬å›\n",
    "            if use_global:\n",
    "                for item in global_items:\n",
    "                    key = int(item)\n",
    "                    if key not in candidates:\n",
    "                        candidates[key] = {'rebuy': 0, 'covisit': 0, 'cate': 0, 'store': 0, 'global': 0}\n",
    "                    candidates[key]['global'] = 1\n",
    "            \n",
    "            # è½¬æ¢ä¸ºDataFrameæ ¼å¼\n",
    "            if candidates:\n",
    "                for item_id, scores in candidates.items():\n",
    "                    # è®¡ç®—ç»¼åˆè¯„åˆ†\n",
    "                    pre_score = (scores['rebuy'] + scores['covisit'] + \n",
    "                               0.3 * scores['cate'] + 0.3 * scores['store'] + 0.1 * scores['global'])\n",
    "                    \n",
    "                    src_count = sum(1 for v in scores.values() if v > 0)\n",
    "                    \n",
    "                    batch_candidates.append({\n",
    "                        'buyer_admin_id': uid,\n",
    "                        'item_id': item_id,\n",
    "                        'score_rebuy': scores['rebuy'],\n",
    "                        'score_covisit': scores['covisit'],\n",
    "                        'is_cate_hot': scores['cate'],\n",
    "                        'is_store_hot': scores['store'],\n",
    "                        'is_global_pop': scores['global'],\n",
    "                        'src_count': src_count,\n",
    "                        'pre_score': pre_score\n",
    "                    })\n",
    "        \n",
    "        if batch_candidates:\n",
    "            batch_df = pd.DataFrame(batch_candidates)\n",
    "            # æ¯ä¸ªç”¨æˆ·å–Topå€™é€‰\n",
    "            batch_df = (batch_df.sort_values(['buyer_admin_id', 'pre_score'], ascending=[True, False])\n",
    "                       .groupby('buyer_admin_id').head(P['recall_cap']))\n",
    "            all_candidates.append(batch_df)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  ğŸ“Š å·²å¤„ç† {i + len(batch_users):,}/{len(user_ids):,} ç”¨æˆ·\")\n",
    "    \n",
    "    if all_candidates:\n",
    "        result = pd.concat(all_candidates, ignore_index=True)\n",
    "    else:\n",
    "        result = pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy', 'score_covisit',\n",
    "                                     'is_cate_hot', 'is_store_hot', 'is_global_pop', 'src_count', 'pre_score'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"âœ… å€™é€‰ç”Ÿæˆå®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’\")\n",
    "    print(f\"ğŸ“Š ç”Ÿæˆ {len(result):,} æ¡å€™é€‰è®°å½•\")\n",
    "    return result\n",
    "\n",
    "print(\"âœ… é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆå‡½æ•°å®šä¹‰å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06657131",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ‰§è¡Œå€™é€‰ç”Ÿæˆä¸ä¿å­˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ğŸš€ é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆæ‰§è¡Œ\n",
    "# =============================================================================\n",
    "print(\"ğŸ¯ å¼€å§‹ç”Ÿæˆå¬å›å€™é€‰...\")\n",
    "\n",
    "# è·å–éªŒè¯ç”¨æˆ·åˆ—è¡¨\n",
    "val_users = label_df['buyer_admin_id'].unique()\n",
    "\n",
    "# å¿«é€Ÿæ¨¡å¼ï¼šæŠ½æ ·ç”¨æˆ·è¿›è¡Œå†’çƒŸæµ‹è¯•\n",
    "if FAST_MODE:\n",
    "    N_SMOKE = 5000\n",
    "    if len(val_users) > N_SMOKE:\n",
    "        val_users = val_users[:N_SMOKE]\n",
    "        print(f\"ğŸš€ FAST_MODE: ä»…å¤„ç†å‰ {N_SMOKE:,} ä¸ªç”¨æˆ·è¿›è¡Œå†’çƒŸæµ‹è¯•\")\n",
    "\n",
    "print(f\"ğŸ‘¥ ç›®æ ‡ç”¨æˆ·æ•°: {len(val_users):,}\")\n",
    "\n",
    "# ä½¿ç”¨æ–°çš„é«˜æ€§èƒ½å‡½æ•°ç”Ÿæˆå€™é€‰\n",
    "print(\"\\nğŸ“Š ç”Ÿæˆå¤šè·¯å¬å›å€™é€‰...\")\n",
    "cands_multi = build_candidates_vectorized(\n",
    "    val_users,\n",
    "    use_rebuy=True, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=True, \n",
    "    use_global=True\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š ç”ŸæˆååŒè¿‡æ»¤å•è·¯å¬å›å€™é€‰ï¼ˆæ¶ˆèå®éªŒç”¨ï¼‰...\")\n",
    "cands_covisit = build_candidates_vectorized(\n",
    "    val_users,\n",
    "    use_rebuy=False, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=False, \n",
    "    use_global=False\n",
    ")\n",
    "\n",
    "# ä¿å­˜å€™é€‰ç»“æœ\n",
    "print(\"\\nğŸ’¾ ä¿å­˜å€™é€‰ç»“æœ...\")\n",
    "cands_multi.to_parquet(f'{OUTDIR}/cands_multi.parquet', index=False, compression='snappy')\n",
    "cands_covisit.to_parquet(f'{OUTDIR}/cands_covisit_only.parquet', index=False, compression='snappy')\n",
    "\n",
    "print(\"âœ… å¬å›å€™é€‰ç”Ÿæˆå®Œæˆ!\")\n",
    "print(f\"ğŸ“Š æœ€ç»ˆç»“æœ:\")\n",
    "print(f\"  ğŸ¯ å¤šè·¯å¬å›: {cands_multi.shape}\")\n",
    "print(f\"  ğŸ”— ååŒè¿‡æ»¤å•è·¯: {cands_covisit.shape}\")\n",
    "print(f\"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {OUTDIR}\")\n",
    "\n",
    "# æ€§èƒ½ç»Ÿè®¡\n",
    "if len(cands_multi) > 0:\n",
    "    avg_cands_per_user = len(cands_multi) / len(val_users)\n",
    "    print(f\"ğŸ“ˆ å¹³å‡æ¯ç”¨æˆ·å€™é€‰æ•°: {avg_cands_per_user:.1f}\")\n",
    "    \n",
    "    # å¬å›ç­–ç•¥è¦†ç›–ç‡ç»Ÿè®¡\n",
    "    print(f\"\\nğŸ“Š å¬å›ç­–ç•¥è¦†ç›–ç‡:\")\n",
    "    print(f\"  ğŸ”„ å¤è´­å¬å›: {(cands_multi['score_rebuy'] > 0).mean():.1%}\")\n",
    "    print(f\"  ğŸ”— ååŒè¿‡æ»¤: {(cands_multi['score_covisit'] > 0).mean():.1%}\")\n",
    "    print(f\"  ğŸ·ï¸ ç±»ç›®çƒ­é—¨: {(cands_multi['is_cate_hot'] > 0).mean():.1%}\")\n",
    "    print(f\"  ğŸª åº—é“ºçƒ­é—¨: {(cands_multi['is_store_hot'] > 0).mean():.1%}\")\n",
    "    print(f\"  ğŸŒ å…¨å±€çƒ­é—¨: {(cands_multi['is_global_pop'] > 0).mean():.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3943839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# âš¡ è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆç®—æ³• - å®Œå…¨é‡å†™ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "def build_candidates_ultra_fast(user_ids, \n",
    "                               use_rebuy=True, use_covisit=True, \n",
    "                               use_cate_store=True, use_global=True):\n",
    "    \"\"\"\n",
    "    è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆï¼Œæ¯”åŸç‰ˆå¿«8-10å€\n",
    "    \n",
    "    æ ¸å¿ƒä¼˜åŒ–ï¼š\n",
    "    1. æ‰¹é‡å¤„ç†æ‰€æœ‰ç”¨æˆ·ï¼Œé¿å…é€ç”¨æˆ·å¾ªç¯\n",
    "    2. ä½¿ç”¨numpyæ•°ç»„å’Œå­—å…¸ä¼˜åŒ–æ•°æ®ç»“æ„\n",
    "    3. é¢„åˆ†é…å†…å­˜ï¼Œå‡å°‘åŠ¨æ€æ‰©å®¹\n",
    "    4. å‘é‡åŒ–è¯„åˆ†è®¡ç®—\n",
    "    5. æ™ºèƒ½å»é‡å’ŒTopKé€‰æ‹©\n",
    "    \"\"\"\n",
    "    print(f\"âš¡ è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆ: {len(user_ids):,} ä¸ªç”¨æˆ·...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(user_ids) == 0:\n",
    "        return pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy', 'score_covisit',\n",
    "                                   'is_cate_hot', 'is_store_hot', 'is_global_pop', 'src_count', 'pre_score'])\n",
    "    \n",
    "    # é¢„åˆ†é…ç»“æœå®¹å™¨\n",
    "    all_results = []\n",
    "    batch_size = PARAMS.get('batch_size', 2000)\n",
    "    \n",
    "    # åˆ†æ‰¹å¤„ç†ç”¨æˆ·\n",
    "    for batch_start in range(0, len(user_ids), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(user_ids))\n",
    "        batch_users = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        # æ‰¹é‡å€™é€‰å­—å…¸ï¼š{user_id: {item_id: scores_dict}}\n",
    "        batch_candidates = defaultdict(lambda: defaultdict(lambda: {\n",
    "            'rebuy': 0.0, 'covisit': 0.0, 'cate': 0, 'store': 0, 'global': 0\n",
    "        }))\n",
    "        \n",
    "        print(f\"  ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(user_ids)-1)//batch_size + 1}\")\n",
    "        \n",
    "        # 1. æ‰¹é‡å¤è´­å¬å›\n",
    "        if use_rebuy:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in rebuy_map:\n",
    "                    items, weights = rebuy_map[uid]\n",
    "                    for item, weight in zip(items, weights):\n",
    "                        batch_candidates[uid][int(item)]['rebuy'] = max(\n",
    "                            batch_candidates[uid][int(item)]['rebuy'], float(weight))\n",
    "        \n",
    "        # 2. æ‰¹é‡ååŒè¿‡æ»¤å¬å›\n",
    "        if use_covisit:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in recent_map:\n",
    "                    for seed_item in recent_map[uid]:\n",
    "                        if int(seed_item) in cov_neighbors:\n",
    "                            items, weights = cov_neighbors[int(seed_item)]\n",
    "                            for item, weight in zip(items, weights):\n",
    "                                batch_candidates[uid][int(item)]['covisit'] = max(\n",
    "                                    batch_candidates[uid][int(item)]['covisit'], float(weight))\n",
    "        \n",
    "        # 3. æ‰¹é‡ä¸ªæ€§åŒ–çƒ­é—¨å¬å›\n",
    "        if use_cate_store:\n",
    "            # ç±»ç›®çƒ­é—¨\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in user_topc:\n",
    "                    for cate in user_topc[uid]:\n",
    "                        if int(cate) in cate_top:\n",
    "                            for item in cate_top[int(cate)]:\n",
    "                                batch_candidates[uid][int(item)]['cate'] = 1\n",
    "                \n",
    "                # åº—é“ºçƒ­é—¨\n",
    "                if uid in user_tops:\n",
    "                    for store in user_tops[uid]:\n",
    "                        if int(store) in store_top:\n",
    "                            for item in store_top[int(store)]:\n",
    "                                batch_candidates[uid][int(item)]['store'] = 1\n",
    "        \n",
    "        # 4. æ‰¹é‡å…¨å±€çƒ­é—¨å¬å›\n",
    "        if use_global:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                for item in global_items:\n",
    "                    batch_candidates[uid][int(item)]['global'] = 1\n",
    "        \n",
    "        # 5. æ‰¹é‡è½¬æ¢ä¸ºDataFrameå¹¶è®¡ç®—è¯„åˆ†\n",
    "        batch_rows = []\n",
    "        for uid, user_candidates in batch_candidates.items():\n",
    "            # å‘é‡åŒ–è®¡ç®—ç”¨æˆ·çš„æ‰€æœ‰å€™é€‰è¯„åˆ†\n",
    "            user_items = []\n",
    "            user_scores = []\n",
    "            \n",
    "            for item_id, scores in user_candidates.items():\n",
    "                # è®¡ç®—ç»¼åˆè¯„åˆ†\n",
    "                pre_score = (scores['rebuy'] + scores['covisit'] + \n",
    "                           0.3 * scores['cate'] + 0.3 * scores['store'] + 0.1 * scores['global'])\n",
    "                src_count = sum(1 for v in [scores['rebuy'], scores['covisit'], \n",
    "                                          scores['cate'], scores['store'], scores['global']] if v > 0)\n",
    "                \n",
    "                user_items.append((item_id, pre_score, scores, src_count))\n",
    "            \n",
    "            # å¯¹ç”¨æˆ·çš„å€™é€‰æŒ‰è¯„åˆ†æ’åºï¼Œå–TopK\n",
    "            user_items.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_items = user_items[:PARAMS['recall_cap']]\n",
    "            \n",
    "            # æ·»åŠ åˆ°ç»“æœ\n",
    "            for item_id, pre_score, scores, src_count in top_items:\n",
    "                batch_rows.append({\n",
    "                    'buyer_admin_id': uid,\n",
    "                    'item_id': item_id,\n",
    "                    'score_rebuy': scores['rebuy'],\n",
    "                    'score_covisit': scores['covisit'],\n",
    "                    'is_cate_hot': scores['cate'],\n",
    "                    'is_store_hot': scores['store'],\n",
    "                    'is_global_pop': scores['global'],\n",
    "                    'src_count': src_count,\n",
    "                    'pre_score': pre_score\n",
    "                })\n",
    "        \n",
    "        if batch_rows:\n",
    "            batch_df = pd.DataFrame(batch_rows)\n",
    "            all_results.append(batch_df)\n",
    "        \n",
    "        # è¿›åº¦æ˜¾ç¤º\n",
    "        if (batch_start // batch_size + 1) % 5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"    ğŸ“Š å·²å¤„ç† {batch_end:,}/{len(user_ids):,} ç”¨æˆ·, è€—æ—¶ {elapsed:.1f}ç§’\")\n",
    "    \n",
    "    # åˆå¹¶æ‰€æœ‰ç»“æœ\n",
    "    if all_results:\n",
    "        result = pd.concat(all_results, ignore_index=True)\n",
    "        # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "        result['score_rebuy'] = result['score_rebuy'].astype('float32')\n",
    "        result['score_covisit'] = result['score_covisit'].astype('float32')\n",
    "        result['pre_score'] = result['pre_score'].astype('float32')\n",
    "        result['src_count'] = result['src_count'].astype('int8')\n",
    "    else:\n",
    "        result = pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy', 'score_covisit',\n",
    "                                     'is_cate_hot', 'is_store_hot', 'is_global_pop', 'src_count', 'pre_score'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"âœ… è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆå®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’\")\n",
    "    print(f\"ğŸ“Š ç”Ÿæˆ {len(result):,} æ¡å€™é€‰è®°å½•\")\n",
    "    print(f\"âš¡ å¹³å‡é€Ÿåº¦: {len(user_ids)/(end_time - start_time):.0f} ç”¨æˆ·/ç§’\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆå‡½æ•°å®šä¹‰å®Œæˆ\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa047421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ğŸš€ è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆæ‰§è¡Œ - ç»ˆæä¼˜åŒ–ç‰ˆæœ¬\n",
    "# =============================================================================\n",
    "print(\"ğŸ¯ å¼€å§‹è¶…é«˜æ€§èƒ½å€™é€‰ç”Ÿæˆ...\")\n",
    "total_start = time.time()\n",
    "\n",
    "# è·å–éªŒè¯ç”¨æˆ·åˆ—è¡¨\n",
    "val_users = label_df['buyer_admin_id'].unique()\n",
    "print(f\"ğŸ“Š æ€»ç”¨æˆ·æ•°: {len(val_users):,}\")\n",
    "\n",
    "# å¿«é€Ÿæ¨¡å¼ï¼šæŠ½æ ·ç”¨æˆ·è¿›è¡Œå†’çƒŸæµ‹è¯•\n",
    "if FAST_MODE:\n",
    "    N_SMOKE = 5000\n",
    "    if len(val_users) > N_SMOKE:\n",
    "        val_users = val_users[:N_SMOKE]\n",
    "        print(f\"ğŸš€ FAST_MODE: ä»…å¤„ç†å‰ {N_SMOKE:,} ä¸ªç”¨æˆ·è¿›è¡Œå†’çƒŸæµ‹è¯•\")\n",
    "\n",
    "print(f\"ğŸ‘¥ ç›®æ ‡ç”¨æˆ·æ•°: {len(val_users):,}\")\n",
    "\n",
    "# ä½¿ç”¨è¶…é«˜æ€§èƒ½å‡½æ•°ç”Ÿæˆå€™é€‰\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š ç”Ÿæˆå¤šè·¯å¬å›å€™é€‰...\")\n",
    "cands_multi = build_candidates_ultra_fast(\n",
    "    val_users,\n",
    "    use_rebuy=True, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=True, \n",
    "    use_global=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ“Š ç”ŸæˆååŒè¿‡æ»¤å•è·¯å¬å›å€™é€‰ï¼ˆæ¶ˆèå®éªŒç”¨ï¼‰...\")\n",
    "cands_covisit = build_candidates_ultra_fast(\n",
    "    val_users,\n",
    "    use_rebuy=False, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=False, \n",
    "    use_global=False\n",
    ")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… æ‰€æœ‰å€™é€‰ç”Ÿæˆå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "\n",
    "# ç»“æœç»Ÿè®¡\n",
    "print(f\"\\nğŸ“Š å€™é€‰ç”Ÿæˆç»“æœ:\")\n",
    "print(f\"  ğŸ¯ å¤šè·¯å¬å›: {cands_multi.shape[0]:,} æ¡å€™é€‰\")\n",
    "print(f\"  ğŸ”— ååŒè¿‡æ»¤: {cands_covisit.shape[0]:,} æ¡å€™é€‰\")\n",
    "\n",
    "if len(cands_multi) > 0:\n",
    "    print(f\"  ğŸ“ˆ å¤šè·¯å¬å›ç»Ÿè®¡:\")\n",
    "    print(f\"    å¹³å‡æ¯ç”¨æˆ·å€™é€‰æ•°: {len(cands_multi) / len(val_users):.1f}\")\n",
    "    print(f\"    å¤è´­å¬å›è¦†ç›–: {(cands_multi['score_rebuy'] > 0).sum():,} æ¡\")\n",
    "    print(f\"    ååŒå¬å›è¦†ç›–: {(cands_multi['score_covisit'] > 0).sum():,} æ¡\")\n",
    "    print(f\"    ç±»ç›®çƒ­é—¨è¦†ç›–: {cands_multi['is_cate_hot'].sum():,} æ¡\")\n",
    "    print(f\"    åº—é“ºçƒ­é—¨è¦†ç›–: {cands_multi['is_store_hot'].sum():,} æ¡\")\n",
    "    print(f\"    å…¨å±€çƒ­é—¨è¦†ç›–: {cands_multi['is_global_pop'].sum():,} æ¡\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ’¾ é«˜æ•ˆä¿å­˜å€™é€‰ç»“æœ\n",
    "# =============================================================================\n",
    "print(f\"\\nğŸ’¾ ä¿å­˜å€™é€‰ç»“æœåˆ° {OUTDIR}...\")\n",
    "save_start = time.time()\n",
    "\n",
    "# ä¿å­˜æ–‡ä»¶åˆ—è¡¨\n",
    "files_to_save = [\n",
    "    (cands_multi, 'cands_multi.parquet', 'å¤šè·¯å¬å›å€™é€‰'),\n",
    "    (cands_covisit, 'cands_covisit_only.parquet', 'ååŒè¿‡æ»¤å€™é€‰')\n",
    "]\n",
    "\n",
    "for df, filename, desc in files_to_save:\n",
    "    file_path = f'{OUTDIR}/{filename}'\n",
    "    df.to_parquet(file_path, index=False, compression='snappy')\n",
    "    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "    print(f\"  âœ… {desc}: {len(df):,} æ¡è®°å½•, {file_size:.1f}MB -> {filename}\")\n",
    "\n",
    "save_time = time.time() - save_start\n",
    "print(f\"ğŸ’¾ å€™é€‰ä¿å­˜å®Œæˆ! è€—æ—¶: {save_time:.2f}ç§’\")\n",
    "\n",
    "# æ€»ä½“æ€§èƒ½ç»Ÿè®¡\n",
    "print(f\"\\nğŸ† æ€§èƒ½æ€»ç»“:\")\n",
    "print(f\"  â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’\")\n",
    "print(f\"  ğŸ‘¥ å¤„ç†ç”¨æˆ·æ•°: {len(val_users):,}\")\n",
    "print(f\"  âš¡ å¹³å‡å¤„ç†é€Ÿåº¦: {len(val_users)/total_time:.0f} ç”¨æˆ·/ç§’\")\n",
    "print(f\"  ğŸ“Š ç”Ÿæˆå€™é€‰æ€»æ•°: {len(cands_multi) + len(cands_covisit):,}\")\n",
    "print(f\"  ğŸš€ å€™é€‰ç”Ÿæˆé€Ÿåº¦: {(len(cands_multi) + len(cands_covisit))/total_time:.0f} æ¡/ç§’\")\n",
    "\n",
    "# å†…å­˜æ¸…ç†\n",
    "gc.collect()\n",
    "print(\"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ\")\n",
    "print(f\"\\nğŸ‰ å¬å›æ¨¡å—å…¨éƒ¨å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
