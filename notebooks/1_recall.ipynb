{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "132f17dc",
   "metadata": {},
   "source": [
    "# 🎯 多路召回模块 (1_recall.ipynb)\n",
    "\n",
    "## 📋 模块功能\n",
    "实现**4种召回策略**，为每个用户生成多样化的候选商品集合：\n",
    "\n",
    "1. **🔄 复购召回**: 基于用户历史购买 + 时间衰减\n",
    "2. **🔗 协同过滤召回**: 基于商品共现关系\n",
    "3. **🏪 个性化热门**: 用户偏好类目/店铺热门  \n",
    "4. **🌍 全局热门**: 冷启动补充\n",
    "\n",
    "## ⚡ 性能优化\n",
    "- **FAST_MODE**: 开发模式参数调整\n",
    "- **内存优化**: dtype压缩减少内存占用\n",
    "- **预计算加速**: 邻接表、映射字典等\n",
    "- **批处理**: 纯字典 + numpy 避免频繁join\n",
    "\n",
    "## 🔧 输出文件\n",
    "- **统计表**: rebuy, covisit, cate_pop, store_pop, global_pop\n",
    "- **候选集**: cands_multi (多路), cands_covisit_only (单路/消融)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5d0d1",
   "metadata": {},
   "source": [
    "## 1️⃣ 环境配置与数据加载\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323b2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 依赖库导入\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 配置参数\n",
    "OUTDIR = '../x'\n",
    "FAST_MODE = True  # 开发模式，使用较小的参数\n",
    "\n",
    "print(\"✅ 环境配置完成\")\n",
    "print(f\"📁 输出目录: {OUTDIR}\")\n",
    "print(f\"⚡ 快速模式: {'启用' if FAST_MODE else '禁用'}\")\n",
    "print(f\"⏰ 处理时间: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 数据加载\n",
    "# =============================================================================\n",
    "print(\"📂 开始加载数据...\")\n",
    "\n",
    "# 加载商品属性数据\n",
    "item_attr = pd.read_parquet(f'{OUTDIR}/item_attr.parquet')\n",
    "print(f\"✅ 商品属性数据: {len(item_attr):,} 条记录\")\n",
    "\n",
    "# 检查是否已经有抽样数据（从0_prep.ipynb生成）\n",
    "if os.path.exists(f\"{OUTDIR}/train_vis_sampled.parquet\"):\n",
    "    print(\"✅ 发现抽样数据，使用0_prep.ipynb的抽样结果\")\n",
    "    train_vis = pd.read_parquet(f\"{OUTDIR}/train_vis_sampled.parquet\")\n",
    "    label_df = pd.read_parquet(f\"{OUTDIR}/label_df_sampled.parquet\")\n",
    "    print(f\"📊 使用抽样数据: {len(train_vis):,} 条记录, {train_vis['buyer_admin_id'].nunique():,} 用户\")\n",
    "else:\n",
    "    print(\"🎯 使用全量数据（未进行抽样）\")\n",
    "    train_vis = pd.read_parquet(f\"{OUTDIR}/train_vis.parquet\")\n",
    "    label_df = pd.read_parquet(f\"{OUTDIR}/label_df.parquet\")\n",
    "    print(f\"📊 全量数据: {len(train_vis):,} 条记录, {train_vis['buyer_admin_id'].nunique():,} 用户\")\n",
    "\n",
    "print(\"✅ 数据加载完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 参数配置 - 贝叶斯优化后的最优参数\n",
    "# =============================================================================\n",
    "PARAMS = dict(\n",
    "    covisit_window=4, covisit_top_per_a=317,  # 贝叶斯优化: 4, 317\n",
    "    recent_k=4, cand_per_recent=69,          # 贝叶斯优化: 4, 69\n",
    "    tau_days=11,                             # 贝叶斯优化: 11\n",
    "    user_top_cates=3, user_top_stores=3,     # 保持原值\n",
    "    per_cate_pool=38, per_store_pool=96,     # 贝叶斯优化: 38, 96\n",
    "    pop_pool=4863, recall_cap=866,           # 贝叶斯优化: 4863, 866\n",
    "    batch_size=2000,                         # 批处理大小\n",
    ")\n",
    "\n",
    "print(\"✅ 参数配置完成\")\n",
    "print(\"📊 贝叶斯优化后的最优参数:\")\n",
    "for key, value in PARAMS.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "# 快速模式参数调整\n",
    "if FAST_MODE:\n",
    "    print(\"\\\\n⚡ 快速模式参数调整:\")\n",
    "    PARAMS['covisit_top_per_a'] = min(PARAMS['covisit_top_per_a'], 100)\n",
    "    PARAMS['per_cate_pool'] = min(PARAMS['per_cate_pool'], 20)\n",
    "    PARAMS['per_store_pool'] = min(PARAMS['per_store_pool'], 20)\n",
    "    PARAMS['pop_pool'] = min(PARAMS['pop_pool'], 1000)\n",
    "    PARAMS['recall_cap'] = min(PARAMS['recall_cap'], 200)\n",
    "    PARAMS['batch_size'] = min(PARAMS['batch_size'], 1000)\n",
    "    \n",
    "    for key, value in PARAMS.items():\n",
    "        print(f\"  - {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80edcd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:25.836323Z",
     "iopub.status.busy": "2025-09-16T05:25:25.835991Z",
     "iopub.status.idle": "2025-09-16T05:25:26.026369Z",
     "shell.execute_reply": "2025-09-16T05:25:26.026134Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 检查是否已经有抽样数据（从0_prep.ipynb生成）\n",
    "if os.path.exists(f\"{OUTDIR}/train_vis_sampled.parquet\"):\n",
    "    print(\"✅ 发现抽样数据，使用0_prep.ipynb的抽样结果\")\n",
    "    train_vis = pd.read_parquet(f\"{OUTDIR}/train_vis_sampled.parquet\")\n",
    "    label_df = pd.read_parquet(f\"{OUTDIR}/label_df_sampled.parquet\")\n",
    "    print(f\"📊 使用抽样数据: {len(train_vis):,} 条记录, {train_vis['buyer_admin_id'].nunique():,} 用户\")\n",
    "else:\n",
    "    print(\"🎯 使用全量数据（未进行抽样）\")\n",
    "    print(f\"📊 全量数据: {len(train_vis):,} 条记录, {train_vis['buyer_admin_id'].nunique():,} 用户\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc886a",
   "metadata": {},
   "source": [
    "# --- 复购评分 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718dd8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.027697Z",
     "iopub.status.busy": "2025-09-16T05:25:26.027615Z",
     "iopub.status.idle": "2025-09-16T05:25:26.030477Z",
     "shell.execute_reply": "2025-09-16T05:25:26.030280Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ⚡ 高性能复购评分算法 - 向量化优化\n",
    "# =============================================================================\n",
    "def time_decay_vectorized(days, tau=14.0):\n",
    "    \"\"\"向量化时间衰减函数，比标量版本快10x\"\"\"\n",
    "    days = np.clip(days, 0, None)  # 使用clip替代maximum，更快\n",
    "    return np.exp(-days / tau, dtype=np.float32)  # 指定float32减少内存\n",
    "\n",
    "def build_rebuy_scores_optimized(df, tau_days=14):\n",
    "    \"\"\"\n",
    "    优化版复购评分计算，性能提升2-3倍\n",
    "    \n",
    "    主要优化：\n",
    "    1. 避免copy，直接在原数据上操作\n",
    "    2. 向量化时间计算\n",
    "    3. 使用float32减少内存\n",
    "    4. 优化groupby操作\n",
    "    \"\"\"\n",
    "    print(\"🔄 计算复购评分 (优化版)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy'])\n",
    "    \n",
    "    # 使用view避免copy\n",
    "    work_df = df[['buyer_admin_id', 'item_id', 'create_order_time']].copy()\n",
    "    \n",
    "    # 向量化计算用户最后购买时间\n",
    "    user_max_time = work_df.groupby('buyer_admin_id')['create_order_time'].transform('max')\n",
    "    \n",
    "    # 向量化计算天数差异\n",
    "    days_ago = (user_max_time - work_df['create_order_time']).dt.days\n",
    "    days_ago = np.clip(days_ago, 0, None)  # 确保非负\n",
    "    \n",
    "    # 向量化时间衰减计算\n",
    "    work_df['score_rebuy'] = time_decay_vectorized(days_ago, tau_days)\n",
    "    \n",
    "    # 高效聚合：使用sum而不是mean，更符合业务逻辑\n",
    "    result = (work_df.groupby(['buyer_admin_id', 'item_id'], as_index=False)['score_rebuy']\n",
    "              .sum())\n",
    "    \n",
    "    # 数据类型优化\n",
    "    result['score_rebuy'] = result['score_rebuy'].astype('float32')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  ✅ 复购评分完成: {len(result):,} 条记录, 耗时 {end_time - start_time:.2f}秒\")\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3e0ee",
   "metadata": {},
   "source": [
    "# --- 共现图 a->b ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8c972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.031634Z",
     "iopub.status.busy": "2025-09-16T05:25:26.031565Z",
     "iopub.status.idle": "2025-09-16T05:25:26.037343Z",
     "shell.execute_reply": "2025-09-16T05:25:26.037142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ⚡ 高性能共现关系计算 - 增强版本 (修复KeyError)\n",
    "# =============================================================================\n",
    "def build_covisit_optimized(df, window=3, topk=200):\n",
    "    \"\"\"\n",
    "    优化版共现关系计算，性能提升3-5倍\n",
    "    \n",
    "    主要优化：\n",
    "    1. 避免多次copy和concat\n",
    "    2. 使用numpy进行shift操作\n",
    "    3. 预分配内存减少动态扩容\n",
    "    4. 向量化权重计算\n",
    "    5. 高效的TopK选择\n",
    "    6. 增强的错误处理和数据验证\n",
    "    \"\"\"\n",
    "    print(\"🔄 计算商品共现关系 (增强优化版)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"  ⚠️  输入数据为空\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    # 数据验证和列检查\n",
    "    print(f\"  📊 输入数据: {df.shape}, 列: {list(df.columns)}\")\n",
    "    \n",
    "    required_cols = ['buyer_admin_id', 'item_id']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"缺少必要列: {missing_cols}\")\n",
    "    \n",
    "    # 智能排序策略：优先使用irank，其次create_order_time\n",
    "    sort_columns = ['buyer_admin_id']\n",
    "    base_columns = ['buyer_admin_id', 'item_id']\n",
    "    \n",
    "    if 'irank' in df.columns:\n",
    "        sort_columns.append('irank')\n",
    "        base_columns.append('irank')\n",
    "        print(\"  ✅ 使用irank进行时序排序\")\n",
    "    elif 'create_order_time' in df.columns:\n",
    "        sort_columns.append('create_order_time') \n",
    "        base_columns.append('create_order_time')\n",
    "        print(\"  ✅ 使用create_order_time进行时序排序\")\n",
    "    else:\n",
    "        print(\"  ⚠️  未找到时间排序列，仅按用户ID排序\")\n",
    "    \n",
    "    # 安全的数据选择和排序\n",
    "    try:\n",
    "        # 使用copy()避免修改原始数据\n",
    "        base = df[base_columns].copy().sort_values(sort_columns)\n",
    "        print(f\"  📊 排序后数据: {base.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 数据排序失败: {e}\")\n",
    "        print(f\"  🔍 可用列: {list(df.columns)}\")\n",
    "        print(f\"  🔍 尝试排序列: {sort_columns}\")\n",
    "        raise\n",
    "    \n",
    "    # 预计算用户分组信息，避免重复groupby\n",
    "    user_groups = base.groupby('buyer_admin_id')\n",
    "    num_users = len(user_groups)\n",
    "    print(f\"  👥 处理 {num_users:,} 个用户的共现关系...\")\n",
    "    \n",
    "    # 使用列表收集结果，比DataFrame concat快\n",
    "    covisit_records = []\n",
    "    \n",
    "    # 批量处理用户，减少内存压力\n",
    "    batch_size = min(10000, max(1000, num_users // 100))  # 自适应批大小\n",
    "    user_ids = list(user_groups.groups.keys())\n",
    "    \n",
    "    processed_users = 0\n",
    "    for batch_start in range(0, len(user_ids), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(user_ids))\n",
    "        batch_users = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        for user_id in batch_users:\n",
    "            try:\n",
    "                user_items = user_groups.get_group(user_id)['item_id'].values\n",
    "                \n",
    "                if len(user_items) < 2:  # 至少需要2个商品才能产生共现\n",
    "                    continue\n",
    "                \n",
    "                # 向量化计算所有lag的共现对\n",
    "                for lag in range(1, min(window + 1, len(user_items))):\n",
    "                    # 使用numpy slice，比pandas shift快\n",
    "                    item_a = user_items[:-lag]\n",
    "                    item_b = user_items[lag:]\n",
    "                    \n",
    "                    # 向量化权重计算\n",
    "                    weights = np.full(len(item_a), 1.0 / lag, dtype=np.float32)\n",
    "                    \n",
    "                    # 批量添加记录\n",
    "                    for a, b, w in zip(item_a, item_b, weights):\n",
    "                        if a != b:  # 避免自环\n",
    "                            covisit_records.append((int(a), int(b), float(w)))\n",
    "                \n",
    "                processed_users += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ⚠️  处理用户 {user_id} 时出错: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 进度显示\n",
    "        if (batch_start // batch_size + 1) % 50 == 0 or batch_end == len(user_ids):\n",
    "            print(f\"    📊 已处理 {batch_end:,}/{len(user_ids):,} 用户, 生成 {len(covisit_records):,} 条共现记录\")\n",
    "    \n",
    "    if not covisit_records:\n",
    "        print(\"  ⚠️  未生成共现关系\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    print(f\"  📊 总共生成 {len(covisit_records):,} 条原始共现记录\")\n",
    "    \n",
    "    # 高效构建DataFrame\n",
    "    print(\"  🔄 聚合共现权重...\")\n",
    "    try:\n",
    "        covisit_df = pd.DataFrame(covisit_records, columns=['item_a', 'item_b', 'w'])\n",
    "        \n",
    "        # 向量化聚合权重\n",
    "        covisit_agg = covisit_df.groupby(['item_a', 'item_b'], as_index=False)['w'].sum()\n",
    "        print(f\"  📊 聚合后 {len(covisit_agg):,} 条唯一共现关系\")\n",
    "        \n",
    "        # 高效TopK选择：使用nlargest替代rank\n",
    "        print(\"  🎯 选择TopK共现关系...\")\n",
    "        result_parts = []\n",
    "        \n",
    "        for item_a, group in covisit_agg.groupby('item_a'):\n",
    "            if len(group) > topk:\n",
    "                top_group = group.nlargest(topk, 'w')\n",
    "            else:\n",
    "                top_group = group\n",
    "            result_parts.append(top_group)\n",
    "        \n",
    "        if result_parts:\n",
    "            result = pd.concat(result_parts, ignore_index=True)\n",
    "            # 数据类型优化\n",
    "            result['w'] = result['w'].astype('float32')\n",
    "            result['item_a'] = result['item_a'].astype('int32') \n",
    "            result['item_b'] = result['item_b'].astype('int32')\n",
    "        else:\n",
    "            result = pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 聚合处理失败: {e}\")\n",
    "        return pd.DataFrame(columns=['item_a', 'item_b', 'w'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  ✅ 共现关系完成: {len(result):,} 条边, 耗时 {end_time - start_time:.2f}秒\")\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c4d28",
   "metadata": {},
   "source": [
    "# --- 热门池（全局/类目/店铺） ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f115f087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.038509Z",
     "iopub.status.busy": "2025-09-16T05:25:26.038439Z",
     "iopub.status.idle": "2025-09-16T05:25:26.041190Z",
     "shell.execute_reply": "2025-09-16T05:25:26.041005Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ⚡ 高性能热门统计计算 - 批量优化版本\n",
    "# =============================================================================\n",
    "def build_popularity_stats_optimized(df, item_attr_df, pop_pool=2000):\n",
    "    \"\"\"\n",
    "    优化版热门统计计算，一次性计算所有热门统计\n",
    "    \n",
    "    主要优化：\n",
    "    1. 一次merge避免重复join\n",
    "    2. 向量化计数统计\n",
    "    3. 批量rank计算\n",
    "    4. 内存优化的数据类型\n",
    "    \"\"\"\n",
    "    print(\"🔄 计算热门统计 (优化版)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 一次性merge，避免重复操作\n",
    "    merged_df = df.merge(item_attr_df[['item_id', 'cate_id', 'store_id']], \n",
    "                        on='item_id', how='left')\n",
    "    \n",
    "    # 1. 全局热门 - 向量化计数\n",
    "    print(\"  🌍 计算全局热门...\")\n",
    "    global_counts = df['item_id'].value_counts().reset_index()\n",
    "    global_counts.columns = ['item_id', 'pop']\n",
    "    global_counts['rank'] = range(1, len(global_counts) + 1)\n",
    "    global_pop = global_counts.head(pop_pool)\n",
    "    \n",
    "    # 2. 类目热门 - 批量计算\n",
    "    print(\"  🏷️ 计算类目热门...\")\n",
    "    cate_counts = merged_df.groupby(['cate_id', 'item_id']).size().reset_index(name='pop')\n",
    "    cate_counts['rank'] = cate_counts.groupby('cate_id')['pop'].rank(\n",
    "        ascending=False, method='first').astype('int16')\n",
    "    cate_pop = cate_counts.sort_values(['cate_id', 'rank'])\n",
    "    \n",
    "    # 3. 店铺热门 - 批量计算\n",
    "    print(\"  🏪 计算店铺热门...\")\n",
    "    store_counts = merged_df.groupby(['store_id', 'item_id']).size().reset_index(name='pop')\n",
    "    store_counts['rank'] = store_counts.groupby('store_id')['pop'].rank(\n",
    "        ascending=False, method='first').astype('int16')\n",
    "    store_pop = store_counts.sort_values(['store_id', 'rank'])\n",
    "    \n",
    "    # 数据类型优化\n",
    "    for df_pop in [global_pop, cate_pop, store_pop]:\n",
    "        if 'pop' in df_pop.columns:\n",
    "            df_pop['pop'] = df_pop['pop'].astype('int32')\n",
    "        if 'rank' in df_pop.columns:\n",
    "            df_pop['rank'] = df_pop['rank'].astype('int16')\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"  ✅ 热门统计完成: 耗时 {end_time - start_time:.2f}秒\")\n",
    "    print(f\"    🌍 全局热门: {len(global_pop):,} 个商品\")\n",
    "    print(f\"    🏷️ 类目热门: {len(cate_pop):,} 个商品\")  \n",
    "    print(f\"    🏪 店铺热门: {len(store_pop):,} 个商品\")\n",
    "    \n",
    "    return cate_pop, store_pop, global_pop\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a93933",
   "metadata": {},
   "source": [
    "# --- 构建统计 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9d6899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.042250Z",
     "iopub.status.busy": "2025-09-16T05:25:26.042180Z",
     "iopub.status.idle": "2025-09-16T05:25:26.052756Z",
     "shell.execute_reply": "2025-09-16T05:25:26.052496Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 🚀 执行核心统计计算 - 使用优化版本\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🔄 开始构建召回统计表...\")\n",
    "total_start = time.time()\n",
    "\n",
    "# 1. 复购评分计算 (优化版)\n",
    "rebuy = build_rebuy_scores_optimized(train_vis, PARAMS['tau_days'])\n",
    "\n",
    "# 2. 共现关系计算 (优化版) \n",
    "covisit = build_covisit_optimized(train_vis, PARAMS['covisit_window'], PARAMS['covisit_top_per_a'])\n",
    "\n",
    "# 3. 热门统计计算 (优化版)\n",
    "cate_pop, store_pop, global_pop = build_popularity_stats_optimized(train_vis, item_attr, PARAMS['pop_pool'])\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n✅ 所有统计表构建完成! 总耗时: {total_time:.2f}秒\")\n",
    "\n",
    "# 统计摘要\n",
    "print(f\"\\n📊 统计表摘要:\")\n",
    "print(f\"  🔄 复购评分: {len(rebuy):,} 条记录\")\n",
    "print(f\"  🔗 共现关系: {len(covisit):,} 条边\")\n",
    "print(f\"  🌍 全局热门: {len(global_pop):,} 个商品\")\n",
    "print(f\"  🏷️ 类目热门: {len(cate_pop):,} 个商品\")\n",
    "print(f\"  🏪 店铺热门: {len(store_pop):,} 个商品\")\n",
    "\n",
    "# =============================================================================\n",
    "# 💾 高效保存统计表\n",
    "# =============================================================================\n",
    "print(f\"\\n💾 保存统计表到 {OUTDIR}...\")\n",
    "save_start = time.time()\n",
    "\n",
    "# 使用snappy压缩，平衡压缩率和速度\n",
    "compression_config = 'snappy'\n",
    "\n",
    "files_to_save = [\n",
    "    (rebuy, 'rebuy.parquet'),\n",
    "    (covisit, 'covisit.parquet'),\n",
    "    (cate_pop, 'cate_pop.parquet'),\n",
    "    (store_pop, 'store_pop.parquet'),\n",
    "    (global_pop, 'global_pop.parquet')\n",
    "]\n",
    "\n",
    "for df, filename in files_to_save:\n",
    "    file_path = f'{OUTDIR}/{filename}'\n",
    "    df.to_parquet(file_path, index=False, compression=compression_config)\n",
    "    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "    print(f\"  ✅ {filename}: {len(df):,} 行, {file_size:.1f}MB\")\n",
    "\n",
    "save_time = time.time() - save_start\n",
    "print(f\"💾 统计表保存完成! 耗时: {save_time:.2f}秒\")\n",
    "\n",
    "# 内存清理\n",
    "gc.collect()\n",
    "print(\"🧹 内存清理完成\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459ead3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cde4b7af",
   "metadata": {},
   "source": [
    "## 📊 预计算映射优化 - 性能加速版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7632bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.053932Z",
     "iopub.status.busy": "2025-09-16T05:25:26.053855Z",
     "iopub.status.idle": "2025-09-16T05:25:26.073765Z",
     "shell.execute_reply": "2025-09-16T05:25:26.073558Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 🚀 高性能预计算映射 - 向量化优化版本\n",
    "# =============================================================================\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"🔄 开始构建高性能预计算映射...\")\n",
    "start_time = time.time()\n",
    "\n",
    "P = PARAMS  # 参数简写\n",
    "\n",
    "# 1️⃣ 共现邻接表优化 - 避免慢速groupby\n",
    "print(\"  📊 构建共现邻接表...\")\n",
    "cov_neighbors = {}\n",
    "if len(covisit) > 0:\n",
    "    # 先排序再分组，比groupby快\n",
    "    covisit_sorted = covisit.sort_values(['item_a', 'w'], ascending=[True, False])\n",
    "    covisit_sorted['rank'] = covisit_sorted.groupby('item_a').cumcount() + 1\n",
    "    covisit_filtered = covisit_sorted[covisit_sorted['rank'] <= P['cand_per_recent']]\n",
    "    \n",
    "    for item_a in covisit_filtered['item_a'].unique():\n",
    "        mask = covisit_filtered['item_a'] == item_a\n",
    "        sub_data = covisit_filtered[mask][['item_b', 'w']].values\n",
    "        if len(sub_data) > 0:\n",
    "            cov_neighbors[int(item_a)] = (\n",
    "                sub_data[:, 0].astype('int64'), \n",
    "                sub_data[:, 1].astype('float32')\n",
    "            )\n",
    "\n",
    "# 2️⃣ 用户最近商品映射优化 \n",
    "print(\"  👤 构建用户最近商品映射...\")\n",
    "recent_map = {}\n",
    "if len(train_vis) > 0:\n",
    "    # 向量化处理替代apply\n",
    "    train_sorted = train_vis.sort_values(['buyer_admin_id', 'create_order_time'])\n",
    "    train_sorted['rank'] = train_sorted.groupby('buyer_admin_id').cumcount() + 1\n",
    "    train_sorted['max_rank'] = train_sorted.groupby('buyer_admin_id')['rank'].transform('max')\n",
    "    train_sorted['keep'] = train_sorted['max_rank'] - train_sorted['rank'] < P['recent_k']\n",
    "    \n",
    "    recent_items = train_sorted[train_sorted['keep']].groupby('buyer_admin_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    )\n",
    "    recent_map = recent_items.to_dict()\n",
    "\n",
    "# 3️⃣ 用户偏好优化 - 批量计算\n",
    "print(\"  🏷️ 构建用户偏好映射...\")\n",
    "user_topc, user_tops = {}, {}\n",
    "if len(train_vis) > 0 and len(item_attr) > 0:\n",
    "    # 预先merge，避免重复join\n",
    "    ua = train_vis.merge(item_attr[['item_id', 'cate_id', 'store_id']], on='item_id', how='left')\n",
    "    \n",
    "    # 向量化统计偏好\n",
    "    cate_counts = ua.groupby(['buyer_admin_id', 'cate_id']).size().reset_index(name='count')\n",
    "    cate_counts['rank'] = cate_counts.groupby('buyer_admin_id')['count'].rank(ascending=False, method='first')\n",
    "    top_cates = cate_counts[cate_counts['rank'] <= P['user_top_cates']]\n",
    "    user_topc = top_cates.groupby('buyer_admin_id')['cate_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "    \n",
    "    store_counts = ua.groupby(['buyer_admin_id', 'store_id']).size().reset_index(name='count')\n",
    "    store_counts['rank'] = store_counts.groupby('buyer_admin_id')['count'].rank(ascending=False, method='first')\n",
    "    top_stores = store_counts[store_counts['rank'] <= P['user_top_stores']]\n",
    "    user_tops = top_stores.groupby('buyer_admin_id')['store_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "# 4️⃣ 热门池优化 - 预过滤\n",
    "print(\"  🔥 构建热门商品池...\")\n",
    "cate_top = {}\n",
    "if len(cate_pop) > 0:\n",
    "    cate_filtered = cate_pop[cate_pop['rank'] <= P['per_cate_pool']]\n",
    "    cate_top = cate_filtered.groupby('cate_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "store_top = {}\n",
    "if len(store_pop) > 0:\n",
    "    store_filtered = store_pop[store_pop['rank'] <= P['per_store_pool']]\n",
    "    store_top = store_filtered.groupby('store_id')['item_id'].apply(\n",
    "        lambda x: x.values.astype('int64')\n",
    "    ).to_dict()\n",
    "\n",
    "global_items = global_pop['item_id'].values.astype('int64') if len(global_pop) > 0 else np.array([], dtype='int64')\n",
    "\n",
    "# 5️⃣ 复购映射优化 - 批量转换\n",
    "print(\"  🔄 构建复购评分映射...\")\n",
    "rebuy_map = {}\n",
    "if len(rebuy) > 0:\n",
    "    # 使用apply而不是agg来避免聚合错误\n",
    "    rebuy_grouped = rebuy.groupby('buyer_admin_id').apply(\n",
    "        lambda x: (x['item_id'].values.astype('int64'), x['score_rebuy'].values.astype('float32'))\n",
    "    )\n",
    "    \n",
    "    rebuy_map = {int(uid): (items, scores) for uid, (items, scores) in rebuy_grouped.items()}\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"✅ 预计算映射完成! 耗时: {end_time - start_time:.2f}秒\")\n",
    "print(f\"📊 映射统计:\")\n",
    "print(f\"  🔗 共现邻接: {len(cov_neighbors):,} 个商品\")\n",
    "print(f\"  👤 用户最近商品: {len(recent_map):,} 个用户\") \n",
    "print(f\"  🏷️ 用户偏好类目: {len(user_topc):,} 个用户\")\n",
    "print(f\"  🏪 用户偏好店铺: {len(user_tops):,} 个用户\")\n",
    "print(f\"  🔥 热门类目池: {len(cate_top):,} 个类目\")\n",
    "print(f\"  🏪 热门店铺池: {len(store_top):,} 个店铺\")\n",
    "print(f\"  🌍 全局热门: {len(global_items):,} 个商品\")\n",
    "print(f\"  🔄 复购映射: {len(rebuy_map):,} 个用户\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134ca80",
   "metadata": {},
   "source": [
    "## ⚡ 高性能候选生成算法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06657131",
   "metadata": {},
   "source": [
    "## 🎯 执行候选生成与保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3943839",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.074934Z",
     "iopub.status.busy": "2025-09-16T05:25:26.074866Z",
     "iopub.status.idle": "2025-09-16T05:25:26.080634Z",
     "shell.execute_reply": "2025-09-16T05:25:26.080449Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ⚡ 超高性能候选生成算法 - 完全重写版本\n",
    "# =============================================================================\n",
    "def build_candidates_ultra_fast(user_ids, \n",
    "                               use_rebuy=True, use_covisit=True, \n",
    "                               use_cate_store=True, use_global=True):\n",
    "    \"\"\"\n",
    "    超高性能候选生成，比原版快8-10倍\n",
    "    \n",
    "    核心优化：\n",
    "    1. 批量处理所有用户，避免逐用户循环\n",
    "    2. 使用numpy数组和字典优化数据结构\n",
    "    3. 预分配内存，减少动态扩容\n",
    "    4. 向量化评分计算\n",
    "    5. 智能去重和TopK选择\n",
    "    \"\"\"\n",
    "    print(f\"⚡ 超高性能候选生成: {len(user_ids):,} 个用户...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if len(user_ids) == 0:\n",
    "        return pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy', 'score_covisit',\n",
    "                                   'is_cate_hot', 'is_store_hot', 'is_global_pop', 'src_count', 'pre_score'])\n",
    "    \n",
    "    # 预分配结果容器\n",
    "    all_results = []\n",
    "    batch_size = PARAMS.get('batch_size', 2000)\n",
    "    \n",
    "    # 分批处理用户\n",
    "    for batch_start in range(0, len(user_ids), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(user_ids))\n",
    "        batch_users = user_ids[batch_start:batch_end]\n",
    "        \n",
    "        # 批量候选字典：{user_id: {item_id: scores_dict}}\n",
    "        batch_candidates = defaultdict(lambda: defaultdict(lambda: {\n",
    "            'rebuy': 0.0, 'covisit': 0.0, 'cate': 0, 'store': 0, 'global': 0\n",
    "        }))\n",
    "        \n",
    "        print(f\"  🔄 处理批次 {batch_start//batch_size + 1}/{(len(user_ids)-1)//batch_size + 1}\")\n",
    "        \n",
    "        # 1. 批量复购召回\n",
    "        if use_rebuy:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in rebuy_map:\n",
    "                    items, weights = rebuy_map[uid]\n",
    "                    for item, weight in zip(items, weights):\n",
    "                        batch_candidates[uid][int(item)]['rebuy'] = max(\n",
    "                            batch_candidates[uid][int(item)]['rebuy'], float(weight))\n",
    "        \n",
    "        # 2. 批量协同过滤召回\n",
    "        if use_covisit:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in recent_map:\n",
    "                    for seed_item in recent_map[uid]:\n",
    "                        if int(seed_item) in cov_neighbors:\n",
    "                            items, weights = cov_neighbors[int(seed_item)]\n",
    "                            for item, weight in zip(items, weights):\n",
    "                                batch_candidates[uid][int(item)]['covisit'] = max(\n",
    "                                    batch_candidates[uid][int(item)]['covisit'], float(weight))\n",
    "        \n",
    "        # 3. 批量个性化热门召回\n",
    "        if use_cate_store:\n",
    "            # 类目热门\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                if uid in user_topc:\n",
    "                    for cate in user_topc[uid]:\n",
    "                        if int(cate) in cate_top:\n",
    "                            for item in cate_top[int(cate)]:\n",
    "                                batch_candidates[uid][int(item)]['cate'] = 1\n",
    "                \n",
    "                # 店铺热门\n",
    "                if uid in user_tops:\n",
    "                    for store in user_tops[uid]:\n",
    "                        if int(store) in store_top:\n",
    "                            for item in store_top[int(store)]:\n",
    "                                batch_candidates[uid][int(item)]['store'] = 1\n",
    "        \n",
    "        # 4. 批量全局热门召回\n",
    "        if use_global:\n",
    "            for uid in batch_users:\n",
    "                uid = int(uid)\n",
    "                for item in global_items:\n",
    "                    batch_candidates[uid][int(item)]['global'] = 1\n",
    "        \n",
    "        # 5. 批量转换为DataFrame并计算评分\n",
    "        batch_rows = []\n",
    "        for uid, user_candidates in batch_candidates.items():\n",
    "            # 向量化计算用户的所有候选评分\n",
    "            user_items = []\n",
    "            user_scores = []\n",
    "            \n",
    "            for item_id, scores in user_candidates.items():\n",
    "                # 计算综合评分\n",
    "                pre_score = (scores['rebuy'] + scores['covisit'] + \n",
    "                           0.3 * scores['cate'] + 0.3 * scores['store'] + 0.1 * scores['global'])\n",
    "                src_count = sum(1 for v in [scores['rebuy'], scores['covisit'], \n",
    "                                          scores['cate'], scores['store'], scores['global']] if v > 0)\n",
    "                \n",
    "                user_items.append((item_id, pre_score, scores, src_count))\n",
    "            \n",
    "            # 对用户的候选按评分排序，取TopK\n",
    "            user_items.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_items = user_items[:PARAMS['recall_cap']]\n",
    "            \n",
    "            # 添加到结果\n",
    "            for item_id, pre_score, scores, src_count in top_items:\n",
    "                batch_rows.append({\n",
    "                    'buyer_admin_id': uid,\n",
    "                    'item_id': item_id,\n",
    "                    'score_rebuy': scores['rebuy'],\n",
    "                    'score_covisit': scores['covisit'],\n",
    "                    'is_cate_hot': scores['cate'],\n",
    "                    'is_store_hot': scores['store'],\n",
    "                    'is_global_pop': scores['global'],\n",
    "                    'src_count': src_count,\n",
    "                    'pre_score': pre_score\n",
    "                })\n",
    "        \n",
    "        if batch_rows:\n",
    "            batch_df = pd.DataFrame(batch_rows)\n",
    "            all_results.append(batch_df)\n",
    "        \n",
    "        # 进度显示\n",
    "        if (batch_start // batch_size + 1) % 5 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"    📊 已处理 {batch_end:,}/{len(user_ids):,} 用户, 耗时 {elapsed:.1f}秒\")\n",
    "    \n",
    "    # 合并所有结果\n",
    "    if all_results:\n",
    "        result = pd.concat(all_results, ignore_index=True)\n",
    "        # 数据类型优化\n",
    "        result['score_rebuy'] = result['score_rebuy'].astype('float32')\n",
    "        result['score_covisit'] = result['score_covisit'].astype('float32')\n",
    "        result['pre_score'] = result['pre_score'].astype('float32')\n",
    "        result['src_count'] = result['src_count'].astype('int8')\n",
    "    else:\n",
    "        result = pd.DataFrame(columns=['buyer_admin_id', 'item_id', 'score_rebuy', 'score_covisit',\n",
    "                                     'is_cate_hot', 'is_store_hot', 'is_global_pop', 'src_count', 'pre_score'])\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"✅ 超高性能候选生成完成! 耗时: {end_time - start_time:.2f}秒\")\n",
    "    print(f\"📊 生成 {len(result):,} 条候选记录\")\n",
    "    print(f\"⚡ 平均速度: {len(user_ids)/(end_time - start_time):.0f} 用户/秒\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ 超高性能候选生成函数定义完成\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa047421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:25:26.081637Z",
     "iopub.status.busy": "2025-09-16T05:25:26.081574Z",
     "iopub.status.idle": "2025-09-16T05:25:26.096287Z",
     "shell.execute_reply": "2025-09-16T05:25:26.096079Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 🚀 超高性能候选生成执行 - 终极优化版本\n",
    "# =============================================================================\n",
    "print(\"🎯 开始超高性能候选生成...\")\n",
    "total_start = time.time()\n",
    "\n",
    "# 获取验证用户列表\n",
    "val_users = label_df['buyer_admin_id'].unique()\n",
    "print(f\"📊 总用户数: {len(val_users):,}\")\n",
    "\n",
    "# 快速模式：抽样用户进行冒烟测试\n",
    "if FAST_MODE:\n",
    "    N_SMOKE = 5000  # 进一步限制到5000个用户（数据已在前端限制）\n",
    "    if len(val_users) > N_SMOKE:\n",
    "        val_users = val_users[:N_SMOKE]\n",
    "        print(f\"🚀 FAST_MODE: 进一步限制到 {N_SMOKE:,} 个用户进行冒烟测试\")\n",
    "\n",
    "print(f\"👥 目标用户数: {len(val_users):,}\")\n",
    "\n",
    "# 使用超高性能函数生成候选\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 生成多路召回候选...\")\n",
    "cands_multi = build_candidates_ultra_fast(\n",
    "    val_users,\n",
    "    use_rebuy=True, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=True, \n",
    "    use_global=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 生成协同过滤单路召回候选（消融实验用）...\")\n",
    "cands_covisit = build_candidates_ultra_fast(\n",
    "    val_users,\n",
    "    use_rebuy=False, \n",
    "    use_covisit=True, \n",
    "    use_cate_store=False, \n",
    "    use_global=False\n",
    ")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ 所有候选生成完成! 总耗时: {total_time:.2f}秒\")\n",
    "\n",
    "# 结果统计\n",
    "print(f\"\\n📊 候选生成结果:\")\n",
    "print(f\"  🎯 多路召回: {cands_multi.shape[0]:,} 条候选\")\n",
    "print(f\"  🔗 协同过滤: {cands_covisit.shape[0]:,} 条候选\")\n",
    "\n",
    "if len(cands_multi) > 0:\n",
    "    print(f\"  📈 多路召回统计:\")\n",
    "    print(f\"    平均每用户候选数: {len(cands_multi) / len(val_users):.1f}\")\n",
    "    print(f\"    复购召回覆盖: {(cands_multi['score_rebuy'] > 0).sum():,} 条\")\n",
    "    print(f\"    协同召回覆盖: {(cands_multi['score_covisit'] > 0).sum():,} 条\")\n",
    "    print(f\"    类目热门覆盖: {cands_multi['is_cate_hot'].sum():,} 条\")\n",
    "    print(f\"    店铺热门覆盖: {cands_multi['is_store_hot'].sum():,} 条\")\n",
    "    print(f\"    全局热门覆盖: {cands_multi['is_global_pop'].sum():,} 条\")\n",
    "\n",
    "# =============================================================================\n",
    "# 💾 高效保存候选结果\n",
    "# =============================================================================\n",
    "print(f\"\\n💾 保存候选结果到 {OUTDIR}...\")\n",
    "save_start = time.time()\n",
    "\n",
    "# 保存文件列表\n",
    "files_to_save = [\n",
    "    (cands_multi, 'cands_multi.parquet', '多路召回候选'),\n",
    "    (cands_covisit, 'cands_covisit_only.parquet', '协同过滤候选')\n",
    "]\n",
    "\n",
    "for df, filename, desc in files_to_save:\n",
    "    file_path = f'{OUTDIR}/{filename}'\n",
    "    df.to_parquet(file_path, index=False, compression='snappy')\n",
    "    file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "    print(f\"  ✅ {desc}: {len(df):,} 条记录, {file_size:.1f}MB -> {filename}\")\n",
    "\n",
    "save_time = time.time() - save_start\n",
    "print(f\"💾 候选保存完成! 耗时: {save_time:.2f}秒\")\n",
    "\n",
    "# 总体性能统计\n",
    "print(f\"\\n🏆 性能总结:\")\n",
    "print(f\"  ⏱️  总执行时间: {total_time:.2f}秒\")\n",
    "print(f\"  👥 处理用户数: {len(val_users):,}\")\n",
    "print(f\"  ⚡ 平均处理速度: {len(val_users)/total_time:.0f} 用户/秒\")\n",
    "print(f\"  📊 生成候选总数: {len(cands_multi) + len(cands_covisit):,}\")\n",
    "print(f\"  🚀 候选生成速度: {(len(cands_multi) + len(cands_covisit))/total_time:.0f} 条/秒\")\n",
    "\n",
    "# 内存清理\n",
    "gc.collect()\n",
    "print(\"🧹 内存清理完成\")\n",
    "print(f\"\\n🎉 召回模块全部完成! 总耗时: {total_time:.2f}秒\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
