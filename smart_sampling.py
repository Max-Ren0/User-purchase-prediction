#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½åˆ†å±‚æŠ½æ ·ç®—æ³• - å®Œå…¨ä¿®å¤ç‰ˆæœ¬
é’ˆå¯¹æ¨èç³»ç»Ÿæ•°æ®ç‰¹ç‚¹è®¾è®¡çš„é«˜è´¨é‡æŠ½æ ·ç­–ç•¥
"""

import pandas as pd
import numpy as np
import hashlib
from typing import Tuple, Dict, Any
import warnings
warnings.filterwarnings('ignore')

def _stable_subseed(*parts, base_seed: int = 42) -> int:
    """åŸºäº md5 çš„ç¨³å®šå­ç§å­ï¼ˆä¸åŒè¿›ç¨‹/é‡å¯ä¸€è‡´ï¼‰"""
    h = hashlib.md5("||".join(map(str, parts)).encode()).hexdigest()
    return base_seed ^ int(h[:8], 16)

def _dist_align(counts: pd.Series, labels) -> np.ndarray:
    """å°†åˆ†ç±»è®¡æ•°æŒ‰ç»Ÿä¸€ labels å¯¹é½å¹¶å½’ä¸€æˆæ¦‚ç‡åˆ†å¸ƒï¼›ç©ºåˆ™å‡åŒ€åˆ†å¸ƒ"""
    v = counts.reindex(labels, fill_value=0).to_numpy(dtype=np.float64)
    s = v.sum()
    if s <= 0:
        return np.ones(len(labels)) / len(labels)
    return v / s

def _waterfill_allocation(strata_counts: pd.Series, target: int, min_per: int, max_per: int) -> Dict:
    """æ°´ä½è¡¥é½åˆ†é…ï¼šä¼˜å…ˆå¡«æ»¡å°å±‚ï¼Œå†æŒ‰æ¯”ä¾‹åˆ†é…å‰©ä½™"""
    n_strata = len(strata_counts)
    if n_strata == 0:
        return {}
    
    # 1) å…ˆç»™æ¯å±‚åˆ†é…æœ€å°é…é¢
    alloc = {}
    remaining = target
    for key, count in strata_counts.items():
        take = min(min_per, count, remaining)
        alloc[key] = take
        remaining -= take
        if remaining <= 0:
            break
    
    if remaining <= 0:
        return alloc
    
    # 2) æŒ‰æ¯”ä¾‹åˆ†é…å‰©ä½™é…é¢
    if remaining > 0:
        # è®¡ç®—æ¯å±‚å¯åˆ†é…çš„ä¸Šé™
        available = {k: min(max_per - alloc[k], strata_counts[k] - alloc[k]) for k in alloc}
        total_available = sum(available.values())
        
        if total_available > 0:
            # æŒ‰æ¯”ä¾‹åˆ†é…
            for key in available:
                if available[key] > 0:
                    prop = available[key] / total_available
                    add = min(int(remaining * prop), available[key])
                    alloc[key] += add
                    remaining -= add
                    if remaining <= 0:
                        break
    
    return alloc

def smart_stratified_sampling(train_df: pd.DataFrame, 
                            target_users: int = 10000,
                            random_seed: int = 42) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    æ™ºèƒ½åˆ†å±‚æŠ½æ ·ç®—æ³•
    
    Args:
        train_df: åŸå§‹è®­ç»ƒæ•°æ®
        target_users: ç›®æ ‡ç”¨æˆ·æ•°é‡
        random_seed: éšæœºç§å­
    
    Returns:
        sampled_df: æŠ½æ ·åçš„æ•°æ®
        sampling_info: æŠ½æ ·ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½åˆ†å±‚æŠ½æ ·: {len(train_df):,} æ¡è®°å½• -> ç›®æ ‡ {target_users:,} ç”¨æˆ·")
    
    # 1) åˆ†æç”¨æˆ·ç‰¹å¾
    user_stats = analyze_user_characteristics(train_df)
    
    # 2) å®šä¹‰åˆ†å±‚ç­–ç•¥
    strata_definition = define_strata(user_stats, target_users)
    
    # 3) æ‰§è¡Œåˆ†å±‚æŠ½æ ·ï¼ˆæ°´ä½è¡¥é½ + ç¨³å®šéšæœºï¼‰
    sampled_users = perform_stratified_sampling(user_stats, strata_definition, random_seed)
    
    # 4) ç”ŸæˆæŠ½æ ·æ•°æ®
    sampled_df = train_df[train_df['buyer_admin_id'].isin(sampled_users)].copy()
    
    # 5) è®¡ç®—æŠ½æ ·ç»Ÿè®¡ï¼ˆç”¨ JSD/MAEï¼‰
    sampling_info = calculate_sampling_statistics(train_df, sampled_df, user_stats, sampled_users)
    
    print(f"âœ… æŠ½æ ·å®Œæˆ: {len(sampled_df):,} æ¡è®°å½•, {len(sampled_users):,} ç”¨æˆ·")
    print(f"ğŸ“Š æ•°æ®ä¿ç•™ç‡: {len(sampled_df)/max(1,len(train_df))*100:.1f}%")
    
    return sampled_df, sampling_info

def analyze_user_characteristics(train_df: pd.DataFrame) -> pd.DataFrame:
    """åˆ†æç”¨æˆ·ç‰¹å¾ï¼Œä¸ºåˆ†å±‚æŠ½æ ·åšå‡†å¤‡"""
    print("  ğŸ“Š åˆ†æç”¨æˆ·ç‰¹å¾...")
    
    g = train_df.groupby('buyer_admin_id')
    user_stats = g.agg({
        'item_id': ['count', 'nunique'],
        'create_order_time': ['min', 'max']
    }).round(2)
    user_stats.columns = ['purchase_count', 'unique_items', 'first_purchase', 'last_purchase']
    user_stats['purchase_span_days'] = (user_stats['last_purchase'] - user_stats['first_purchase']).dt.days + 1
    user_stats['avg_purchase_interval'] = user_stats['purchase_span_days'] / user_stats['purchase_count']
    user_stats['purchase_frequency'] = user_stats['purchase_count'] / user_stats['unique_items']
    
    print(f"    ğŸ“ˆ ç”¨æˆ·ç»Ÿè®¡: {len(user_stats):,} ä¸ªç”¨æˆ·")
    print(f"    ğŸ“Š è´­ä¹°æ¬¡æ•°èŒƒå›´: {user_stats['purchase_count'].min()} - {user_stats['purchase_count'].max()}")
    print(f"    ğŸ›ï¸ å•†å“ç§ç±»èŒƒå›´: {user_stats['unique_items'].min()} - {user_stats['unique_items'].max()}")
    
    return user_stats.reset_index()

def define_strata(user_stats: pd.DataFrame, target_users: int) -> Dict[str, Any]:
    """å®šä¹‰åˆ†å±‚ç­–ç•¥"""
    print("  ğŸ¯ å®šä¹‰åˆ†å±‚ç­–ç•¥...")
    
    # åŸºäºåˆ†ä½æ•°å®šä¹‰åˆ†å±‚ï¼Œç¡®ä¿æ›´å¥½çš„åˆ†å¸ƒä¿æŒ
    purchase_25 = user_stats['purchase_count'].quantile(0.25)
    purchase_50 = user_stats['purchase_count'].quantile(0.50)
    purchase_75 = user_stats['purchase_count'].quantile(0.75)
    purchase_90 = user_stats['purchase_count'].quantile(0.90)
    
    diversity_25 = user_stats['unique_items'].quantile(0.25)
    diversity_50 = user_stats['unique_items'].quantile(0.50)
    diversity_75 = user_stats['unique_items'].quantile(0.75)
    diversity_90 = user_stats['unique_items'].quantile(0.90)
    
    # æŒ‰è´­ä¹°æ¬¡æ•°åˆ†å±‚ï¼ˆåŸºäºåˆ†ä½æ•°ï¼Œæ›´ç²¾ç¡®ï¼‰
    user_stats['purchase_tier'] = pd.cut(
        user_stats['purchase_count'], 
        bins=[0, purchase_25, purchase_50, purchase_75, purchase_90, float('inf')],
        labels=['very_low', 'low', 'medium', 'high', 'very_high']
    )
    
    # æŒ‰å•†å“å¤šæ ·æ€§åˆ†å±‚ï¼ˆåŸºäºåˆ†ä½æ•°ï¼‰
    user_stats['diversity_tier'] = pd.cut(
        user_stats['unique_items'],
        bins=[0, diversity_25, diversity_50, diversity_75, diversity_90, float('inf')],
        labels=['very_low_diversity', 'low_diversity', 'medium_diversity', 'high_diversity', 'very_high_diversity']
    )
    
    # æŒ‰æ—¶é—´åˆ†å¸ƒåˆ†å±‚ï¼ˆç¡®ä¿æ—¶é—´å¤šæ ·æ€§ï¼‰
    user_stats['time_tier'] = pd.cut(
        user_stats['first_purchase'].dt.day,
        bins=[0, 10, 20, 31],
        labels=['early_month', 'mid_month', 'late_month']
    )
    
    # è®¡ç®—æ¯å±‚çš„ç”¨æˆ·æ•°é‡
    strata_counts = user_stats.groupby(['purchase_tier', 'diversity_tier', 'time_tier']).size()
    
    # åŸºäºåŸå§‹åˆ†å¸ƒæ¯”ä¾‹åˆ†é…é…é¢
    strata_proportions = strata_counts / strata_counts.sum()
    
    # å®šä¹‰æŠ½æ ·æ¯”ä¾‹ï¼ˆåŸºäºåŸå§‹åˆ†å¸ƒæ¯”ä¾‹ï¼Œç¡®ä¿ä»£è¡¨æ€§ï¼‰
    strata_definition = {
        'strata_counts': strata_counts,
        'strata_proportions': strata_proportions,
        'target_users': target_users,
        'min_per_stratum': max(1, target_users // 100),  # æ¯å±‚æœ€å°‘ç”¨æˆ·æ•°
        'max_per_stratum': target_users // 2,  # æ¯å±‚æœ€å¤šç”¨æˆ·æ•°
    }
    
    print(f"    ğŸ“Š åˆ†å±‚ç»„åˆæ•°: {len(strata_counts)}")
    print(f"    ğŸ¯ ç›®æ ‡ç”¨æˆ·: {target_users:,}")
    print(f"    ğŸ“ æ¯å±‚é…é¢èŒƒå›´: {strata_definition['min_per_stratum']} - {strata_definition['max_per_stratum']}")
    print(f"    ğŸ“ˆ è´­ä¹°æ¬¡æ•°åˆ†ä½æ•°: {purchase_25:.1f}, {purchase_50:.1f}, {purchase_75:.1f}, {purchase_90:.1f}")
    print(f"    ğŸ›ï¸ å¤šæ ·æ€§åˆ†ä½æ•°: {diversity_25:.1f}, {diversity_50:.1f}, {diversity_75:.1f}, {diversity_90:.1f}")
    
    return strata_definition

def perform_stratified_sampling(user_stats: pd.DataFrame,
                                strata_definition: Dict[str, Any],
                                random_seed: int) -> np.ndarray:
    """åŸºäºåŸå§‹åˆ†å¸ƒæ¯”ä¾‹çš„åˆ†å±‚æŠ½æ ·"""
    print("  ğŸ² æ‰§è¡Œåˆ†å±‚æŠ½æ ·...")
    
    strata_counts = strata_definition['strata_counts']
    strata_proportions = strata_definition['strata_proportions']
    target_users = strata_definition['target_users']
    min_per = strata_definition['min_per_stratum']
    max_per = strata_definition['max_per_stratum']
    
    # åŸºäºåŸå§‹åˆ†å¸ƒæ¯”ä¾‹åˆ†é…é…é¢
    proportional_alloc = {}
    for key, proportion in strata_proportions.items():
        # æŒ‰æ¯”ä¾‹åˆ†é…ï¼Œä½†ç¡®ä¿åœ¨æœ€å°/æœ€å¤§èŒƒå›´å†…
        allocated = max(min_per, min(max_per, int(target_users * proportion)))
        proportional_alloc[key] = allocated
    
    # è°ƒæ•´é…é¢ï¼Œç¡®ä¿æ€»æ•°æ¥è¿‘ç›®æ ‡
    total_allocated = sum(proportional_alloc.values())
    if total_allocated != target_users:
        # æŒ‰æ¯”ä¾‹è°ƒæ•´
        adjustment_factor = target_users / total_allocated
        for key in proportional_alloc:
            proportional_alloc[key] = max(min_per, min(max_per, int(proportional_alloc[key] * adjustment_factor)))
    
    # å¦‚æœä»ç„¶ä¸åŒ¹é…ï¼Œä½¿ç”¨æ°´ä½è¡¥é½
    final_alloc = _waterfill_allocation(strata_counts, target_users, min_per, max_per)
    
    sampled_users = []
    for key, sample_size in final_alloc.items():
        if sample_size <= 0:
            continue
        pt, dt, tt = key
        mask = (
            (user_stats['purchase_tier'] == pt) &
            (user_stats['diversity_tier'] == dt) &
            (user_stats['time_tier'] == tt)
        )
        pool = user_stats.loc[mask, 'buyer_admin_id'].to_numpy()
        if pool.size == 0:
            continue
        sub_seed = _stable_subseed(pt, dt, tt, base_seed=random_seed)
        sub_rng = np.random.default_rng(sub_seed)
        take = min(sample_size, pool.size)
        sampled_users.extend(sub_rng.choice(pool, size=take, replace=False))
    
    # å¦‚æœä¸è¶³ï¼Œä»å‰©ä½™ç”¨æˆ·ä¸­è¡¥é½
    if len(sampled_users) < target_users:
        remaining = user_stats[~user_stats['buyer_admin_id'].isin(sampled_users)]['buyer_admin_id'].to_numpy()
        needed = target_users - len(sampled_users)
        if len(remaining) >= needed:
            sub = np.random.default_rng(_stable_subseed("pad", base_seed=random_seed)).choice(
                remaining, size=needed, replace=False)
            sampled_users.extend(sub)
    
    # å¦‚æœè¶…å‡ºï¼Œéšæœºåˆ é™¤
    if len(sampled_users) > target_users:
        sampled_users = np.random.default_rng(_stable_subseed("trim", base_seed=random_seed)).choice(
            sampled_users, size=target_users, replace=False)
    
    print(f"    âœ… æŠ½æ ·å®Œæˆ: {len(sampled_users):,} ä¸ªç”¨æˆ·")
    return np.array(sampled_users)

def calculate_sampling_statistics(original_df: pd.DataFrame, 
                                sampled_df: pd.DataFrame, 
                                user_stats: pd.DataFrame, 
                                sampled_users: np.ndarray) -> Dict[str, Any]:
    """è®¡ç®—æŠ½æ ·ç»Ÿè®¡ä¿¡æ¯"""
    print("  ğŸ“Š è®¡ç®—æŠ½æ ·ç»Ÿè®¡...")
    
    # åŸºç¡€ç»Ÿè®¡
    original_users = original_df['buyer_admin_id'].nunique()
    original_records = len(original_df)
    sampled_records = len(sampled_df)
    
    # ç”¨æˆ·ç‰¹å¾å¯¹æ¯”
    original_user_stats = user_stats
    sampled_user_stats = user_stats[user_stats['buyer_admin_id'].isin(sampled_users)]
    
    # è®¡ç®—åˆ†å¸ƒä¿æŒåº¦
    def calculate_distribution_preservation(original, sampled, column):
        orig_dist = original[column].value_counts(normalize=True).sort_index()
        samp_dist = sampled[column].value_counts(normalize=True).sort_index()
        
        # æ‰¾åˆ°å…±åŒåŒºé—´
        common_bins = list(set(orig_dist.index) & set(samp_dist.index))
        if len(common_bins) == 0:
            return 0.0
        
        # è®¡ç®—KLæ•£åº¦ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        from scipy.stats import entropy
        orig_common = orig_dist[common_bins] / orig_dist[common_bins].sum()
        samp_common = samp_dist[common_bins] / samp_dist[common_bins].sum()
        
        kl_div = entropy(samp_common, orig_common)
        return 1 - kl_div  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    
    # è®¡ç®—ç»Ÿè®¡ç›¸ä¼¼åº¦
    def calculate_statistical_similarity(original, sampled, column):
        orig_mean = original[column].mean()
        samp_mean = sampled[column].mean()
        orig_std = original[column].std()
        samp_std = sampled[column].std()
        
        # å‡å€¼ç›¸å¯¹è¯¯å·®
        mean_error = abs(orig_mean - samp_mean) / orig_mean if orig_mean > 0 else 0
        # æ ‡å‡†å·®ç›¸å¯¹è¯¯å·®
        std_error = abs(orig_std - samp_std) / orig_std if orig_std > 0 else 0
        
        # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆè¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
        similarity = 1 - (mean_error + std_error) / 2
        return max(0, similarity)
    
    # è´­ä¹°æ¬¡æ•°åˆ†å¸ƒä¿æŒåº¦
    purchase_preservation = calculate_distribution_preservation(
        original_user_stats, sampled_user_stats, 'purchase_count'
    )
    
    # å•†å“å¤šæ ·æ€§åˆ†å¸ƒä¿æŒåº¦
    diversity_preservation = calculate_distribution_preservation(
        original_user_stats, sampled_user_stats, 'unique_items'
    )
    
    # ç»Ÿè®¡ç›¸ä¼¼åº¦
    purchase_stat_similarity = calculate_statistical_similarity(
        original_user_stats, sampled_user_stats, 'purchase_count'
    )
    
    diversity_stat_similarity = calculate_statistical_similarity(
        original_user_stats, sampled_user_stats, 'unique_items'
    )
    
    # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
    overall_quality = (purchase_preservation + diversity_preservation + 
                      purchase_stat_similarity + diversity_stat_similarity) / 4
    
    sampling_info = {
        'original_users': original_users,
        'sampled_users': len(sampled_users),
        'original_records': original_records,
        'sampled_records': sampled_records,
        'user_sampling_ratio': len(sampled_users) / original_users,
        'record_sampling_ratio': sampled_records / original_records,
        'purchase_preservation': purchase_preservation,
        'diversity_preservation': diversity_preservation,
        'purchase_stat_similarity': purchase_stat_similarity,
        'diversity_stat_similarity': diversity_stat_similarity,
        'overall_quality': overall_quality,
        'avg_purchase_original': original_user_stats['purchase_count'].mean(),
        'avg_purchase_sampled': sampled_user_stats['purchase_count'].mean(),
        'avg_diversity_original': original_user_stats['unique_items'].mean(),
        'avg_diversity_sampled': sampled_user_stats['unique_items'].mean(),
        'std_purchase_original': original_user_stats['purchase_count'].std(),
        'std_purchase_sampled': sampled_user_stats['purchase_count'].std(),
        'std_diversity_original': original_user_stats['unique_items'].std(),
        'std_diversity_sampled': sampled_user_stats['unique_items'].std(),
    }
    
    print(f"    ğŸ“ˆ ç”¨æˆ·æŠ½æ ·ç‡: {sampling_info['user_sampling_ratio']*100:.1f}%")
    print(f"    ğŸ“Š è®°å½•æŠ½æ ·ç‡: {sampling_info['record_sampling_ratio']*100:.1f}%")
    print(f"    ğŸ¯ è´­ä¹°ä¿æŒåº¦: JSD {sampling_info['purchase_preservation']:.3f} | ç»Ÿè®¡ {sampling_info['purchase_stat_similarity']:.3f}")
    print(f"    ğŸ›ï¸ å¤šæ ·ä¿æŒåº¦: JSD {sampling_info['diversity_preservation']:.3f} | ç»Ÿè®¡ {sampling_info['diversity_stat_similarity']:.3f}")
    print(f"    â­ ç»¼åˆè´¨é‡åˆ†æ•°: {sampling_info['overall_quality']:.3f}")
    
    return sampling_info

def validate_sampling_quality(sampling_info: Dict[str, Any]) -> bool:
    """éªŒè¯æŠ½æ ·è´¨é‡"""
    print("  ğŸ” éªŒè¯æŠ½æ ·è´¨é‡...")
    
    # ä½¿ç”¨æ–°çš„ç»¼åˆè´¨é‡æŒ‡æ ‡
    quality_checks = {
        'ç”¨æˆ·æ•°é‡åˆç†': 5000 <= sampling_info['sampled_users'] <= 15000,
        'è®°å½•æ•°é‡åˆç†': sampling_info['sampled_records'] >= 10000,
        'è´­ä¹°ä¿æŒè‰¯å¥½(JSD)': sampling_info['purchase_preservation'] >= 0.6,
        'å¤šæ ·ä¿æŒè‰¯å¥½(JSD)': sampling_info['diversity_preservation'] >= 0.6,
        'è´­ä¹°ç»Ÿè®¡ç›¸ä¼¼åº¦': sampling_info['purchase_stat_similarity'] >= 0.7,
        'å¤šæ ·ç»Ÿè®¡ç›¸ä¼¼åº¦': sampling_info['diversity_stat_similarity'] >= 0.7,
        'ç»¼åˆè´¨é‡åˆ†æ•°': sampling_info['overall_quality'] >= 0.65,
    }
    
    all_passed = all(quality_checks.values())
    
    print("    ğŸ“‹ è´¨é‡æ£€æŸ¥ç»“æœ:")
    for check, passed in quality_checks.items():
        status = "âœ…" if passed else "âŒ"
        value = sampling_info.get(check.replace('åˆç†', '').replace('è‰¯å¥½', '').replace('ç›¸ä¼¼åº¦', '').replace('åˆ†æ•°', ''), '')
        if isinstance(value, (int, float)):
            print(f"      {status} {check}: {value:.3f}")
        else:
            print(f"      {status} {check}")
    
    if all_passed:
        print("    ğŸ‰ æŠ½æ ·è´¨é‡éªŒè¯é€šè¿‡!")
    else:
        print("    âš ï¸ æŠ½æ ·è´¨é‡éœ€è¦æ”¹è¿›ï¼ˆå¯é€‚åº¦æ”¾å®½é˜ˆå€¼æˆ–è°ƒæ•´åˆ†å±‚/é…é¢ï¼‰")
    
    return all_passed

if __name__ == "__main__":
    # æµ‹è¯•æŠ½æ ·ç®—æ³•
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½åˆ†å±‚æŠ½æ ·ç®—æ³•")
    
    # åŠ è½½æ•°æ®
    train_df = pd.read_csv('data/Antai_hackathon_train.csv')
    train_df['create_order_time'] = pd.to_datetime(train_df['create_order_time'])
    
    # æ‰§è¡ŒæŠ½æ ·
    sampled_df, sampling_info = smart_stratified_sampling(
        train_df, 
        target_users=10000, 
        random_seed=42
    )
    
    # éªŒè¯è´¨é‡
    validate_sampling_quality(sampling_info)
    
    # ä¿å­˜æŠ½æ ·ç»“æœ
    sampled_df.to_parquet('x/train_sampled_10k.parquet', index=False)
    print("ğŸ’¾ æŠ½æ ·æ•°æ®å·²ä¿å­˜: x/train_sampled_10k.parquet")
